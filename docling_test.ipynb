{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee82940e-24bb-47b4-b630-0113d2fdcb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0901 17:19:18.041000 95139 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecbe186-155b-41e4-a78c-87fe5228fc95",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1077279-7f5f-46fc-ac55-100688a7ec59",
   "metadata": {},
   "source": [
    "## Docling pdf (Text) with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a7ccf1-802a-483a-ad14-1c873a64221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document confidence ===\n",
      "Mean grade: excellent  (score=0.915)\n",
      "Low  grade: good   (score=0.838)\n",
      "OCR score: nan  Layout score: 0.829  Parse score: 1.0\n",
      "\n",
      "=== Per‑page confidence ===\n",
      "Page 0: OCR=— Layout=0.812 Parse=1.000 Table=— | mean_grade=excellent (score=0.906)\n",
      "Page 1: OCR=— Layout=0.937 Parse=1.000 Table=— | mean_grade=excellent (score=0.969)\n",
      "Page 2: OCR=— Layout=0.827 Parse=1.000 Table=— | mean_grade=excellent (score=0.914)\n",
      "Page 3: OCR=— Layout=0.740 Parse=1.000 Table=— | mean_grade=good (score=0.870)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10247"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions, EasyOcrOptions, RapidOcrOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import PdfFormatOption\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "pdf_path = \"ExampleOCR/NLP-CV-NachaiLim.pdf\"\n",
    "\n",
    "pipeline = PdfPipelineOptions(\n",
    "    do_structure=True,\n",
    "    detect_headings=True,\n",
    "    do_ocr=True,\n",
    "    ocr_options=EasyOcrOptions(\n",
    "        lang=[\"th\"]\n",
    "    ),\n",
    "    image_dpi=300                    # increase resolution for OCR\n",
    ")\n",
    "\n",
    "conv = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline)})\n",
    "res = conv.convert(pdf_path)\n",
    "\n",
    "rep = res.confidence  # ConversionResult.confidence\n",
    "\n",
    "print(\"=== Document confidence ===\")\n",
    "print(f\"Mean grade: {rep.mean_grade.value}  (score={rep.mean_score:.3f})\")\n",
    "print(f\"Low  grade: {rep.low_grade.value}   (score={rep.low_score:.3f})\")\n",
    "print(f\"OCR score: {rep.ocr_score:.3f}  Layout score: {rep.layout_score:.3f}  Parse score: {rep.parse_score}\")\n",
    "\n",
    "def fmt(x):\n",
    "    return \"—\" if (x is None or (isinstance(x, float) and math.isnan(x))) else f\"{x:.3f}\"\n",
    "\n",
    "print(\"\\n=== Per‑page confidence ===\")\n",
    "for idx, scores in rep.pages.items():  # dict: idx -> PageConfidenceScores\n",
    "    print(\n",
    "        f\"Page {idx}: \"\n",
    "        f\"OCR={fmt(scores.ocr_score)} \"\n",
    "        f\"Layout={fmt(scores.layout_score)} \"\n",
    "        f\"Parse={fmt(scores.parse_score)} \"\n",
    "        f\"Table={fmt(scores.table_score)} \"\n",
    "        f\"| mean_grade={scores.mean_grade.value} (score={fmt(scores.mean_score)})\"\n",
    "    )\n",
    "OUT_MD = Path(\"ocr_thai_comparison/output_NLP-CV-NachaiLim.md\")\n",
    "md = res.document.export_to_markdown()\n",
    "OUT_MD.write_text(md, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c286743c-b728-4b88-bedd-95ab00a26542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Nachai Limsettho\n",
      "\n",
      "Senior Data Scientist (NLP specialist)\n",
      "\n",
      "Nationality:\n",
      "\n",
      "Birthday:\n",
      "\n",
      "Mobile:\n",
      "\n",
      "Email:\n",
      "\n",
      "Address:\n",
      "\n",
      "Thai\n",
      "\n",
      "12 August 1988 +65 93234775 nachailim@gmail.com 140 Hillview Ave, Singapore 669600\n",
      "\n",
      "## PROFESSIONAL SUMMARY\n",
      "\n",
      "With over a decade of experience as a seasoned Data Scientist, I offer extensive expertise, particularly in Natural Language Processing (NLP), complemented by a strong technical background and hands-on involvement in data handling, manipulation, and analysis across dynami\n"
     ]
    }
   ],
   "source": [
    "print(res.document.export_to_markdown()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d824ad1-0822-4c72-a94d-ae5ca65f68cf",
   "metadata": {},
   "source": [
    "## Docling png with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fb3707-fce1-4e51-a6de-cd475dfc7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions, TesseractCliOcrOptions, EasyOcrOptions, OcrMacOptions\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from pathlib import Path\n",
    "import os, math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966d4c30-b850-47c8-b2f4-8a2510528b06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fmt(x):\n",
    "    return \"—\" if (x is None or (isinstance(x, float) and math.isnan(x))) else f\"{x:.3f}\"\n",
    "    \n",
    "def eval_docling_ocr(rep, print_md_length = 200):\n",
    "    print(\"=== Document confidence ===\")\n",
    "    print(f\"Mean grade: {rep.mean_grade.value}  (score={rep.mean_score:.3f})\")\n",
    "    print(f\"Low  grade: {rep.low_grade.value}   (score={rep.low_score:.3f})\")\n",
    "    print(f\"OCR score: {rep.ocr_score:.3f}  Layout score: {rep.layout_score:.3f}  Parse score: {rep.parse_score}\")\n",
    "        \n",
    "    print(\"\\n=== Per‑page confidence ===\")\n",
    "    for idx, scores in rep.pages.items():  # dict: idx -> PageConfidenceScores\n",
    "        print(\n",
    "            f\"Page {idx}: \"\n",
    "            f\"OCR={fmt(scores.ocr_score)} \"\n",
    "            f\"Layout={fmt(scores.layout_score)} \"\n",
    "            f\"Parse={fmt(scores.parse_score)} \"\n",
    "            f\"Table={fmt(scores.table_score)} \"\n",
    "            f\"| mean_grade={scores.mean_grade.value} (score={fmt(scores.mean_score)})\"\n",
    "        )\n",
    "    \n",
    "    print(res.document.export_to_markdown()[:print_md_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4541e3-1b13-49b3-aa18-3cfc78811215",
   "metadata": {},
   "source": [
    "#### Docling png with OCR: Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d0e324-e7f2-47dd-86ae-7659f88e9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_image_file = \"ExampleOCR/ocr_example_1_page_1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9981c761-0f84-4373-9ce8-d480a40b7a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document confidence ===\n",
      "Mean grade: good  (score=0.843)\n",
      "Low  grade: fair   (score=0.763)\n",
      "OCR score: 0.931  Layout score: 0.754  Parse score: nan\n",
      "\n",
      "=== Per‑page confidence ===\n",
      "Page 0: OCR=0.931 Layout=0.754 Parse=— Table=— | mean_grade=good (score=0.843)\n",
      "<!-- image -->\n",
      "\n",
      "Q/ 7 พ ร ะ ร า ช บ ั ญ ญ ั ต ิ\n",
      "\n",
      "ก า ร จ ั ด ซื อ จ ั ด จ ้ า ง แล ะ ก า ร บ ร ิ ห า ร พ ั ส ด ุ ภา ค ร ั ฐ\n",
      "\n",
      "W.A. ๒ ๕ ๒ ๐\n",
      "\n",
      "## ส ม เด ็ จ พ ร ะ เจ ้ า อ ย ู ่ ห ั ว ม ห า ว ชิ ร า ล ง ก \n"
     ]
    }
   ],
   "source": [
    "pipeline = PdfPipelineOptions(\n",
    "    do_ocr=True,\n",
    "    ocr_options=TesseractCliOcrOptions(lang=[\"tha\",\"eng\"], force_full_page_ocr=True),  # Thai + English\n",
    "    image_dpi=350                                                                      # upscale for small text\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.IMAGE: PdfFormatOption(pipeline_options=pipeline)}\n",
    ")\n",
    "\n",
    "res = converter.convert(ocr_image_file)\n",
    "rep = getattr(res, \"confidence\", None)\n",
    "eval_docling_ocr(rep, print_md_length = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a616dc-0ad1-4b24-863d-2e6192a3403b",
   "metadata": {},
   "source": [
    "#### Docling png with OCR: EasyOcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435f13ae-e433-403e-920a-8d3315645b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document confidence ===\n",
      "Mean grade: fair  (score=0.784)\n",
      "Low  grade: fair   (score=0.766)\n",
      "OCR score: 0.804  Layout score: 0.764  Parse score: nan\n",
      "\n",
      "=== Per‑page confidence ===\n",
      "Page 0: OCR=0.804 Layout=0.764 Parse=— Table=— | mean_grade=fair (score=0.784)\n",
      "<!-- image -->\n",
      "\n",
      "พระราชบัญญัติ การจัดซื้อจัดจ้างเละการบริหารพัสดุภาครัฐ พ.ศ. ๒๕๖0\n",
      "\n",
      "## สมเด็จพระเจ้าอยู่หัวมหาวชิราลงกรณฺฺบดินทรเทพยวรางกูร\n",
      "\n",
      "ให้ไว้ ณ วันที่ ๒๔ กุมภาพันธ์ พ.ศ. ๒๕๖0 เป็นปีที่ ๒ ในรัชกาลป\n"
     ]
    }
   ],
   "source": [
    "pipeline = PdfPipelineOptions(\n",
    "    do_ocr=True,\n",
    "    ocr_options=EasyOcrOptions(lang=[\"th\", \"en\"]),  # Thai + English\n",
    "    image_dpi=350                                   # upscale for small text\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.IMAGE: PdfFormatOption(pipeline_options=pipeline)}\n",
    ")\n",
    "\n",
    "res = converter.convert(ocr_image_file)\n",
    "rep = getattr(res, \"confidence\", None)\n",
    "eval_docling_ocr(rep, print_md_length = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec285a-43ef-46c3-a4f5-274b70f07bac",
   "metadata": {},
   "source": [
    "#### Docling png with OCR: OcrMac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c0006d-6fba-4cd4-8d4c-a2489c1a933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document confidence ===\n",
      "Mean grade: good  (score=0.804)\n",
      "Low  grade: fair   (score=0.770)\n",
      "OCR score: 0.841  Layout score: 0.766  Parse score: nan\n",
      "\n",
      "=== Per‑page confidence ===\n",
      "Page 0: OCR=0.841 Layout=0.766 Parse=— Table=— | mean_grade=good (score=0.804)\n",
      "<!-- image -->\n",
      "\n",
      "พระราชบัญญัติ การจัดซื้อจัดจ้างและการบริหารพัสดุภาครัฐ พ.ศ. ๒๕๖๐\n",
      "\n",
      "## สมเด็จพระเจ้าอยู่หัวมหาวชิราลงกรณ บดินทรเทพยวรางกูร\n",
      "\n",
      "ให้ไว้ ณ วันที่ ๒๔ กุมภาพันธ์ พ.ศ. ๒๕๖๐ เป็นปีที่๒ ในรัชกาลปัจ\n"
     ]
    }
   ],
   "source": [
    "pipeline = PdfPipelineOptions(\n",
    "    do_ocr=True,\n",
    "    ocr_options=OcrMacOptions(\n",
    "        lang=[\"th-TH\"],\n",
    "        force_full_page_ocr=True\n",
    "    ),\n",
    "    image_dpi=300                                   # upscale for small text\n",
    ")\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.IMAGE: PdfFormatOption(pipeline_options=pipeline)}\n",
    ")\n",
    "\n",
    "res = converter.convert(ocr_image_file)\n",
    "rep = getattr(res, \"confidence\", None)\n",
    "eval_docling_ocr(rep, print_md_length = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7e6f4-e249-4c25-bcea-64adddea9c45",
   "metadata": {},
   "source": [
    "## Docling png with OCR: Comparison, Image adjustment, PDF and Image ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fbb5897-7663-4de9-a6e3-6190d4642147",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_and_path_check(source_path):\n",
    "    # If image: maybe downscale; if PDF: use as-is\n",
    "    if is_image(source_path):\n",
    "        INPUT_KIND = InputFormat.IMAGE\n",
    "        SAFE_PATH = safe_downscale_if_needed(source_path)\n",
    "    elif is_pdf(source_path):\n",
    "        INPUT_KIND = InputFormat.PDF\n",
    "        SAFE_PATH = source_path\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input type: {source_path}\")\n",
    "    return INPUT_KIND, SAFE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30eb644-177d-4675-908d-6246331f0f8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def is_image(path: str) -> bool:\n",
    "    ext = Path(path).suffix.lower()\n",
    "    return ext in {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\", \".webp\"}\n",
    "\n",
    "def is_pdf(path: str) -> bool:\n",
    "    return Path(path).suffix.lower() == \".pdf\"\n",
    "\n",
    "def safe_downscale_if_needed(in_path: str, max_pixels: int = 120_000_000) -> str:\n",
    "    \"\"\"\n",
    "    Only used for large raster images. PDFs are left untouched.\n",
    "    \"\"\"\n",
    "    im = Image.open(in_path)\n",
    "    w, h = im.size\n",
    "    pixels = w * h\n",
    "    if pixels > max_pixels:\n",
    "        scale = math.sqrt(max_pixels / pixels)\n",
    "        new_w, new_h = max(1, int(w*scale)), max(1, int(h*scale))\n",
    "        im = Image.Image.resize(im, (new_w, new_h), Image.LANCZOS)\n",
    "        out_path = OUT_DIR / (Path(in_path).stem + f\"_downscaled_{new_w}x{new_h}.png\")\n",
    "        im.save(out_path, optimize=True)\n",
    "        return str(out_path)\n",
    "    return in_path\n",
    "\n",
    "def build_converter(ocr_opt, INPUT_KIND):\n",
    "    \"\"\"\n",
    "    Important:\n",
    "    - For PDFs, DO NOT force full-page OCR. Docling will retain embedded text and\n",
    "      OCR only where needed (e.g., scanned regions).\n",
    "    - For images, forcing full-page OCR is fine.\n",
    "    \"\"\"\n",
    "    # If Tesseract is selected and input is IMAGE, allow force_full_page_ocr=True (already set in engines below).\n",
    "    # If input is PDF and ocr_opt is Tesseract with force_full_page_ocr=True,\n",
    "    # clone it without forcing full-page OCR to preserve embedded text.\n",
    "    if isinstance(ocr_opt, TesseractCliOcrOptions) and INPUT_KIND == InputFormat.PDF:\n",
    "        ocr_opt = TesseractCliOcrOptions(\n",
    "            lang=ocr_opt.lang,\n",
    "            force_full_page_ocr=False  # preserve embedded text in PDFs\n",
    "        )\n",
    "\n",
    "    pipe = PdfPipelineOptions(\n",
    "        do_ocr=True,                 # enables OCR when needed\n",
    "        ocr_options=ocr_opt,\n",
    "        image_dpi=350,               # used when rasterizing pages or images\n",
    "        do_table_structure=True\n",
    "    )\n",
    "\n",
    "    # Use PdfFormatOption regardless; Docling routes by InputFormat\n",
    "    return DocumentConverter(\n",
    "        format_options={INPUT_KIND: PdfFormatOption(pipeline_options=pipe)}\n",
    "    )\n",
    "\n",
    "def first_nonempty(*names):\n",
    "    for n in names:\n",
    "        if n is not None and n != \"\":\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "def run_and_report(name, ocr_options, INPUT_KIND, SAFE_PATH, prev_length = 200):\n",
    "    print(f\"\\n===== {name} ({'PDF' if INPUT_KIND==InputFormat.PDF else 'IMAGE'}) =====\")\n",
    "    conv = build_converter(ocr_options, INPUT_KIND)\n",
    "    res  = conv.convert(SAFE_PATH, raises_on_error=False)\n",
    "\n",
    "    print(\"Status:\", res.status)\n",
    "\n",
    "    # Optional message fields (may not exist):\n",
    "    msg = first_nonempty(\n",
    "        getattr(res, \"message\", None),\n",
    "        getattr(res, \"msg\", None),\n",
    "        getattr(res, \"error_message\", None),\n",
    "        getattr(res, \"detail\", None),\n",
    "    )\n",
    "    if msg and \"SUCCESS\" not in str(res.status):\n",
    "        print(\"Message:\", msg)\n",
    "\n",
    "    rep = getattr(res, \"confidence\", None)\n",
    "    if rep:\n",
    "        print(f\"Doc mean_grade={getattr(rep,'mean_grade',None)} score={getattr(rep,'mean_score',None)}\")\n",
    "        print(f\"OCR score={getattr(rep,'ocr_score',None)}  Layout score={getattr(rep,'layout_score',None)}  Parse score={getattr(rep,'parse_score',None)}\")\n",
    "        if getattr(rep, \"pages\", None):\n",
    "            import math as _m\n",
    "            def fmt(x): return \"—\" if (x is None or (isinstance(x, float) and _m.isnan(x))) else f\"{x:.3f}\"\n",
    "            for idx, scores in rep.pages.items():\n",
    "                print(f\"Page {idx}: OCR={fmt(scores.ocr_score)} Layout={fmt(scores.layout_score)} Parse={fmt(scores.parse_score)} Table={fmt(scores.table_score)}\")\n",
    "\n",
    "    md = res.document.export_to_markdown() if getattr(res, \"document\", None) else \"\"\n",
    "\n",
    "    # Save OCR output separately based on original file name\n",
    "    stem = Path(SAFE_PATH).stem\n",
    "    out_md = OUT_DIR / f\"{stem}.{name}.md\"\n",
    "    out_md.write_text(md, encoding=\"utf-8\")\n",
    "    print(f\"Markdown saved -> {out_md}\")\n",
    "    print(\"Preview:\", (md[:prev_length].replace(\"\\n\",\"\\\\n\") if md else \"<empty>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206160c2-da79-47f2-ad84-ceb9f2804087",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = {\n",
    "    # For images, forcing full-page OCR is OK; for PDFs we override to False in build_converter()\n",
    "    \"tesseract_cli\": TesseractCliOcrOptions(lang=[\"tha\",\"eng\"], force_full_page_ocr=True),\n",
    "    \"easyocr\":       EasyOcrOptions(lang=[\"th\", \"en\"]),\n",
    "    \"macos\":         OcrMacOptions(lang=[\"th-TH\", \"en-US\"]),  # macOS only\n",
    "}\n",
    "OUT_DIR = Path(\"ocr_thai_comparison\"); OUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca5c3d9-922f-42d7-92b5-fd09df824403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== tesseract_cli (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8426271629548505\n",
      "OCR score=0.9310655533996213  Layout score=0.7541887725100798  Parse score=nan\n",
      "Page 0: OCR=0.931 Layout=0.754 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_example_1_page_1.tesseract_cli.md\n",
      "Preview: <!-- image -->\\n\\nQ/ 7 พ ร ะ ร า ช บ ั ญ ญ ั ต ิ\\n\\nก า ร จ ั ด ซื อ จ ั ด จ ้ า ง แล ะ ก า ร บ ร ิ ห า ร พ ั ส ด ุ ภา ค ร ั ฐ\\n\\nW.A. ๒ ๕ ๒ ๐\\n\\n## ส ม เด ็ จ พ ร ะ เจ ้ า อ ย ู ่ ห ั ว ม ห า ว ชิ ร า ล ง ก \n",
      "\n",
      "===== easyocr (IMAGE) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.FAIR score=0.7842323623939853\n",
      "OCR score=0.8044643238871299  Layout score=0.7640004009008408  Parse score=nan\n",
      "Page 0: OCR=0.804 Layout=0.764 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_example_1_page_1.easyocr.md\n",
      "Preview: <!-- image -->\\n\\nพระราชบัญญัติ การจัดซื้อจัดจ้างเละการบริหารพัสดุภาครัฐ พ.ศ. ๒๕๖0\\n\\n## สมเด็จพระเจ้าอยู่หัวมหาวชิราลงกรณฺฺบดินทรเทพยวรางกูร\\n\\nให้ไว้ ณ วันที่ ๒๔ กุมภาพันธ์ พ.ศ. ๒๕๖0 เป็นปีที่ ๒ ในรัชกาลป\n",
      "\n",
      "===== macos (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8035857270889242\n",
      "OCR score=0.8413793115780271  Layout score=0.7657921425998211  Parse score=nan\n",
      "Page 0: OCR=0.841 Layout=0.766 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_example_1_page_1.macos.md\n",
      "Preview: <!-- image -->\\n\\nพระราชบัญญัติ การจัดซื้อจัดจ้างและการบริหารพัสดุภาครัฐ พ.ศ. ๒๕๖๐\\n\\n## สมเด็จพระเจ้าอยู่หัวมหาวชิราลงกรณ บดินทรเทพยวรางกูร\\n\\nให้ไว้ ณ วันที่ ๒๔ กุมภาพันธ์ พ.ศ. ๒๕๖๐ เป็นปีที่๒ ในรัชกาลปัจ\n"
     ]
    }
   ],
   "source": [
    "source_path = \"ExampleOCR/ocr_example_1_page_1.png\"\n",
    "INPUT_KIND, SAFE_PATH = format_and_path_check(source_path)\n",
    "for name, opt in engines.items():\n",
    "    run_and_report(name, opt, INPUT_KIND, SAFE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8e052f-71df-4485-a3e4-363a4bb8d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== tesseract_cli (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8939240848939718\n",
      "OCR score=0.9375191104255319  Layout score=0.8503290593624115  Parse score=nan\n",
      "Page 0: OCR=0.938 Layout=0.850 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee.tesseract_cli.md\n",
      "Preview: ## WS! เค ร ื ่ อ ง บ ด ก า แฟ ด ้ ว ย น ะ\\n\\n<!-- image -->\\n\\n- (1) 1 Shot\\n- (2) 2 Shot\\n- (3) Power\\n- (4) Manual\\n- (5) Steaming\\n6. ห ม ุ น ป ร ั บ ร ะ ด ั บ ไอ น ํ า\\n\\n<!-- image -->\\n\\n<!-- image -->\n",
      "\n",
      "===== easyocr (IMAGE) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8635927734307284\n",
      "OCR score=0.8821304104457632  Layout score=0.8450551364156935  Parse score=nan\n",
      "Page 0: OCR=0.882 Layout=0.845 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee.easyocr.md\n",
      "Preview: ## ฟริ!! เครืองบดกาแฟ ดวยนะ\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n- 2 2 shot\\n- 3 power\\n- 4 manual\\n- steaming\\n6. หมุนปรับ ระดับไอนำ\\n\\n<!-- image -->\n",
      "\n",
      "===== macos (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8170206595550884\n",
      "OCR score=0.8  Layout score=0.8340413191101768  Parse score=nan\n",
      "Page 0: OCR=0.800 Layout=0.834 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee.macos.md\n",
      "Preview: ## ฟรี!! เครื่องบดกาแฟ ด้วยนะ\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n- 1 1 Shot\\n- 2) 2 Shot\\n- 3 Power\\n- 4 Manual\\n- 5 Steaming\\n- 6 หมุนปรับ ระดับไอน้ำ\\n- 7 แท็งก์ใส่น้ำ\\n\\n<!-- image -->\n"
     ]
    }
   ],
   "source": [
    "source_path = 'ExampleOCR/ocr_coffee.jpg'\n",
    "INPUT_KIND, SAFE_PATH = format_and_path_check(source_path)\n",
    "for name, opt in engines.items():\n",
    "    run_and_report(name, opt, INPUT_KIND, SAFE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "809a189d-198c-4d8d-935b-b9e1b45c74ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== tesseract_cli (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.FAIR score=0.7711517124818361\n",
      "OCR score=0.8601732874233127  Layout score=0.6821301375403594  Parse score=nan\n",
      "Page 0: OCR=0.860 Layout=0.682 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee2.tesseract_cli.md\n",
      "Preview: <!-- image -->\\n\\n## ส ่ ว น ป ร ะ ก อ บ / Element\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## ก ภา แฟ / Coffee\\n\\n1. ใส ่ เม ล ็ ต ก า แฟ ล ง ใน ซ่ อ ง ด ้ า น บ น Fill coffee bean on the top\\n2. เ ล ื อ ก เม น ุ\n",
      "\n",
      "===== easyocr (IMAGE) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.FAIR score=0.6787039724325388\n",
      "OCR score=0.6621525375998231  Layout score=0.6952554072652545  Parse score=nan\n",
      "Page 0: OCR=0.662 Layout=0.695 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee2.easyocr.md\n",
      "Preview: <!-- image -->\\n\\n## coffee  macaie\\n\\n## ส่วน ประกอบ/ element\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n- choose your drink and pres:\n",
      "\n",
      "===== macos (IMAGE) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.FAIR score=0.7057631720196117\n",
      "OCR score=0.75  Layout score=0.6615263440392234  Parse score=nan\n",
      "Page 0: OCR=0.750 Layout=0.662 Parse=— Table=—\n",
      "Markdown saved -> ocr_thai_comparison/ocr_coffee2.macos.md\n",
      "Preview: <!-- image -->\\n\\n## วิธีใช้เครื่องทำกาแฟ USING COFFEE MACHINE\\n\\n## ส่วนประกอบ / Element\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\n## กาแฟ / Coffee\\n\\n1. ใส่เมล็ดกาแฟลงในช่องด้านบน Fill coffee bean on the top\\n2. เล\n"
     ]
    }
   ],
   "source": [
    "source_path = 'ExampleOCR/ocr_coffee2.png'\n",
    "INPUT_KIND, SAFE_PATH = format_and_path_check(source_path)\n",
    "for name, opt in engines.items():\n",
    "    run_and_report(name, opt, INPUT_KIND, SAFE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ebc9b6-a3b6-45f4-bd50-c04ab4425bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== tesseract_cli (PDF) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.EXCELLENT score=0.9376878770318338\n",
      "OCR score=0.9013770414545454  Layout score=0.9116865896409557  Parse score=1.0\n",
      "Page 0: OCR=0.901 Layout=0.912 Parse=1.000 Table=—\n",
      "Markdown saved -> ocr_thai_comparison/office_cm_manual.tesseract_cli.md\n",
      "Preview: ## คู่มือการใช้งานเครื่องชงกาแฟส านักงาน\\n\\n## 1. ภาพรวม\\n\\nคู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในส านักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่อง ไปจนถึงการชงกาแฟ\\n\\n## 2. ส่ วนประกอบของเครื่\n",
      "\n",
      "===== easyocr (PDF) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8624245706224904\n",
      "OCR score=0.6755871222265155  Layout score=0.9116865896409557  Parse score=1.0\n",
      "Page 0: OCR=0.676 Layout=0.912 Parse=1.000 Table=—\n",
      "Markdown saved -> ocr_thai_comparison/office_cm_manual.easyocr.md\n",
      "Preview: ## คู่มือการใช้งานเครื่องชงกาแฟส านักงาน\\n\\n## 1. ภาพรวม\\n\\nคู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในส านักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่อง ไปจนถึงการชงกาแฟ\\n\\n## 2. ส่ วนประกอบของเครื่\n",
      "\n",
      "===== macos (PDF) =====\n",
      "Status: ConversionStatus.SUCCESS\n",
      "Doc mean_grade=QualityGrade.GOOD score=0.8544233923426586\n",
      "OCR score=0.7083333333333334  Layout score=0.8549368436946425  Parse score=1.0\n",
      "Page 0: OCR=0.708 Layout=0.855 Parse=1.000 Table=—\n",
      "Markdown saved -> ocr_thai_comparison/office_cm_manual.macos.md\n",
      "Preview: วิธีใช้เครื่องทำกาแฟ\\n\\nUSING COFFEE MACHINE\\n\\nส่วนประกอบ / Element\\n\\nจอแสดงเมนู\\n\\nMenu Display\\n\\nแก้วตีฟองนม\\n\\nMilk Frother\\n\\n## คู่มือการใช้งานเครื่องชงกาแฟส านักงาน\\n\\n## 1. ภาพรวม\\n\\nถังน้ำ\\n\\nWater Tank\\n\\nถาดรอ\n"
     ]
    }
   ],
   "source": [
    "source_path = 'ExampleOCR/office_cm_manual.pdf'\n",
    "INPUT_KIND, SAFE_PATH = format_and_path_check(source_path)\n",
    "for name, opt in engines.items():\n",
    "    run_and_report(name, opt, INPUT_KIND, SAFE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99455c06-8d14-4e53-bb9f-0aa201f4cc5d",
   "metadata": {},
   "source": [
    "# Vision Lanugae Model (VLM) with locally host Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f6774-99a4-4d7b-8386-76a194989c26",
   "metadata": {},
   "source": [
    "# Ollama Setup & Usage Guide\n",
    "\n",
    "## 1. Verify Ollama Daemon\n",
    "```bash\n",
    "ollama ps\n",
    "```\n",
    "If you see no error → the Ollama server (daemon) is running.  \n",
    "If you see “cannot connect” → start it manually:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "(or `brew services start ollama` if you installed with Homebrew).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pull a Model\n",
    "Ollama hosts many pre-built models. For example, to pull **LLaMA 3.1**:\n",
    "```bash\n",
    "ollama pull llama3.1\n",
    "```\n",
    "Other popular models:\n",
    "```bash\n",
    "ollama pull mistral\n",
    "ollama pull codellama\n",
    "ollama pull gemma:2b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Run a Model (Chat Mode)\n",
    "```bash\n",
    "ollama run llama3.1\n",
    "```\n",
    "This drops you into an interactive REPL where you can type questions.  \n",
    "Exit with `Ctrl+C`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Run One-Shot Prompts\n",
    "```bash\n",
    "ollama run mistral \"Explain transformers in 3 bullet points\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Use via API\n",
    "Ollama runs a local REST API at `http://localhost:11434`.\n",
    "\n",
    "Example with `curl`:\n",
    "```bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.1\",\n",
    "  \"prompt\": \"Write a haiku about data science\"\n",
    "}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Manage Models\n",
    "- List installed models:\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "- Remove a model:\n",
    "```bash\n",
    "ollama rm mistral\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Autostart (Optional)\n",
    "If you want Ollama to always run in background after boot:\n",
    "```bash\n",
    "brew services start ollama\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c814bad5-779f-4797-91d4-58522406210e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc455fb-ee5d-411c-9a62-26f5e6254ead",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ollama_vlm_call(model, prompt, image_path=None):\n",
    "    img_b64 = None\n",
    "    if image_path:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    if img_b64:\n",
    "        payload[\"images\"] = [img_b64]\n",
    "\n",
    "    resp = requests.post(\"http://localhost:11434/api/generate\", json=payload, timeout=300)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get(\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcd2f188-83b1-4694-96ae-1db0ed341b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The image is a promotional poster for a coffee machine with instructions on how to use it. It features both English and Thai text, indicating that the product is marketed in regions where these languages are prevalent. The poster includes images of the coffee machine and its various components, such as the water tank, milk frother, and fill coffee bean buttons.\n",
      "\n",
      "The top section of the poster has a header with the title \"USING COFFEE MACHINE\" in bold blue letters. Below this header, there is a list of elements or features of the machine, each accompanied by an image:\n",
      "- Menu Display\n",
      "- Fill coffee bean\n",
      "- Water Tank\n",
      "- Water Tray\n",
      "\n",
      "The middle section of the poster has a larger image of the coffee machine with labels pointing to these features. The text below this image is in Thai and provides instructions on how to use the machine, which are as follows:\n",
      "1. ซาเพื่อคลาดให้สุดกับคนที่มีความต้อง\n",
      "2. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "3. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "4. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "5. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "6. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "7. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "8. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "9. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "10. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "11. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "12. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "13. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "14. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "15. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "16. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "17. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "18. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "19. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "20. ซาเพื่อคลาดให้สุดกับคนที่ไม่มีความต้อง\n",
      "\n",
      "The bottom section of the poster has a button labeled \"Choose your drink and press\" with an arrow pointing to it, indicating that this is the next step in using the machine. The overall design of the poster is clean and straightforward, making it easy for users to understand how to operate the coffee machine.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"ExampleOCR/ocr_coffee2.png\"\n",
    "VLM_MODEL  = \"granite3.2-vision:latest\"   # or \"llama3.2-vision:latest\"\n",
    "VLM_PROMPT = \"Describe this image in 3 bullet points.\"\n",
    "# VLM_PROMPT = (\n",
    "#       \"You are converting a Thai PDF page to Markdown.\\n\"\n",
    "#       \"Rules:\\n\"\n",
    "#       \"1) Output ONLY valid Markdown, no tables unless the page itself contains a data table.\\n\"\n",
    "#       \"2) Preserve Thai text exactly; do not guess missing characters. If unsure, write (ไม่ชัดเจน).\\n\"\n",
    "#       \"3) Keep the original numbering (1., 2., 3.…). Use #, ## for headings.\\n\"\n",
    "#       \"4) No extra commentary.\\n\"\n",
    "#     )\n",
    "\n",
    "print(\n",
    "    ollama_vlm_call(\n",
    "        model=VLM_MODEL, \n",
    "        prompt = VLM_PROMPT,\n",
    "        image_path=image_path\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094a1d0-04bd-4490-9538-95c529a2dbe7",
   "metadata": {},
   "source": [
    "# Adding Vision Lanugae Model (VLM) to Document pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe40a76-ed28-4c8c-80b4-0feac06bed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "PDF_PATH = Path(\"ExampleOCR/office_cm_manual.pdf\")\n",
    "OUT_MD   = Path(\"ocr_thai_comparison/output.md\")\n",
    "OUT_MD.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434/api/generate\"\n",
    "VLM_MODEL  = \"granite3.2-vision:latest\"   # or \"llama3.2-vision:latest\"\n",
    "VLM_PROMPT = \"Describe this image in 3 bullet points.\"\n",
    "# VLM_PROMPT = (\n",
    "#       \"You are converting a Thai PDF page to Markdown.\\n\"\n",
    "#       \"Rules:\\n\"\n",
    "#       \"1) Output ONLY valid Markdown, no tables unless the page itself contains a data table.\\n\"\n",
    "#       \"2) Preserve Thai text exactly; do not guess missing characters. If unsure, write (ไม่ชัดเจน).\\n\"\n",
    "#       \"3) Keep the original numbering (1., 2., 3.…). Use #, ## for headings.\\n\"\n",
    "#       \"4) No extra commentary.\\n\"\n",
    "#     )\n",
    "DPI_REGION = 360  # 300–420 is a good sweet spot\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6de3df64-463b-4ebc-acd8-56503469a0cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Tuple, Optional\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef977a5f-69ea-4e0a-b9d1-f96477d54205",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VLM functions\n",
    "def ollama_vlm_bytes(image_bytes: bytes, model: str, prompt: str, *, temperature=0, top_p=0.1) -> str:\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [b64],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": temperature, \"top_p\": top_p},\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=300)\n",
    "    r.raise_for_status()\n",
    "    return (r.json().get(\"response\") or \"\").strip()\n",
    "\n",
    "\n",
    "def iter_pdf_images_best_quality(pdf_path: Path) -> Iterator[Tuple[int, bytes, str]]:\n",
    "    \"\"\"\n",
    "    Yield (page_index, image_bytes, ext) for each image on each page, preferring:\n",
    "      A) original embedded raster bytes via doc.extract_image(xref)\n",
    "      B) otherwise, a high‑DPI crop of the image's bounding box\n",
    "    \"\"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for pno, page in enumerate(doc):\n",
    "            # A) Try embedded rasters in reading order\n",
    "            seen_xrefs = set()\n",
    "            for info in page.get_images(full=True):\n",
    "                xref = info[0]\n",
    "                if xref in seen_xrefs:\n",
    "                    continue\n",
    "                seen_xrefs.add(xref)\n",
    "\n",
    "                try:\n",
    "                    emb = doc.extract_image(xref)  # {'image': bytes, 'ext': 'png'|'jpg'...}\n",
    "                    img_bytes = emb[\"image\"]\n",
    "                    ext = emb.get(\"ext\", \"png\")\n",
    "                    yield (pno, img_bytes, ext)\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    pass  # fall through to region render\n",
    "\n",
    "            # B) If we couldn't extract bytes, use layout dict to find image regions and crop at high DPI\n",
    "            rawdict = page.get_text(\"rawdict\")\n",
    "            if not rawdict:\n",
    "                continue\n",
    "\n",
    "            # rawdict['blocks'] may contain blocks with {'type': 1, 'image': xref, 'bbox': [x0,y0,x1,y1], ...}\n",
    "            regions = []\n",
    "            for block in rawdict.get(\"blocks\", []):\n",
    "                if block.get(\"type\") == 1 and \"image\" in block and \"bbox\" in block:\n",
    "                    xref = block[\"image\"]\n",
    "                    bbox = block[\"bbox\"]  # [x0, y0, x1, y1]\n",
    "                    regions.append((xref, bbox))\n",
    "\n",
    "            if not regions:\n",
    "                continue\n",
    "\n",
    "            # Render only those regions at DPI_REGION (faster & sharper than full page render)\n",
    "            scale = DPI_REGION / 72.0\n",
    "            mat = fitz.Matrix(scale, scale)\n",
    "            # Render the page once at high DPI; then crop bytes for each region\n",
    "            pix_full = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img_full = pix_full.tobytes(\"png\")\n",
    "\n",
    "            # We'll crop via Pillow to keep quality. Import lazily to avoid hard dep if not needed.\n",
    "            from PIL import Image\n",
    "            full_img = Image.open(io.BytesIO(img_full)).convert(\"RGB\")\n",
    "\n",
    "            for xref, (x0, y0, x1, y1) in regions:\n",
    "                # Convert PDF coords -> pixel coords under current matrix\n",
    "                # fitz applies matrix before rasterization; multiply by scale\n",
    "                px0, py0, px1, py1 = int(x0 * scale), int(y0 * scale), int(x1 * scale), int(y1 * scale)\n",
    "                px0, py0 = max(px0, 0), max(py0, 0)\n",
    "                px1, py1 = min(px1, full_img.width), min(py1, full_img.height)\n",
    "                if px1 <= px0 or py1 <= py0:\n",
    "                    continue\n",
    "\n",
    "                crop = full_img.crop((px0, py0, px1, py1))\n",
    "                buf = io.BytesIO()\n",
    "                crop.save(buf, format=\"PNG\", optimize=True)\n",
    "                yield (pno, buf.getvalue(), \"png\")\n",
    "\n",
    "# If your Markdown has explicit placeholders, we’ll fill them in order:\n",
    "def fill_image_tags_with_best_quality(md: str, pdf_path: Path, *, prompt: str = VLM_PROMPT) -> str:\n",
    "    IMG_TAG_RE = re.compile(r\"<!--\\s*image\\s*-->\", re.IGNORECASE)\n",
    "    matches = list(IMG_TAG_RE.finditer(md))\n",
    "    if not matches:\n",
    "        return md\n",
    "\n",
    "    img_stream = list(iter_pdf_images_best_quality(pdf_path))\n",
    "    if not img_stream:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            stream = []\n",
    "            mat = fitz.Matrix(DPI_REGION / 72.0, DPI_REGION / 72.0)\n",
    "            for pno, page in enumerate(doc):\n",
    "                pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "                stream.append((pno, pix.tobytes(\"png\"), \"png\"))\n",
    "            img_stream = stream\n",
    "\n",
    "    parts = []; last = 0; idx = 0\n",
    "    for m in matches:\n",
    "        parts.append(md[last:m.start()])\n",
    "        pno, img_bytes, ext = img_stream[idx % len(img_stream)]\n",
    "        idx += 1\n",
    "        desc = ollama_vlm_bytes(img_bytes, VLM_MODEL, prompt) or \"(no result)\"\n",
    "        parts.append(m.group(0) + \"\\n\" + desc + \"\\n<!-- image end -->\")\n",
    "        last = m.end()\n",
    "\n",
    "    parts.append(md[last:])\n",
    "    return \"\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05f0ffb-bc6e-4a69-bdc2-bcfde3638647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote ocr_thai_comparison/output.md\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Example usage -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Get your OCR’d Markdown (you already have this from Docling EasyOCR pipeline)\n",
    "    from docling.datamodel.base_models import InputFormat\n",
    "    from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions\n",
    "    from docling.document_converter import PdfFormatOption, DocumentConverter\n",
    "\n",
    "    pipeline = PdfPipelineOptions(\n",
    "        do_ocr=True,\n",
    "        ocr_options=EasyOcrOptions(lang=[\"th\"]),\n",
    "        image_dpi=300,\n",
    "    )\n",
    "    conv = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline)})\n",
    "    res = conv.convert(str(PDF_PATH))\n",
    "    md = res.document.export_to_markdown()\n",
    "\n",
    "    # 2) Fill the placeholders with high‑quality image bytes from the PDF\n",
    "    md2 = fill_image_tags_with_best_quality(md, PDF_PATH, prompt=VLM_PROMPT)\n",
    "\n",
    "    OUT_MD.write_text(md2, encoding=\"utf-8\")\n",
    "    print(f\"✅ Wrote {OUT_MD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12a850-4c23-4066-a4d5-297bb1b01e17",
   "metadata": {},
   "source": [
    "## Adding Vision Lanugae Model (VLM) to Document pipeline + Fixing PDF text typos from exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef19e4c2-55ec-4e8b-b0ca-c06361409105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, unicodedata, logging\n",
    "import fitz  # PyMuPDF\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from typing import Set, Iterable\n",
    "from functools import lru_cache\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "log = logging.getLogger(\"thai-fixer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f1ebfa8-43ca-4e70-917f-1d52944071e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = Path(\"ExampleOCR/office_cm_manual.pdf\")\n",
    "OUT_MD   = Path(\"ocr_thai_comparison/output_office_cm_manual.md\")\n",
    "# PDF_PATH = Path(\"ExampleOCR/ocr_example_p1.pdf\")\n",
    "# OUT_MD   = Path(\"ocr_thai_comparison/output_ocr_example_p1.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58547b26-8da9-4d51-b2e2-2f391200d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_MD.parent.mkdir(parents=True, exist_ok=True)\n",
    "token_penalty_weight = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88709f20-d264-4800-9cf9-a5a7e4299bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached Thai lexicon for fallback scoring\n",
    "THAI_LEXICON = set(thai_words())  # ~80k+ words depending on version\n",
    "\n",
    "# ----------------------------\n",
    "# Thai regex classes & utils\n",
    "# ----------------------------\n",
    "THAI  = r\"[\\u0E00-\\u0E7F]\"   # อักษรไทยทุกตัว (ก-ฮ, สระ, วรรณยุกต์ ฯลฯ)\n",
    "TONE  = r\"[\\u0E48-\\u0E4C]\"   # วรรณยุกต์  ◌่ ◌้ ◌๊ ◌๋ และ ์\n",
    "MARK  = r\"[\\u0E30-\\u0E4E]\"   # สระ/เครื่องหมายประกอบ รวมถึง ำ\n",
    "CONS  = r\"[ก-ฮ]\"            # พยัญชนะไทย\n",
    "W     = r\"[\\s\\u200B\\u00A0]\"  # ช่องว่าง/ZWSP/NBSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86484575-3716-477b-943c-132bbe5b5913",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: PyMuPDF‑guided repairs\n",
    "\n",
    "# Text Cleansing\n",
    "def _clean_controls(\n",
    "    s: str, \n",
    "    ZW   = r\"[\\u200B\\u200C\\u200D\\uFEFF]\",      # ZWSP, ZWNJ, ZWJ, BOM\n",
    "    CTRL = r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\" # ASCII control\n",
    ")-> str:\n",
    "    \"\"\"\n",
    "    Clean control and zero-width artifacts from a string.\n",
    "\n",
    "    Operations:\n",
    "      1) Normalize newlines: convert CRLF/CR -> LF.\n",
    "      2) Apply NFC Unicode normalization (compose combining sequences).\n",
    "      3) Remove zero-width characters (ZW).\n",
    "      4) Remove general control characters (CTRL).\n",
    "\n",
    "    Args:\n",
    "        s (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with normalized newlines, NFC form,\n",
    "             and no stray zero-width/control characters.\n",
    "    \"\"\"\n",
    "    # Step 1: normalize line endings to '\\n'\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Step 2: Unicode NFC normalization (compose accents/marks consistently)\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # Step 3: remove zero-width characters (ZW should be defined as a regex, e.g. r\"[\\u200B\\u200C\\u200D\\uFEFF]\")\n",
    "    s = re.sub(ZW, \"\", s)\n",
    "\n",
    "    # Step 4: remove other control characters (CTRL should be defined as a regex, e.g. r\"[\\x00-\\x1F\\x7F]\")\n",
    "    s = re.sub(CTRL, \"\", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Thai token splitting\n",
    "def _thai_tokens(s: str):\n",
    "    \"\"\"\n",
    "    Split a string into Thai tokens by removing non-Thai characters.\n",
    "\n",
    "    This uses the Unicode Thai block (U+0E00–U+0E7F).\n",
    "    Any sequence of consecutive Thai characters is treated as a token,\n",
    "    while non-Thai characters (punctuation, spaces, digits, Latin letters, etc.)\n",
    "    act as delimiters.\n",
    "\n",
    "    Args:\n",
    "        s (str): Input string containing Thai text (possibly mixed with other text).\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of Thai tokens (only sequences of Thai characters).\n",
    "    \"\"\"\n",
    "    # re.split with pattern [^\\u0E00-\\u0E7F]+:\n",
    "    # - Split at any run of non-Thai characters\n",
    "    # - Keep only non-empty results\n",
    "    return [t for t in re.split(rf\"[^\\u0E00-\\u0E7F]+\", s) if t]\n",
    "\n",
    "def _has_split_am(token: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a Thai token contains a 'split' sara am (ำ) character.\n",
    "\n",
    "    In Unicode, the Thai vowel 'ำ' (sara am, U+0E33) is a ligature of:\n",
    "      - U+0E32 (า, sara aa)\n",
    "      - U+0E4D ( ํ, nikkhahit)\n",
    "\n",
    "    Some OCR/PDF extraction tools mistakenly split 'ำ' into these\n",
    "    two separate codepoints, possibly with invisible characters \n",
    "    (e.g., zero-width space) between them.\n",
    "\n",
    "    This function detects such cases.\n",
    "\n",
    "    Args:\n",
    "        token (str): The string (token) to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the token contains a split 'ำ', otherwise False.\n",
    "    \"\"\"\n",
    "    # Regex pattern explanation:\n",
    "    # - \\u0E32 + W* + \\u0E4D → matches 'า' followed by optional invisible chars + ' ํ'\n",
    "    # - \\u0E4D + W* + \\u0E32 → matches ' ํ' followed by optional invisible chars + 'า'\n",
    "    # - W is assumed to be defined elsewhere (zero-width / whitespace pattern)\n",
    "    return bool(\n",
    "        re.search(r\"\\u0E32\" + W + r\"*\\u0E4D\"   # 'า' ... ' ํ'\n",
    "                  r\"|\"                         # OR\n",
    "                  r\"\\u0E4D\" + W + r\"*\\u0E32\", # ' ํ' ... 'า'\n",
    "                  token)\n",
    "    )\n",
    "\n",
    "def _fuse_split_am(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a Thai token by fusing split 'ำ' (sara am) sequences\n",
    "    into the correct single character U+0E33.\n",
    "\n",
    "    Thai 'ำ' (sara am, U+0E33) = 'า' (U+0E32) + ' ํ' (U+0E4D).\n",
    "    Some OCR/PDF tools incorrectly separate them, sometimes with\n",
    "    tone marks and/or invisible characters in between.\n",
    "\n",
    "    This function repairs such sequences:\n",
    "\n",
    "      Case A: Consonant + [tone]? + า + ํ  -> Consonant + [tone]? + ำ\n",
    "      Case B: Consonant + ํ + า + [tone]? -> Consonant + [tone]? + ำ\n",
    "\n",
    "    Args:\n",
    "        token (str): Input Thai string.\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized string with all split 'ำ' fused.\n",
    "    \"\"\"\n",
    "    # Case A: consonant + optional tone + 'า' + ' ํ'\n",
    "    # e.g. \"นา\\u0E4D\" -> \"น้ำ\"\n",
    "    token = re.sub(\n",
    "        rf\"({THAI})(?:({TONE}))?{W}*\\u0E32{W}*\\u0E4D\", \n",
    "        r\"\\1\\2ำ\", \n",
    "        token\n",
    "    )\n",
    "\n",
    "    # Case B: consonant + ' ํ' + 'า' + optional tone\n",
    "    # e.g. \"น\\u0E4Dา\" -> \"นา\\u0E33\" -> \"น้ำ\"\n",
    "    token = re.sub(\n",
    "        rf\"({THAI}){W}*\\u0E4D{W}*\\u0E32(?:({TONE}))?\", \n",
    "        r\"\\1\\2ำ\", \n",
    "        token\n",
    "    )\n",
    "\n",
    "    return token\n",
    "\n",
    "def _build_broken_regex_from_word(w: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a regex pattern that matches a 'broken' version of a Thai word.\n",
    "\n",
    "    Special handling for 'ำ' (sara am, U+0E33):\n",
    "      - Accepts the normal form: ำ\n",
    "      - Accepts split form: 'า' + [gap] + ' ํ' OR ' ํ' + [gap] + 'า'\n",
    "      - Accepts degraded form: just 'า' (in case ํ was dropped entirely)\n",
    "\n",
    "    For other Thai characters:\n",
    "      - Match the character itself, followed by optional invisible/whitespace chars (W*).\n",
    "\n",
    "    For non-Thai characters:\n",
    "      - Match the exact escaped literal (no W* after).\n",
    "\n",
    "    The final pattern is wrapped with lookbehind/lookahead:\n",
    "      - (?<![ก-๿]) ensures the match is not preceded by a Thai character.\n",
    "      - (?![ก-๿]) ensures the match is not followed by a Thai character.\n",
    "      This prevents partial matches inside larger Thai words.\n",
    "\n",
    "    Args:\n",
    "        w (str): The target Thai word to build a permissive regex for.\n",
    "\n",
    "    Returns:\n",
    "        str: A regex string that matches both correct and broken variants of `w`.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for ch in w:\n",
    "        if ch == \"ำ\":\n",
    "            # Accept normal 'ำ' OR split 'า ํ' OR reversed ' ํ า' OR degraded 'า'\n",
    "            parts.append(\n",
    "                r\"(?:\\u0E32\" + W + r\"*\\u0E4D\"  # 'า' ... ' ํ'\n",
    "                r\"|\\u0E4D\" + W + r\"*\\u0E32\"    # ' ํ' ... 'า'\n",
    "                r\"|\\u0E32)\"                    # just 'า'\n",
    "            )\n",
    "        elif re.match(r\"[\\u0E00-\\u0E7F]\", ch):\n",
    "            # Thai letter: itself + optional zero-widths/whitespace (W*)\n",
    "            parts.append(re.escape(ch) + W + r\"*\")\n",
    "        else:\n",
    "            # Non-Thai char: match literally\n",
    "            parts.append(re.escape(ch))\n",
    "\n",
    "    # Join all character parts into one regex\n",
    "    pat = \"\".join(parts)\n",
    "\n",
    "    # Wrap with lookarounds to ensure whole-word match in Thai context\n",
    "    return rf\"(?:(?<![\\u0E00-\\u0E7F]))({pat})(?:(?![\\u0E00-\\u0E7F]))\"\n",
    "\n",
    "# Create list of potential broken Thai words from pymupdf, assuming it can read Thai pdf text better\n",
    "def words_that_should_have_am_from_pdf(pdf_path: Path):\n",
    "    should_have = set()\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            try:\n",
    "                flags = fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE\n",
    "            except AttributeError:\n",
    "                flags = 0\n",
    "            txt = page.get_text(\"text\", flags=flags) or \"\"\n",
    "            txt = _clean_controls(txt)\n",
    "            for tok in _thai_tokens(txt):\n",
    "                if \"ำ\" in tok or _has_split_am(tok):\n",
    "                    fused = _fuse_split_am(tok)\n",
    "                    if len(fused) >= 2 and \"ำ\" in fused:\n",
    "                        should_have.add(fused)\n",
    "            # common short forms that often lose ํ\n",
    "            should_have.update({\"น้ำ\", \"ทำ\", \"กำ\"})\n",
    "    return should_have\n",
    "\n",
    "# Repair broken Thai words using words list from pymupdf, assuming it can read Thai pdf text better\n",
    "def apply_pymupdf_guided_repairs(md: str, pdf_path: Path):\n",
    "    \"\"\"\n",
    "    Normalize Thai text in `md` using cues from the original PDF text, with a focus on fixing\n",
    "    broken 'ำ' (sara am, U+0E33) and misplaced combining marks/tones.\n",
    "\n",
    "    Pipeline:\n",
    "      1) Clean control/zero-width artifacts (via `_clean_controls`).\n",
    "      2) Derive candidate words that *should* contain 'ำ' from the PDF (via\n",
    "         `words_that_should_have_am_from_pdf`), falling back to a small seed list if missing.\n",
    "      3) For each candidate word `w`, build a permissive regex ( `_build_broken_regex_from_word(w)` )\n",
    "         that matches broken variants (split 'ำ', reversed order, lost nikkhahit) and replace with `w`.\n",
    "      4) Re-attach combining marks/tones to the *left* Thai base character (don’t glue across words).\n",
    "      5) Apply minimal dictionary nudges for common OCR breakages (with robust zero-width gaps).\n",
    "      6) Collapse only *excess* horizontal whitespace and trim stray spaces around newlines.\n",
    "\n",
    "    Args:\n",
    "        md (str): Markdown/plain text extracted from a PDF (may contain Thai and zero-width artifacts).\n",
    "        pdf_path (Path): Source PDF to inspect for ground-truth Thai words (esp. those with 'ำ').\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, int]: (fixed_md, num_replacements)\n",
    "            fixed_md        : normalized text\n",
    "            num_replacements: total count of replacements across all sub-steps\n",
    "    \"\"\"\n",
    "    fixed = _clean_controls(md)  # remove ZW chars, BOMs, etc.\n",
    "    total = 0\n",
    "\n",
    "    # 1) Get candidate 'ำ' words from the PDF; fall back to a small seed set if none found.\n",
    "    candidates = words_that_should_have_am_from_pdf(pdf_path)\n",
    "    if not candidates:\n",
    "        # conservative defaults (keep short/common forms only)\n",
    "        candidates = {\"น้ำ\", \"จำเป็น\", \"ทำ\", \"กำ\"}\n",
    "\n",
    "    # 2) Replace broken variants of each candidate with the canonical form.\n",
    "    #    Sort by length DESC to replace longer words first (avoid partial overlaps).\n",
    "    for w in sorted(candidates, key=len, reverse=True):\n",
    "        pat = _build_broken_regex_from_word(w)  # permissive matcher for broken 'ำ'\n",
    "        rx = re.compile(pat)\n",
    "        if rx.search(fixed):\n",
    "            # You can pass `w` directly; the lambda is fine but not required.\n",
    "            fixed, n = rx.subn(w, fixed)\n",
    "            total += n\n",
    "\n",
    "    # 3) Re-attach combining marks/tones to the LEFT base Thai letter within a word.\n",
    "    #    Pattern: (THAI)(W+)(MARK/TONE)  ->  \\1\\2  (remove the gap; keep order)\n",
    "    fixed, n = re.subn(rf\"({THAI}){W}+({MARK})\", r\"\\1\\2\", fixed); total += n\n",
    "    fixed, n = re.subn(rf\"({THAI}){W}+({TONE})\", r\"\\1\\2\", fixed); total += n\n",
    "\n",
    "    # 4) Minimal targeted dictionary-style nudges for very frequent OCR glitches.\n",
    "    #    Zero-width tolerant (W*), but scoped to short, unambiguous patterns.\n",
    "    for pat, rep in [\n",
    "        (rf\"น{W}*้{W}*า\",    \"น้ำ\"),    # e.g., 'น ้ า' -> 'น้ำ'  \n",
    "        (rf\"จ{W}*า{W}*เป็น\", \"จำเป็น\"), # e.g., 'จ า เป็น' -> 'จำเป็น'\n",
    "        (rf\"ส{W}*า{W}*คัญ\",  \"สำคัญ\"),  # e.g., 'ส า คัญ' -> 'สำคัญ'\n",
    "        (rf\"ส{W}*า{W}*นัก\",  \"สำนัก\"),  # e.g., 'ส า นัก' -> 'สำนัก'\n",
    "        (rf\"บ{W}*า{W}*รุง\",  \"บำรุง\"),  # e.g., 'บ า รุง' -> 'บำรุง'\n",
    "        (rf\"ประจ{W}*า\",     \"ประจำ\"), # e.g., 'ประจ า' -> 'ประจำ'\n",
    "        (rf\"แนะน{W}*า\",     \"แนะนำ\"), # e.g., 'แนะน า' -> 'แนะนำ'\n",
    "    ]:\n",
    "        fixed, n = re.subn(pat, rep, fixed); total += n\n",
    "\n",
    "    # 5) Whitespace normalization (non-destructive):\n",
    "    #    - Collapse runs of spaces/tabs to a single space.\n",
    "    #    - Normalize newline surroundings: no trailing/leading spaces around '\\n'.\n",
    "    fixed, n = re.subn(r\"[ \\t]{2,}\", \" \", fixed); total += n\n",
    "    fixed, n = re.subn(r\"[ \\t]*\\n[ \\t]*\", \"\\n\", fixed); total += n\n",
    "\n",
    "    return fixed, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45eeb334-344a-459d-9b44-96826eb4cc20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Safe normalizer\n",
    "def normalize_thai(text: str):\n",
    "    \"\"\"\n",
    "    Normalize common Thai OCR/PDF artifacts.\n",
    "\n",
    "    Steps:\n",
    "      1) Remove zero-width/controls via `_clean_controls`.\n",
    "      2) Fuse split 'ำ' (U+0E33) only when the nikkhahit (U+0E4D) is present,\n",
    "         handling both 'า ... ํ' and ' ํ ... า' orders, preserving optional tone mark.\n",
    "      3) Pull combining marks/tones onto the LEFT base Thai letter (within a word),\n",
    "         without crossing word boundaries (pattern requires a Thai base immediately left).\n",
    "      4) Collapse only excess spaces/tabs and trim around newlines.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input string possibly containing Thai text and OCR artifacts.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, int]: (fixed_text, num_replacements)\n",
    "            fixed_text: normalized output\n",
    "            num_replacements: total number of regex substitutions applied\n",
    "    \"\"\"\n",
    "    num = 0\n",
    "\n",
    "    # 0) Remove ZW/controls (you provide this; must not alter visible content)\n",
    "    t = _clean_controls(text)\n",
    "\n",
    "    # 1) Merge split 'ำ' (structured; do NOT invent 'ำ' unless ํ exists)\n",
    "    t2, n = re.subn(rf\"({THAI})(?:({TONE}))?{W}*\\u0E32{W}*\\u0E4D\", r\"\\1\\2ำ\", t); num += n\n",
    "    t2, n = re.subn(rf\"({THAI}){W}*\\u0E4D{W}*\\u0E32(?:({TONE}))?\", r\"\\1\\2ำ\", t2); num += n\n",
    "\n",
    "    # 2) Re-attach combining marks/tones to the immediate LEFT base\n",
    "    #    (prevents gluing across words because a Thai base is required on the left)\n",
    "    t2, n = re.subn(rf\"({THAI}){W}+({MARK})\", r\"\\1\\2\", t2); num += n\n",
    "    t2, n = re.subn(rf\"({THAI}){W}+({TONE})\", r\"\\1\\2\", t2); num += n\n",
    "\n",
    "    # 3) Gentle whitespace normalization (non-destructive):\n",
    "    #    - collapse long runs of spaces/tabs\n",
    "    #    - trim spaces around newlines\n",
    "    t2, n = re.subn(r\"[ \\t]{2,}\", \" \", t2); num += n\n",
    "    t2, n = re.subn(r\"[ \\t]*\\n[ \\t]*\", \"\\n\", t2); num += n\n",
    "\n",
    "    return t2, num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc1c06b5-f6c9-4af0-ad77-8835ba7cfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TONE = \"่้๊๋\"                # Tone marks\n",
    "LEADING_VOWEL = \"เแโใไ\"   # Leading vowels (appear before consonant)\n",
    "_FINAL_NY  = \"นมงญรลวนย\"  # Common codas (ny/nasal/final consonants)\n",
    "VT_KEEP = set(\"ะาิีึืุูเแโใไ\")  # compare only these; ignore tones; DO map ำ→า before compare\n",
    "\n",
    "# --- Lexicon heuristics ---\n",
    "SHORT_AM_OK = {\"ทำ\", \"น้ำ\"}  # intentionally exclude 'คำ','กำ'\n",
    "\n",
    "# High-frequency บูสต์สำหรับคำที่มี 'ำ' ที่มักเจอในคู่มือ/เอกสาร\n",
    "BOOST_AM_SET = {\n",
    "    \"สำนักงาน\", \"สำหรับ\", \"สำคัญ\", \"กำลัง\", \"กำหนด\", \"ตำแหน่ง\",\n",
    "    \"คำสั่ง\", \"จำนวน\", \"อำนวย\", \"อำนาจ\", \"สำรอง\", \"สำนึก\", \"สำนัก\",\n",
    "    \"ทำความ\", \"ประจำ\", \"บำรุง\", \"แนะนา\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "788fb543-05bf-4886-99b5-e3b9b113330f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Statistical “ำ” fixer\n",
    "def _tokenize_th(text: str, engine: str = \"newmm\"):\n",
    "    \"\"\"\n",
    "    Tokenize Thai text into a list of words.\n",
    "\n",
    "    Args:\n",
    "        text (str): Thai string (raw).\n",
    "        engine (str): Tokenization engine to use (default: \"newmm\").\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of non-empty word tokens.\n",
    "    \"\"\"\n",
    "    # Normalize first (helps with OCR / PDF artifacts)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    tokens = word_tokenize(text, engine=engine)\n",
    "    return [tok for tok in tokens if tok.strip()]\n",
    "    \n",
    "def _thai_spans(text: str):\n",
    "    \"\"\"\n",
    "    Find contiguous spans of Thai text in a larger string.\n",
    "\n",
    "    A \"Thai span\" is defined here as:\n",
    "      • One or more Thai characters (THAI+)\n",
    "      • Optionally followed by repeated sequences of:\n",
    "            (zero-width/whitespace gap + more Thai characters)\n",
    "\n",
    "    This allows small gaps (W) between Thai characters but keeps the whole\n",
    "    run together as one span. Useful for locating Thai phrases inside\n",
    "    mixed-language or OCR-extracted text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input string possibly containing Thai and non-Thai text.\n",
    "\n",
    "    Yields:\n",
    "        tuple[int, int]: Start and end indices of each Thai span in the text.\n",
    "    \"\"\"\n",
    "    # Regex breakdown:\n",
    "    #   THAI+              → one or more Thai characters\n",
    "    #   (?:W+ THAI+)*      → zero or more groups of gap + Thai chars\n",
    "    for m in re.finditer(rf\"{THAI}+(?:{W}+{THAI}+)*\", text):\n",
    "        yield m.start(), m.end()    \n",
    "\n",
    "def _score_thai(\n",
    "    text: str,\n",
    "    base_lex:             float = 0.6,  # base score if token is in lexicon\n",
    "    base_thai:            float = 0.15, # base score if token looks Thai but not in lexicon\n",
    "    am_bonus_val:         float = 3.0,  # bonus for plausible 'ำ' merges\n",
    "    am_bonus_minlen:      float = 4.0,  # min length to allow am_bonus (unless in BOOST_AM_SET)\n",
    "    length_bump_factor:   float = 0.03, # per-character reward for lexicon words\n",
    "    length_bump_cap:      int   = 12,   # max chars rewarded by length bump\n",
    "    token_penalty_weight: float = 0.7,  # cohesion penalty per token\n",
    "    broken_na_penalty:    float = 2.0,  # penalty per broken 'น้ำ'\n",
    "    glued_mark_penalty:   float = 1.0,  # penalty per glued mark\n",
    "        # Penalty applied when a Thai combining mark (e.g., ◌ั, ◌์, ◌ํ) \n",
    "        # is separated from its base consonant/vowel by spaces or zero-width chars.  \n",
    "        # Example: \"กั\" (correct) vs. \"ก ◌ั\" or \"ก\\u200Bั\" (glued mark → penalized).\n",
    "    suspect_am_penalty:   float = 1.5   # penalty per 'ำ' + leading vowel\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute a cohesion-aware score for Thai text normalization candidates.\n",
    "\n",
    "    Parameters let you adjust scoring weights without touching the code.\n",
    "\n",
    "    Args:\n",
    "        text (str): Candidate string (already normalized/segmented).\n",
    "        base_lex: Score boost for dictionary words.\n",
    "        base_thai: Score boost for Thai-looking tokens not in lexicon.\n",
    "        am_bonus_val: Bonus when 'ำ' occurs in a plausible merged token.\n",
    "        am_bonus_minlen: Minimum token length for 'ำ' bonus (if in lexicon).\n",
    "        length_bump_factor: Per-character bump for lexicon tokens (up to length_bump_cap).\n",
    "        length_bump_cap: Maximum token length rewarded for length bump.\n",
    "        token_penalty_weight: Penalty multiplier per token (prefers fewer tokens).\n",
    "        broken_na_penalty: Penalty per broken 'น้ำ'.\n",
    "        glued_mark_penalty: Penalty per glued mark + gap.\n",
    "        suspect_am_penalty: Penalty per 'ำ' followed by a leading vowel.\n",
    "\n",
    "    Returns:\n",
    "        float: Cohesion-aware score; higher is better.\n",
    "    \"\"\"\n",
    "    toks = _tokenize_th(text)\n",
    "    n_tokens = len(toks)\n",
    "    log.debug(\"[_score_thai] Text: %r\", text)\n",
    "    log.debug(\"[_score_thai] Tokens (%d): %s\", n_tokens, toks)\n",
    "\n",
    "    total = 0.0\n",
    "    dict_hits = 0\n",
    "    am_bonus_hits = 0\n",
    "\n",
    "    for t in toks:\n",
    "        in_lex = (t in THAI_LEXICON)\n",
    "        base = base_lex if in_lex else (base_thai if re.match(THAI, t) else 0.0)\n",
    "        if in_lex:\n",
    "            dict_hits += 1\n",
    "\n",
    "        # 'ำ' bonus only for valid merges\n",
    "        am_bonus = 0.0\n",
    "        if (\"ำ\" in t) and ((in_lex and len(t) >= am_bonus_minlen) or (t in BOOST_AM_SET)):\n",
    "            am_bonus = am_bonus_val\n",
    "            am_bonus_hits += 1\n",
    "\n",
    "        # Length bump (lexicon tokens only, capped)\n",
    "        length_bump = (min(len(t), length_bump_cap) * length_bump_factor) if in_lex else 0.0\n",
    "\n",
    "        contrib = base + am_bonus + length_bump\n",
    "        total += contrib\n",
    "\n",
    "        log.debug(\n",
    "            \"    Token: %s  base=%.2f  am=%.2f  len+=%.2f  -> %.2f\",\n",
    "            t, base, am_bonus, length_bump, contrib\n",
    "        )\n",
    "\n",
    "    # Cohesion penalty (fewer tokens preferred)\n",
    "    token_penalty = token_penalty_weight * n_tokens\n",
    "    total -= token_penalty\n",
    "    log.debug(\"    Cohesion penalty: -%.2f (n_tokens=%d)\", token_penalty, n_tokens)\n",
    "\n",
    "    # Pattern penalties\n",
    "    broken_na = len(re.findall(r\"น\" + W + r\"+้\" + W + r\"+า\", text))\n",
    "    glued_mark = len(re.findall(rf\"{MARK}{W}+{THAI}\", text))\n",
    "    suspect_am_follow = len(re.findall(r\"ำ[เแโใไ]\", text))\n",
    "\n",
    "    penalty = (broken_na_penalty * broken_na +\n",
    "               glued_mark_penalty * glued_mark +\n",
    "               suspect_am_penalty * suspect_am_follow)\n",
    "\n",
    "    if penalty:\n",
    "        log.debug(\n",
    "            \"    Pattern penalty: broken_na=%d glued_mark=%d suspect_am_follow=%d -> -%.2f\",\n",
    "            broken_na, glued_mark, suspect_am_follow, penalty\n",
    "        )\n",
    "        total -= penalty\n",
    "\n",
    "    log.info(\n",
    "        f\"[_score_thai] {text} Total: %.4f (dict_hits=%d, am_bonus=%d, tokens=%d)\",\n",
    "        total, dict_hits, am_bonus_hits, n_tokens\n",
    "    )\n",
    "    return total\n",
    "\n",
    "def _vt_profile(s: str) -> str:\n",
    "    \"\"\"Vowel profile used to block harmful substitutions. Tones ignored; ำ normalized to า.\"\"\"\n",
    "    s = s.replace(\"ำ\", \"า\")\n",
    "    return \"\".join(ch for ch in s if ch in VT_KEEP)\n",
    "\n",
    "def _try_am_variants(\n",
    "    line: str,\n",
    "    # --- Behavior knobs ---\n",
    "    window_half: int = 8,                         # Context window around 'า' to test substitution\n",
    "    merge_token_bonus: float = 0.7,               # Reward per token merged if v2 reduces token count\n",
    "    boost_hit_bonus: float = 0.5,                 # Extra boost if the new token is in BOOST_AM_SET\n",
    "    margin_if_lex: float = 0.90,                  # Required margin if original token was valid lexicon\n",
    "    margin_if_nonlex: float = 0.10,               # Required margin otherwise\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Attempt to correct split/misrecognized 'ำ' in Thai text.\n",
    "\n",
    "    Strategy:\n",
    "      • Look for 'า' that could actually be 'ำ'.\n",
    "      • For each candidate, build two variants:\n",
    "            v1 = original window, v2 = with 'ำ' instead of 'า'.\n",
    "      • Splice them into the whole string, score with `_score_thai`.\n",
    "      • Prefer v2 if it scores clearly better (margin depends on context).\n",
    "      • Apply strong guards to avoid harming valid words.\n",
    "\n",
    "    Guards:\n",
    "      - Don’t break a lexicon word unless replacement is also valid.\n",
    "      - Don’t change vowel/tone profile.\n",
    "      - Block micro-patterns like [tone] + ำ + final consonant.\n",
    "      - Avoid suspicious 'ำ' followed by leading vowels (e.g., “กำแฟ”).\n",
    "    \"\"\"\n",
    "\n",
    "    THAI_SPAN_RX = re.compile(rf\"{THAI}+\")\n",
    "    CONS_RX      = re.compile(CONS)\n",
    "    THAI_RX      = re.compile(THAI)\n",
    "\n",
    "    def choose_best(s: str) -> str:\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(s):\n",
    "            ch = s[i]\n",
    "\n",
    "            # --- Candidate check: only process 'า' ---\n",
    "            if ch == \"า\":\n",
    "                # Check left context: consonant immediately left,\n",
    "                # or tone + consonant two chars left\n",
    "                prev = s[i - 1] if i > 0 else \"\"\n",
    "                prevprev = s[i - 2] if i > 1 else \"\"\n",
    "                left_consonant = (\n",
    "                    bool(CONS_RX.match(prev)) or\n",
    "                    (prev in _TONE and CONS_RX.match(prevprev or \"\") is not None)\n",
    "                )\n",
    "\n",
    "                # Check right context: must be followed by Thai char or end-of-span\n",
    "                nxt = s[i + 1] if i + 1 < len(s) else \"\"\n",
    "                right_thai_or_end = bool(THAI_RX.match(nxt)) or (i + 1 == len(s))\n",
    "\n",
    "                if left_consonant and right_thai_or_end:\n",
    "                    # --- Build local window around 'า' ---\n",
    "                    start = max(0, i - window_half)\n",
    "                    end   = min(len(s), i + window_half)\n",
    "                    window = s[start:end]\n",
    "                    win_pos = i - start  # offset of 'า' inside the window\n",
    "\n",
    "                    # Candidate windows:\n",
    "                    v1 = window                          # original\n",
    "                    v2 = window[:win_pos] + \"ำ\" + window[win_pos + 1:]  # with 'ำ'\n",
    "\n",
    "                    # Splice into whole string for scoring\n",
    "                    cand1 = s[:start] + v1 + s[end:]\n",
    "                    cand2 = s[:start] + v2 + s[end:]\n",
    "\n",
    "                    # Score both variants\n",
    "                    sc1 = _score_thai(cand1)\n",
    "                    sc2 = _score_thai(cand2)\n",
    "\n",
    "                    # Tokenize each window to compare structure\n",
    "                    w1_toks = _tokenize_th(v1)\n",
    "                    w2_toks = _tokenize_th(v2)\n",
    "\n",
    "                    # Bonus if v2 reduces number of tokens (better cohesion)\n",
    "                    if len(w2_toks) < len(w1_toks):\n",
    "                        sc2 += merge_token_bonus * (len(w1_toks) - len(w2_toks))\n",
    "\n",
    "                    # --- Identify the changed token in each sequence ---\n",
    "                    def covering(tokens, pos):\n",
    "                        acc = 0\n",
    "                        for idx, tk in enumerate(tokens):\n",
    "                            nxt_pos = acc + len(tk)\n",
    "                            if acc <= pos < nxt_pos:\n",
    "                                return idx, tk\n",
    "                            acc = nxt_pos\n",
    "                        return None, None\n",
    "\n",
    "                    idx1, tok1 = covering(w1_toks, win_pos)\n",
    "                    idx2, tok2 = covering(w2_toks, win_pos)\n",
    "\n",
    "                    # If coverage failed or both tokens look the same,\n",
    "                    # fallback: find the first differing token\n",
    "                    if tok1 is None or tok2 is None or tok1 == tok2:\n",
    "                        for j in range(min(len(w1_toks), len(w2_toks))):\n",
    "                            if w1_toks[j] != w2_toks[j]:\n",
    "                                idx1 = idx2 = j\n",
    "                                tok1 = w1_toks[j]\n",
    "                                tok2 = w2_toks[j]\n",
    "                                break\n",
    "                        if tok1 is None or tok2 is None:\n",
    "                            # If no meaningful change found → keep original 'า'\n",
    "                            out.append(ch); i += 1; continue\n",
    "\n",
    "                    # --- Guard checks ---\n",
    "                    tok1_is_lex = (tok1 in THAI_LEXICON)\n",
    "                    touched_valid = (\n",
    "                        (\"ำ\" in tok2) and (\n",
    "                            (tok2 in THAI_LEXICON and (len(tok2) >= 3 or tok2 in SHORT_AM_OK)) or\n",
    "                            (tok2 in BOOST_AM_SET)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Guard 1: Don't ruin a valid word\n",
    "                    if tok1_is_lex and not touched_valid:\n",
    "                        out.append(ch); i += 1; continue\n",
    "\n",
    "                    # Guard 2: Preserve vowel/tone profile\n",
    "                    if tok1_is_lex and _vt_profile(tok1) != _vt_profile(tok2):\n",
    "                        out.append(ch); i += 1; continue\n",
    "\n",
    "                    # Guard 3: Block pattern [tone] + ำ + final consonant\n",
    "                    harm = (\n",
    "                        win_pos - 1 >= 0 and window[win_pos - 1] in _TONE and\n",
    "                        win_pos + 1 < len(window) and window[win_pos + 1] in _FINAL_NY\n",
    "                    )\n",
    "                    if harm:\n",
    "                        out.append(ch); i += 1; continue\n",
    "\n",
    "                    # Guard 4: Avoid suspicious words ending with 'ำ' then starting with leading vowel\n",
    "                    suspicious_follow = any(\n",
    "                        w2_toks[j].endswith(\"ำ\")\n",
    "                        and j + 1 < len(w2_toks)\n",
    "                        and w2_toks[j + 1]\n",
    "                        and w2_toks[j + 1][0] in LEADING_VOWEL\n",
    "                        for j in range(len(w2_toks))\n",
    "                    )\n",
    "\n",
    "                    # Extra boost for highly frequent BOOST_AM_SET words\n",
    "                    if touched_valid and tok2 in BOOST_AM_SET:\n",
    "                        sc2 += boost_hit_bonus\n",
    "\n",
    "                    # --- Decision threshold ---\n",
    "                    margin = margin_if_lex if tok1_is_lex else margin_if_nonlex\n",
    "\n",
    "                    # Accept substitution only if:\n",
    "                    #  - new token is valid\n",
    "                    #  - no suspicious follow pattern\n",
    "                    #  - and sc2 beats sc1 by margin\n",
    "                    if touched_valid and not suspicious_follow and (sc2 > sc1 + margin):\n",
    "                        out.append(\"ำ\")\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "            # Default: keep the current character\n",
    "            out.append(ch)\n",
    "            i += 1\n",
    "\n",
    "        return \"\".join(out)\n",
    "\n",
    "    # --- Process only Thai spans; skip non-Thai segments ---\n",
    "    spans = [m.span() for m in THAI_SPAN_RX.finditer(line)]\n",
    "    if not spans:\n",
    "        return line\n",
    "\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for a, b in spans:\n",
    "        parts.append(line[last:a])             # Non-Thai prefix\n",
    "        parts.append(choose_best(line[a:b]))   # Repair inside Thai span\n",
    "        last = b\n",
    "    parts.append(line[last:])                  # Remaining tail\n",
    "\n",
    "    return \"\".join(parts)\n",
    "\n",
    "def _fix_tone_am_n(\n",
    "    text: str,\n",
    "    _TONE: str = \"่้๊๋\",      # tone marks (Mai Ek/Tho/Tri/Chattawa)\n",
    "    margin: float = 0.20, # minimal score gain required to accept ำ→า rewrite\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Selectively rewrite the pattern “[tone]ำน” → “[tone]าน” *only if* it improves cohesion.\n",
    "\n",
    "    Why:\n",
    "      OCR/PDF extraction sometimes produces words like “ด้ำน” (ด + ้ + ำ + น),\n",
    "      which should be “ด้าน”. This pass targets exactly: TONE + ำ + น inside Thai spans,\n",
    "      and applies the rewrite only when the scored quality clearly improves.\n",
    "\n",
    "    Assumes (defined elsewhere in the module):\n",
    "      - _thai_spans(text) -> iterator[(start, end)]: yields contiguous Thai-only spans.\n",
    "      - _score_thai(s) -> float: cohesion/lexicon score (higher is better).\n",
    "\n",
    "    Args:\n",
    "      text: Input string (ideally NFC-normalized and control-cleaned).\n",
    "      _TONE: String of tone marks to match.\n",
    "      margin: Score delta needed to accept the rewrite (keeps behavior conservative).\n",
    "\n",
    "    Returns:\n",
    "      The text with safe “[tone]ำน” → “[tone]าน” corrections applied.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> _fix_tone_am_n(\"ด้ำนข้าง\")\n",
    "    'ด้านข้าง'\n",
    "    >>> _fix_tone_am_n(\"ทัำน\")  # if scoring says it's not an improvement\n",
    "    'ทัำน'\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    last = 0\n",
    "\n",
    "    # Compile matcher *per call* so _TONE can be overridden safely.\n",
    "    # Pattern: ([tone])ำน  → captures any tone, followed by sara-am, then น\n",
    "    TONE_AM_N = re.compile(rf\"([{re.escape(_TONE)}])ำน\")\n",
    "\n",
    "    for a, b in _thai_spans(text):\n",
    "        # Emit non-Thai prefix unchanged\n",
    "        out.append(text[last:a])\n",
    "\n",
    "        # Work inside the Thai-only slice\n",
    "        span = text[a:b]\n",
    "        i = 0\n",
    "\n",
    "        while True:\n",
    "            m = TONE_AM_N.search(span, i)\n",
    "            if not m:\n",
    "                break\n",
    "\n",
    "            # Index (within span) of the 'ำ' we might change.\n",
    "            # Match is [tone][ำ][น], so 'ำ' is at start+1.\n",
    "            j = m.start(0) + 1\n",
    "\n",
    "            # Score the original span vs. a trial where that 'ำ' → 'า' (keeping the trailing 'น').\n",
    "            base_score = _score_thai(span, BOOST_AM_SET)\n",
    "            trial = span[:j] + \"า\" + span[j+1:]\n",
    "            trial_score = _score_thai(trial, BOOST_AM_SET)\n",
    "\n",
    "            # Accept only if the trial wins by at least `margin`\n",
    "            if trial_score > base_score + margin:\n",
    "                span = trial\n",
    "                # After editing, rewind a bit to catch overlapping/adjacent candidates\n",
    "                # e.g., '... ้ำนน ...' where the first change reveals the next one.\n",
    "                i = max(0, j - 2)\n",
    "            else:\n",
    "                # Skip past this match to avoid infinite loops\n",
    "                i = m.end(0)\n",
    "\n",
    "        # Emit the possibly-edited Thai span\n",
    "        out.append(span)\n",
    "        last = b\n",
    "\n",
    "    # Emit the trailing non-Thai segment\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "# Precompile frequently-used regexes for Step 3 and Step 5.\n",
    "_RX_ATTACH_MARK = re.compile(rf\"({THAI}){W}+({MARK})\")\n",
    "_RX_ATTACH_TONE = re.compile(rf\"({THAI}){W}+({TONE})\")\n",
    "_RX_SPACES      = re.compile(r\"[ \\t]{2,}\")          # collapse runs of spaces/tabs\n",
    "_RX_AROUND_NL   = re.compile(r\"[ \\t]*\\n[ \\t]*\")     # trim spaces around newlines\n",
    "\n",
    "def repair_thai(\n",
    "    text: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Normalize Thai text with a multi-stage pipeline.\n",
    "\n",
    "    Pipeline:\n",
    "      1) Clean controls / NFC:\n",
    "           - Remove zero-width/control chars; normalize Unicode form.\n",
    "      2) Merge split 'ำ' (structural):\n",
    "           - Fuse 'า' + ํ (both orders) into ำ without guessing.\n",
    "      3) Reattach marks/tones to the LEFT base:\n",
    "           - Remove stray gaps so combining marks/tones sit on their base letter.\n",
    "      4) Statistical 'า' → 'ำ' upgrades:\n",
    "           - Use `_try_am_variants` (lexicon + cohesion scoring) to fix likely dropped ำ.\n",
    "      5) Tidy spaces/newlines:\n",
    "           - Collapse extra spaces; strip spaces around line breaks.\n",
    "      6) Targeted cleanup:\n",
    "           - `_fix_tone_am_n` rewrites “[tone]ำน” → “[tone]าน” if the score improves.\n",
    "\n",
    "    Args:\n",
    "        text: Raw Thai text (may contain OCR/PDF artifacts).\n",
    "        THAI_LEXICON: Iterable of valid Thai words (converted to a set internally).\n",
    "\n",
    "    Returns:\n",
    "        Cleaned and normalized Thai text (str).\n",
    "    \"\"\"\n",
    "    # Convert once for O(1) membership checks in downstream steps.\n",
    "    lex = set(THAI_LEXICON)\n",
    "\n",
    "    # 1) Clean controls / NFC normalization\n",
    "    t = _clean_controls(text)\n",
    "\n",
    "    # 2) Merge split 'ำ' structurally (no heuristics; only when ํ present)\n",
    "    t = _fuse_split_am(t)\n",
    "\n",
    "    # 3) Reattach combining marks/tones to the immediate LEFT base (within a word)\n",
    "    #    Examples: \"ก \\u200B ั\" → \"กั\",  \"ก  ้\" → \"ก้\"\n",
    "    t = _RX_ATTACH_MARK.sub(r\"\\1\\2\", t)\n",
    "    t = _RX_ATTACH_TONE.sub(r\"\\1\\2\", t)\n",
    "\n",
    "    # 4) Statistical 'า' -> 'ำ' (line-by-line to avoid cross-line scoring bleed)\n",
    "    lines = [_try_am_variants(ln) for ln in t.split(\"\\n\")]\n",
    "    out = \"\\n\".join(lines)\n",
    "\n",
    "    # 5) Tidy spaces/newlines (non-destructive)\n",
    "    out = _RX_SPACES.sub(\" \", out)      # collapse runs of spaces/tabs\n",
    "    out = _RX_AROUND_NL.sub(\"\\n\", out)  # normalize spaces around newlines\n",
    "\n",
    "    # 6) Targeted tone+ำ+น fix (kept conservative via margin inside the helper)\n",
    "    out = _fix_tone_am_n(out, _TONE=_TONE)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b839be50-a6a5-4bfd-8cf8-75444f3104a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: intraword space fixer\n",
    "def _fix_intraword_spaces(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Repair stray spaces inside Thai words using a lexicon check.\n",
    "\n",
    "    Motivation:\n",
    "      OCR/PDF often inserts spaces inside single Thai words:\n",
    "        - 'ส่ วน'  → 'ส่วน'\n",
    "        - 'เปิ ด'  → 'เปิด'\n",
    "        - 'ปุ่ ม'  → 'ปุ่ม'\n",
    "\n",
    "    Strategy:\n",
    "      1. Work inside Thai spans only (ignore non-Thai context).\n",
    "      2. Iteratively merge adjacent tokens if their concatenation\n",
    "         is a known word in THAI_LEXICON.\n",
    "      3. Additionally, pull tone/mark diacritics across one stray\n",
    "         space (e.g. 'เปิ ด' → 'เปิด').\n",
    "\n",
    "    Args:\n",
    "        text (str): Input Thai text (zero-width spaces already stripped).\n",
    "\n",
    "    Returns:\n",
    "        str: Thai text with repaired intraword spacing.\n",
    "    \"\"\"\n",
    "\n",
    "    def fix_span(span: str) -> str:\n",
    "        \"\"\"\n",
    "        Within a Thai-only span:\n",
    "        - Split on normal spaces.\n",
    "        - Iteratively merge adjacent tokens if the merged form\n",
    "          is found in THAI_LEXICON.\n",
    "        \"\"\"\n",
    "        parts = span.split(\" \")\n",
    "        if len(parts) == 1:\n",
    "            return span\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            i = 0\n",
    "            merged_parts = []\n",
    "            while i < len(parts):\n",
    "                if i + 1 < len(parts):\n",
    "                    candidate = parts[i] + parts[i + 1]\n",
    "                    if candidate in THAI_LEXICON:\n",
    "                        merged_parts.append(candidate)\n",
    "                        i += 2\n",
    "                        changed = True\n",
    "                        continue\n",
    "                # fallback: keep current token\n",
    "                merged_parts.append(parts[i])\n",
    "                i += 1\n",
    "            parts = merged_parts\n",
    "\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    # --- Main repair loop ---\n",
    "    res = []\n",
    "    last = 0\n",
    "    # Find Thai spans with at least one space inside (pattern: THAI + W + THAI)\n",
    "    for m in re.finditer(rf\"{THAI}+(?:{W}+{THAI}+)+\", text):\n",
    "        res.append(text[last:m.start()])         # keep non-Thai prefix\n",
    "        res.append(fix_span(text[m.start():m.end()]))\n",
    "        last = m.end()\n",
    "    res.append(text[last:])                      # trailing suffix\n",
    "    fixed = \"\".join(res)\n",
    "\n",
    "    # Extra rule: pull tone/mark over a single space\n",
    "    # e.g., 'เปิ ด' (mark separated by space) → 'เปิด'\n",
    "    fixed = re.sub(rf\"({THAI}{MARK}|{THAI}{TONE}){W}+({THAI})\", r\"\\1\\2\", fixed)\n",
    "\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19f826e6-d933-426e-b3f4-dd2e245ab38f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Adding Short AM and remove the wrong one\n",
    "def _newmm_tokens_with_spans(s: str):\n",
    "    \"\"\"\n",
    "    Tokenize a string using PyThaiNLP's `newmm` engine and return spans.\n",
    "\n",
    "    Each result is a tuple: (start_index, end_index, token).\n",
    "\n",
    "    Why spans?\n",
    "      • They allow mapping character offsets back to tokens (e.g., for aligning OCR fixes).\n",
    "      • Useful when you want to know which token corresponds to a given edit position.\n",
    "\n",
    "    Args:\n",
    "        s (str): Input Thai text (should be NFC-normalized for reliability).\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int, int, str]]: List of (start, end, token) in left-to-right order.\n",
    "    \"\"\"\n",
    "    # Tokenize with PyThaiNLP newmm segmenter\n",
    "    toks = word_tokenize(s, engine=\"newmm\")\n",
    "\n",
    "    spans = []\n",
    "    i = 0\n",
    "    for t in toks:\n",
    "        # Try to find token starting at or after i\n",
    "        j = s.find(t, i)\n",
    "        if j == -1:\n",
    "            # Fallback: if `.find()` fails (rare after normalization),\n",
    "            # assume token starts at current pointer.\n",
    "            j = i\n",
    "        spans.append((j, j + len(t), t))\n",
    "        # Advance pointer past this token\n",
    "        i = j + len(t)\n",
    "\n",
    "    return spans\n",
    "\n",
    "def _promote_short_aa_to_am(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Promote 2-char Thai words of the form Xา → Xำ *only* when the promoted form is\n",
    "    whitelisted in SHORT_AM_OK (e.g., 'ทา'→'ทำ', 'นา'→'นำ').\n",
    "\n",
    "    Scope:\n",
    "      • Acts only inside Thai spans (via _thai_spans) so mixed-language text is untouched.\n",
    "      • Preserves spacing/punctuation outside spans.\n",
    "      • Safe to edit in-place because 'า'→'ำ' does not change token length.\n",
    "\n",
    "    Args:\n",
    "      text: Arbitrary text (preferably already NFC + control-cleaned).\n",
    "\n",
    "    Returns:\n",
    "      The text with whitelisted short Xา→Xำ promotions applied.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    last = 0\n",
    "\n",
    "    for a, b in _thai_spans(text):\n",
    "        # Copy the non-Thai prefix unchanged\n",
    "        out.append(text[last:a])\n",
    "\n",
    "        span = text[a:b]                       # Thai-only slice\n",
    "        toks = _newmm_tokens_with_spans(span)  # [(start, end, token)] left→right\n",
    "\n",
    "        if toks:\n",
    "            for ts, te, tok in toks:\n",
    "                # Only 2-char tokens ending with 'า'\n",
    "                if len(tok) == 2 and tok.endswith(\"า\"):\n",
    "                    x = tok[0]\n",
    "                    cand = x + \"ำ\"\n",
    "\n",
    "                    # Optional guard 1: first char should be a consonant\n",
    "                    # (avoid promoting leading-vowel forms like เา → เอำ)\n",
    "                    if not re.match(CONS, x):\n",
    "                        continue\n",
    "\n",
    "                    # Optional guard 2: don't \"fix\" if original is already a valid word\n",
    "                    # if tok in THAI_LEXICON:   # uncomment if you want this behavior\n",
    "                    #     continue\n",
    "\n",
    "                    # Promote only if whitelisted\n",
    "                    if cand in SHORT_AM_OK:\n",
    "                        # Safe replacement: same length → subsequent spans remain valid\n",
    "                        span = span[:ts] + cand + span[te:]\n",
    "\n",
    "        out.append(span)\n",
    "        last = b\n",
    "\n",
    "    # Trailing non-Thai tail\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "# Precompile once (assumes THAI is defined like r\"[\\u0E00-\\u0E7F]\")\n",
    "_THAI_WORD_RX = re.compile(rf\"{THAI}+\")\n",
    "_AM_LEADING_VOWEL_RX = re.compile(r\"ำ[เแโใไ]\")\n",
    "\n",
    "def _score_tokens(\n",
    "    tokens: Iterable[str],\n",
    "    token_penalty_weight: float = 0.5,  # cohesion penalty per token\n",
    ") -> Tuple[float, int, int]:\n",
    "    \"\"\"\n",
    "    Local cohesion score for a list of Thai tokens.\n",
    "\n",
    "    Scoring:\n",
    "      - + base for lexicon hits; smaller base for Thai-looking tokens\n",
    "      - + length bump for longer valid lexemes (capped)\n",
    "      - + bonus for plausible ำ-tokens (len>=3 or in BOOST_AM_SET)\n",
    "      - − penalty for 'ำ' before a leading vowel (intra- or cross-token)\n",
    "      - − cohesion penalty proportional to token count\n",
    "\n",
    "    Returns:\n",
    "      (total_score, dict_hits, am_hits)\n",
    "    \"\"\"\n",
    "    toks = list(tokens)\n",
    "    n = len(toks)\n",
    "    if n == 0:\n",
    "        return 0.0, 0, 0\n",
    "\n",
    "    total = 0.0\n",
    "    dict_hits = 0\n",
    "    am_hits = 0\n",
    "\n",
    "    for i, t in enumerate(toks):\n",
    "        is_thai_word = _THAI_WORD_RX.fullmatch(t) is not None\n",
    "        in_lex = (t in THAI_LEXICON)\n",
    "\n",
    "        # Base weights\n",
    "        base = 0.6 if in_lex else (0.15 if is_thai_word else 0.0)\n",
    "        if in_lex:\n",
    "            dict_hits += 1\n",
    "\n",
    "        # Length bump for valid lexemes (cap at 12)\n",
    "        length_bump = (min(len(t), 12) * 0.03) if in_lex else 0.0\n",
    "\n",
    "        # Plausible-ำ bonus: only when it's clearly a real merged token\n",
    "        am_bonus = 0.0\n",
    "        if (\"ำ\" in t) and ((in_lex and len(t) >= 3) or (t in BOOST_AM_SET)):\n",
    "            am_bonus = 3.0\n",
    "            am_hits += 1\n",
    "\n",
    "        total += base + length_bump + am_bonus\n",
    "\n",
    "        # Suspicion penalties\n",
    "        # Cross-token: token ends with 'ำ' then next starts with a leading vowel\n",
    "        if t.endswith(\"ำ\") and i + 1 < n:\n",
    "            nxt = toks[i + 1]\n",
    "            if nxt and nxt[0] in LEADING_VOWEL:\n",
    "                total -= 1.5\n",
    "\n",
    "        # Intra-token: 'ำ' immediately followed by a leading vowel inside the same token\n",
    "        if _AM_LEADING_VOWEL_RX.search(t):\n",
    "            total -= 1.5\n",
    "\n",
    "    # Cohesion penalty: fewer tokens preferred\n",
    "    total -= token_penalty_weight * n\n",
    "    return total, dict_hits, am_hits\n",
    "    \n",
    "def _demote_false_am_dynamic(\n",
    "    text: str,\n",
    "    win_radius: int = 6,           # tokens left/right to include in scoring window\n",
    "    improve_margin: float = 0.6,   # required score gain to accept demotion\n",
    "    am_exceptions: set[str] = frozenset({\"คำ\", \"กำ\"}),  # 2-char am words allowed to demote\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Demote suspicious 'ำ' → 'า' **inside tokens only** (newmm spans).\n",
    "\n",
    "    Rules:\n",
    "      • Never demote: tokens in SHORT_AM_OK, tokens in THAI_LEXICON (any length) unless in `am_exceptions`,\n",
    "        and tokens in BOOST_AM_SET.\n",
    "      • Try a demotion per token (all 'ำ'→'า' at once by default), accept only if local cohesion improves\n",
    "        by `improve_margin`.\n",
    "      • Reject if the trial creates any cross-token '…ำ' + leading vowel pattern.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    last = 0\n",
    "\n",
    "    for a, b in _thai_spans(text):\n",
    "        out.append(text[last:a])\n",
    "        span = text[a:b]\n",
    "\n",
    "        tok_spans = _newmm_tokens_with_spans(span)\n",
    "        if not tok_spans:\n",
    "            out.append(span); last = b; continue\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            # iterate over a snapshot; we may mutate and then recompute tok_spans\n",
    "            for idx, (ts, te, tok) in enumerate(tok_spans):\n",
    "                if \"ำ\" not in tok:\n",
    "                    continue\n",
    "\n",
    "                # --- Protection rules ---\n",
    "                if tok in SHORT_AM_OK or tok in BOOST_AM_SET:\n",
    "                    continue\n",
    "                # protect *all* lexicon words unless explicitly whitelisted to demote\n",
    "                if tok in THAI_LEXICON and tok not in am_exceptions:\n",
    "                    continue\n",
    "\n",
    "                # Candidate: demote all 'ำ' inside this token (coarse but cheap).\n",
    "                cand_tok = tok.replace(\"ำ\", \"า\")\n",
    "                if cand_tok == tok:\n",
    "                    continue\n",
    "\n",
    "                # Build scoring window around current token\n",
    "                L = max(0, idx - win_radius)\n",
    "                R = min(len(tok_spans), idx + win_radius + 1)\n",
    "                base_tokens  = [t for _, _, t in tok_spans[L:R]]\n",
    "                trial_tokens = base_tokens[:]\n",
    "                trial_tokens[idx - L] = cand_tok\n",
    "\n",
    "                # Cross-boundary guard after change: avoid '…ำ' + leading vowel\n",
    "                bad_follow = any(\n",
    "                    trial_tokens[j].endswith(\"ำ\")\n",
    "                    and j + 1 < len(trial_tokens)\n",
    "                    and trial_tokens[j + 1]\n",
    "                    and trial_tokens[j + 1][0] in LEADING_VOWEL\n",
    "                    for j in range(len(trial_tokens))\n",
    "                )\n",
    "                if bad_follow:\n",
    "                    continue\n",
    "\n",
    "                # Score acceptance\n",
    "                base_score, _, _   = _score_tokens(base_tokens)\n",
    "                trial_score, _, _  = _score_tokens(trial_tokens)\n",
    "\n",
    "                if trial_score > base_score + improve_margin:\n",
    "                    # Apply demotion at exact character positions\n",
    "                    span = span[:ts] + cand_tok + span[te:]\n",
    "                    # Recompute token spans and restart detection\n",
    "                    tok_spans = _newmm_tokens_with_spans(span)\n",
    "                    changed = True\n",
    "                    break  # restart outer while-loop\n",
    "\n",
    "        out.append(span)\n",
    "        last = b\n",
    "\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "def _all_tokens_lexical(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if newmm tokenization of s covers the whole string (no gaps/overlap)\n",
    "    and every token is in THAI_LEXICON (or BOOST_AM_SET/SHORT_AM_OK).\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    toks = word_tokenize(s, engine=\"newmm\")\n",
    "\n",
    "    # exact cover check\n",
    "    if \"\".join(toks) != s:\n",
    "        return False\n",
    "\n",
    "    LEX_OK = THAI_LEXICON | BOOST_AM_SET | SHORT_AM_OK\n",
    "    return all(t in LEX_OK for t in toks)\n",
    "\n",
    "def _dynamic_am_a_repair_tokenwise(\n",
    "    text: str,\n",
    "    window_tokens: int = 12,      # max tokens in scoring window (centered on current token)\n",
    "    improve_margin: float = 0.6,  # min score gain to accept a change\n",
    "    boost_if_seg: bool = True,    # give a small nudge if candidate’s segmentation is fully lexical\n",
    "    seg_nudge: float = 0.25,      # that small nudge\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Token-wise, position-wise 'ำ' <-> 'า' repair that preserves spaces.\n",
    "    Uses newmm token spans. Only edits NON-lexicon tokens, tries single-position\n",
    "    swaps, and accepts only if cohesion improves by a margin and guards pass.\n",
    "\n",
    "    Upgrades:\n",
    "    - SHORT 2-char aa->am *fast path* when the swapped form is in SHORT_AM_OK\n",
    "      (e.g., 'ทา'→'ทำ', 'นา'→'น้ำ').\n",
    "    - Accept aa->am if the candidate is a single lexicon word OR its\n",
    "      newmm segmentation is fully lexical (e.g., 'บำรุง' + 'รักษา').\n",
    "      # If you truly want “NOT a single lexeme” then require seg-only:\n",
    "      #   single_ok = False\n",
    "      #   seg_ok = _all_tokens_lexical(cand)\n",
    "    \"\"\"\n",
    "\n",
    "    # Precompile Thai fullmatch for speed & correctness\n",
    "    THAI_PLUS_RX = re.compile(rf\"{THAI}+\")\n",
    "    def _is_thai_word(tok: str) -> bool:\n",
    "        return THAI_PLUS_RX.fullmatch(tok) is not None\n",
    "\n",
    "    def single_position_variants(tok: str):\n",
    "        \"\"\"Yield (variant, idx, kind) for single-position swaps only.\"\"\"\n",
    "        # Try am→aa (demotion) per occurrence; avoid 2-char legit words by length guard\n",
    "        for i, ch in enumerate(tok):\n",
    "            if ch == \"ำ\" and len(tok) >= 3:\n",
    "                yield tok[:i] + \"า\" + tok[i+1:], i, \"am2aa\"\n",
    "        # Try aa→am (promotion) per occurrence\n",
    "        for i, ch in enumerate(tok):\n",
    "            if ch == \"า\":\n",
    "                yield tok[:i] + \"ำ\" + tok[i+1:], i, \"aa2am\"\n",
    "\n",
    "    out = []\n",
    "    last = 0\n",
    "\n",
    "    for a, b in _thai_spans(text):\n",
    "        out.append(text[last:a])\n",
    "        span = text[a:b]\n",
    "\n",
    "        tok_spans = _newmm_tokens_with_spans(span)  # [(start,end,token)]\n",
    "        if not tok_spans:\n",
    "            out.append(span); last = b; continue\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "\n",
    "            # Iterate over a snapshot; we may rewrite span and refresh tok_spans\n",
    "            for idx, (ts, te, tok) in enumerate(tok_spans):\n",
    "                if not tok or not _is_thai_word(tok):\n",
    "                    continue\n",
    "\n",
    "                # Hard protections: skip already-good/important tokens\n",
    "                if tok in SHORT_AM_OK or tok in BOOST_AM_SET or (tok in THAI_LEXICON and len(tok) >= 2):\n",
    "                    continue\n",
    "\n",
    "                # ---- 2-char SHORT fast path: aa->am if swapped ∈ SHORT_AM_OK ----\n",
    "                if len(tok) == 2 and \"า\" in tok:\n",
    "                    cand_fast = tok.replace(\"า\", \"ำ\")\n",
    "                    if cand_fast in SHORT_AM_OK:\n",
    "                        # guard: avoid suspicious 'ำ' + leading vowel sequences\n",
    "                        if not re.search(r\"ำ[เแโใไ]\", cand_fast):\n",
    "                            span = span[:ts] + cand_fast + span[te:]\n",
    "                            tok_spans = _newmm_tokens_with_spans(span)\n",
    "                            changed = True\n",
    "                            break  # restart from fresh spans\n",
    "\n",
    "                # Build scoring window around the current token\n",
    "                half = max(1, window_tokens // 2)\n",
    "                L = max(0, idx - half)\n",
    "                R = min(len(tok_spans), idx + half + 1)\n",
    "                base_tokens = [t for _, _, t in tok_spans[L:R]]\n",
    "                base_score, _, _ = _score_tokens(base_tokens)\n",
    "\n",
    "                best_gain = 0.0\n",
    "                best_variant = None\n",
    "\n",
    "                for cand, pos, kind in single_position_variants(tok):\n",
    "                    # Acceptance rules for aa->am (promotion)\n",
    "                    if kind == \"aa2am\":\n",
    "                        single_ok = ((cand in THAI_LEXICON and (len(cand) >= 3 or cand in SHORT_AM_OK))\n",
    "                                     or (cand in BOOST_AM_SET))\n",
    "                        seg_ok = _all_tokens_lexical(cand)\n",
    "                        if not (single_ok or seg_ok):\n",
    "                            continue\n",
    "                    else:\n",
    "                        # am2aa (demotion): only consider because original token is non-lexical already\n",
    "                        # (no extra pre-filter beyond global guards)\n",
    "                        pass\n",
    "\n",
    "                    # Cross-token guard after change: avoid '…ำ' + leading vowel\n",
    "                    trial_tokens = base_tokens[:]\n",
    "                    trial_tokens[idx - L] = cand\n",
    "                    bad_follow = any(\n",
    "                        trial_tokens[j].endswith(\"ำ\")\n",
    "                        and j + 1 < len(trial_tokens)\n",
    "                        and trial_tokens[j + 1]\n",
    "                        and trial_tokens[j + 1][0] in LEADING_VOWEL\n",
    "                        for j in range(len(trial_tokens))\n",
    "                    )\n",
    "                    # Also reject if the candidate itself ends with 'ำ' and the next token starts with a leading vowel\n",
    "                    if bad_follow or re.search(r\"ำ[เแโใไ]\", cand):\n",
    "                        continue\n",
    "\n",
    "                    # Vowel/tone profile guard: keep overall vowel shape (ignoring tones; treat ำ≈า)\n",
    "                    if _vt_profile(tok) != _vt_profile(cand):\n",
    "                        continue\n",
    "\n",
    "                    trial_score, _, _ = _score_tokens(trial_tokens)\n",
    "                    gain = trial_score - base_score\n",
    "\n",
    "                    # Optional small nudge for segmentation-based acceptance (helps 'บำรุงรักษา')\n",
    "                    if boost_if_seg and kind == \"aa2am\" and (cand not in THAI_LEXICON and cand not in BOOST_AM_SET) and _all_tokens_lexical(cand):\n",
    "                        gain += seg_nudge\n",
    "\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_variant = cand\n",
    "\n",
    "                # Commit only if we beat the margin\n",
    "                if best_variant and best_gain > improve_margin:\n",
    "                    span = span[:ts] + best_variant + span[te:]\n",
    "                    tok_spans = _newmm_tokens_with_spans(span)  # re-tokenize\n",
    "                    changed = True\n",
    "                    break  # restart scanning with refreshed spans\n",
    "\n",
    "        out.append(span)\n",
    "        last = b\n",
    "\n",
    "    out.append(text[last:])\n",
    "    return \"\".join(out)\n",
    "\n",
    "def fixing_am_dynamic(\n",
    "    text: str,\n",
    "    # pass through knobs to the deeper routines when you want to tune\n",
    "    token_window: int = 12,\n",
    "    improve_margin: float = 0.6,\n",
    "    run_demoter: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Combined fixer for Thai 'ำ' vs 'า' confusions (space-preserving).\n",
    "\n",
    "    Steps:\n",
    "      1) Promote safe 2-char SHORT_AM_OK tokens (e.g., 'ทา'→'ทำ', 'นา'→'น้ำ').\n",
    "         - Cheap, conservative upgrades; do these first so later passes see better context.\n",
    "      2) Demote suspicious 'ำ' → 'า' when local cohesion improves and guards pass.\n",
    "         - Optional; leave on by default. Comes before step 3 to reduce\n",
    "           obviously bad “ำ” and avoid oscillations.\n",
    "      3) Token-wise single-position repair ('ำ'↔'า') with scoring + guards.\n",
    "         - Handles remaining tricky spots one character at a time.\n",
    "\n",
    "    Returns:\n",
    "      str: Normalized Thai string with safer 'ำ'/'า' usage.\n",
    "    \"\"\"\n",
    "    # 1) Safe promotions first\n",
    "    out = _promote_short_aa_to_am(text)\n",
    "\n",
    "    # (perf) short-circuit if unchanged\n",
    "    if out is text:\n",
    "        # Still optional demoter can catch false 'ำ' in untouched strings\n",
    "        pass\n",
    "\n",
    "    # 2) Conservative demotions\n",
    "    if run_demoter:\n",
    "        new_out = _demote_false_am_dynamic(out)\n",
    "        if new_out != out:\n",
    "            out = new_out\n",
    "\n",
    "    # 3) Single-position token-wise repair (both directions with scoring)\n",
    "    out2 = _dynamic_am_a_repair_tokenwise(\n",
    "        out,\n",
    "        window_tokens=token_window,\n",
    "        improve_margin=improve_margin,\n",
    "    )\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf87650a-8288-4d47-af73-9756174328e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "THAI_WORD_RUN = re.compile(r\"[\\u0E00-\\u0E7F]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d798da36-08fb-4b44-b372-8e9b1c86588a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Step 7 Lexicon repair \n",
    "# def _thai_token_spans(s: str):\n",
    "#     \"\"\"Yield (start, end, token_text) for each contiguous Thai run in s.\"\"\"\n",
    "#     return [(m.start(), m.end(), m.group(0)) for m in THAI_WORD_RUN.finditer(s)]\n",
    "\n",
    "# @lru_cache(maxsize=10000)\n",
    "# def _am_aa_edits1(token: str):\n",
    "#     \"\"\"\n",
    "#     1‑edit alternatives by toggling exactly ONE occurrence of 'ำ'↔'า'.\n",
    "#     No insert/delete/other marks. Always returns a set.\n",
    "#     \"\"\"\n",
    "#     if not token or not isinstance(token, str):\n",
    "#         return set()\n",
    "#     outs = set()\n",
    "#     for i, ch in enumerate(token):\n",
    "#         if ch == \"ำ\":\n",
    "#             outs.add(token[:i] + \"า\" + token[i+1:])\n",
    "#         elif ch == \"า\":\n",
    "#             outs.add(token[:i] + \"ำ\" + token[i+1:])\n",
    "#     return outs\n",
    "\n",
    "# @lru_cache(maxsize=10000)\n",
    "# def _lex_candidates1(token: str):\n",
    "#     \"\"\"\n",
    "#     Lexicon candidates limited to am/aa toggles and filtered by vowel profile.\n",
    "#     - keep only candidates in lexicon/BOOST/SHORT list\n",
    "#     - require _vt_profile(candidate) == _vt_profile(original)  (except the am→aa normalization)\n",
    "#     \"\"\"\n",
    "#     if not token or not isinstance(token, str):\n",
    "#         return set()\n",
    "#     base_prof = _vt_profile(token)\n",
    "#     cands = set()\n",
    "#     for cand in _am_aa_edits1(token):\n",
    "#         if _vt_profile(cand) != base_prof:\n",
    "#             continue\n",
    "#         if cand in THAI_LEXICON or cand in BOOST_AM_SET or cand in SHORT_AM_OK:\n",
    "#             cands.add(cand)\n",
    "#     return cands\n",
    "    \n",
    "# def _lexicon_repair_dynamic(text: str, *, max_token_len: int = 16, window_tokens: int = 12) -> str:\n",
    "#     \"\"\"\n",
    "#     Safer 1‑edit lexicon repair (am/aa only) that preserves original spacing.\n",
    "#     Never changes a token already in the lexicon/BOOST/SHORT sets.\n",
    "#     Only accepts if local cohesion improves by a solid margin.\n",
    "#     \"\"\"\n",
    "#     out = []\n",
    "#     last = 0\n",
    "#     for a, b in _thai_spans(text):\n",
    "#         out.append(text[last:a])\n",
    "#         span = text[a:b]\n",
    "\n",
    "#         tok_spans = _thai_token_spans(span)\n",
    "#         if not tok_spans:\n",
    "#             out.append(span); last = b; continue\n",
    "\n",
    "#         i = 0\n",
    "#         changed = False\n",
    "#         while i < len(tok_spans):\n",
    "#             ts, te, tok = tok_spans[i]\n",
    "\n",
    "#             # Skip OK/short/long tokens or non-Thai\n",
    "#             if (not tok or not isinstance(tok, str)\n",
    "#                 or len(tok) < 2 or len(tok) > max_token_len\n",
    "#                 or not re.match(THAI, tok)\n",
    "#                 or tok in THAI_LEXICON or tok in BOOST_AM_SET or tok in SHORT_AM_OK):\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             cands = _lex_candidates1(tok)\n",
    "#             if not cands:\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             # Score a local window\n",
    "#             L = max(0, i - window_tokens//2)\n",
    "#             R = min(len(tok_spans), i + window_tokens//2 + 1)\n",
    "#             base_tokens = [t for _,_,t in tok_spans[L:R]]\n",
    "#             base_score = _score_tokens(base_tokens)\n",
    "\n",
    "#             best_score = base_score\n",
    "#             best_cand = None\n",
    "\n",
    "#             for cand in cands:\n",
    "#                 trial_tokens = base_tokens[:]\n",
    "#                 trial_tokens[i - L] = cand\n",
    "\n",
    "#                 # Reject suspicious '…ำ' followed by leading vowel across boundary\n",
    "#                 bad_follow = any(\n",
    "#                     trial_tokens[j].endswith(\"ำ\")\n",
    "#                     and j+1 < len(trial_tokens)\n",
    "#                     and trial_tokens[j+1]\n",
    "#                     and trial_tokens[j+1][0] in LEADING_VOWEL\n",
    "#                     for j in range(len(trial_tokens))\n",
    "#                 )\n",
    "#                 if bad_follow:\n",
    "#                     continue\n",
    "\n",
    "#                 trial_score  = _score_tokens(trial_tokens)\n",
    "#                 # stronger margin stops flips like เปิด→เป็ด\n",
    "#                 if trial_score > best_score + 0.8:\n",
    "#                     best_score = trial_score\n",
    "#                     best_cand = cand\n",
    "\n",
    "#             if best_cand is not None:\n",
    "#                 span = span[:ts] + best_cand + span[te:]\n",
    "#                 changed = True\n",
    "#                 tok_spans = _thai_token_spans(span)\n",
    "#                 i = max(0, i - 1)\n",
    "#                 continue\n",
    "\n",
    "#             i += 1\n",
    "\n",
    "#         out.append(span)\n",
    "#         last = b\n",
    "\n",
    "#     out.append(text[last:])\n",
    "#     return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9df229e2-83b0-422d-8d36-5735c755b75e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prev_md_at_each_step(*args, prev_length=800, prev=True):\n",
    "    \"\"\"\n",
    "    Print a preview of each step's markdown/text.\n",
    "\n",
    "    Args:\n",
    "        *args:  The text values at each step (md1, md2, ...).\n",
    "        prev_length (int): How many characters to show from each string.\n",
    "        prev (bool): If False, nothing is printed.\n",
    "    \"\"\"\n",
    "    if not prev:\n",
    "        return\n",
    "\n",
    "    for i, arg in enumerate(args):\n",
    "        snippet = arg[:prev_length] + (\"...\" if len(arg) > prev_length else \"\")\n",
    "        print(f\"md{i+1}: {snippet}\")\n",
    "        print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8beee819-eaad-440d-95e9-8ca70d8dd10a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 0) Extract embedded text with Docling (no OCR)\n",
    "    pipeline = PdfPipelineOptions(do_ocr=False)\n",
    "    conv = DocumentConverter(format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline)})\n",
    "    res = conv.convert(str(PDF_PATH))\n",
    "    md = res.document.export_to_markdown()\n",
    "\n",
    "    # 2) PyMuPDF‑guided ligature repairs\n",
    "    md2, n2 = apply_pymupdf_guided_repairs(md, PDF_PATH)\n",
    "    if n2:\n",
    "        print(f\"[Step 2] PyMuPDF‑guided ligature repairs applied: {n2} replacements\")\n",
    "    else:\n",
    "        print(\"[Step 2] No PyMuPDF‑guided ligature repairs applied\")\n",
    "\n",
    "    # 3) Safe normalization fallback\n",
    "    md3, n1 = normalize_thai(md2)\n",
    "    if md3 != md2:\n",
    "        if n1:\n",
    "            print(f\"[Step 3] Normalization fixes applied: {n1} occurrences\")\n",
    "        else:\n",
    "            print(\"[Step 3] Normalization fixes applied (count not estimated)\")\n",
    "    else:\n",
    "        print(\"[Step 3] Normalization not needed\")\n",
    "\n",
    "    # 4) Statistical “ำ” fixer\n",
    "    md4 = repair_thai(md3)\n",
    "    if md4 != md3:\n",
    "        print(f\"[Step 4] Statistical fixes applied\")\n",
    "    else:\n",
    "        print(\"[Step 4] Statistical Fixer not needed\")\n",
    "        \n",
    "    # 5) Intraword space fixer\n",
    "    md5 = _fix_intraword_spaces(md4)\n",
    "    if md5 != md4:\n",
    "        print(f\"[Step 5] Intraword space fixer applied\")\n",
    "    else:\n",
    "        print(\"[Step 5] Intraword space Fixer not needed\")\n",
    "\n",
    "    # 6) fixing AM dynamic\n",
    "    md6 = fixing_am_dynamic(md5)\n",
    "    if md6 != md5:\n",
    "        print(f\"[Step 6] AM dynamic fixer applied\")\n",
    "    else:\n",
    "        print(\"[Step 6] AM dynamic Fixer not needed\")\n",
    "        \n",
    "    # # 7) fixing lexicon\n",
    "    # md7 = _lexicon_repair_dynamic(md6)\n",
    "    # if md7 != md6:\n",
    "    #     print(f\"[Step 7] Lexicon fixer applied\")\n",
    "    # else:\n",
    "    #     print(\"[Step 7] Lexicon Fixer not needed\")\n",
    "\n",
    "    # (Optional) Fill image placeholders\n",
    "    md10 = fill_image_tags_with_best_quality(md6, PDF_PATH, prompt=VLM_PROMPT)\n",
    "\n",
    "    OUT_MD.write_text(md10, encoding=\"utf-8\")\n",
    "    log.info(\"✅ Wrote %s\", OUT_MD)\n",
    "    prev_md_at_each_step(md, md2, md3, md4, md5, md6, md10, prev_length=1000, prev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37e9aac2-a670-49dc-9b21-9a78d8272aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] PyMuPDF‑guided ligature repairs applied: 102 replacements\n",
      "[Step 3] Normalization not needed\n",
      "[Step 4] Statistical fixes applied\n",
      "[Step 5] Intraword space fixer applied\n",
      "[Step 6] AM dynamic fixer applied\n",
      "md1: ## คู่มือการใช้งานเครื่องชงกาแฟส านักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในส านักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่อง ไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่ วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนส าคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่ม และสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -ส าหรับใส่ เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -ส าหรับท าฟองนม เช่น คาปูชิโน่ ลาเต้\n",
      "- ถังน ้า ( Water Tank) -ส าหรับเก็บน ้าที่ใช้ชงกาแฟ\n",
      "- ถาดรองน ้าทิ้ง ( Water Tray) -ส าหรับรองน ้าล้นหรือ เศษน ้าจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิ ดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิ ดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่ มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ ถังน ้า มีน  ้ าเพียงพอก่อนชง\n",
      "- ท าความสะอาด ถาดรองน ้าทิ้ง และ ...\n",
      "==============================\n",
      "md2: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่ วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สาหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สาหรับทาฟองนมเช่น คาปูชิโน่ ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สาหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สาหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิ ดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิ ดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่ มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ ถังน้ำ มีน้ำเพียงพอก่อนชง\n",
      "- ทาความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็ นประจำ\n",
      "- ใช...\n",
      "==============================\n",
      "md3: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่ วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สาหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สาหรับทาฟองนมเช่น คาปูชิโน่ ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สาหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สาหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิ ดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิ ดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่ มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ ถังน้ำ มีน้ำเพียงพอก่อนชง\n",
      "- ทาความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็ นประจำ\n",
      "- ใช...\n",
      "==============================\n",
      "md4: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่ วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สำหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สำหรับทาฟองนมเช่น คาปูชิโน่ ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สำหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สำหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิ ดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิ ดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่ มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ ถังน้ำ มีน้ำเพียงพอก่อนชง\n",
      "- ทาความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็ นประจำ\n",
      "- ใช...\n",
      "==============================\n",
      "md5: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สำหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สำหรับทาฟองนมเช่น คาปูชิโน่ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สำหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สำหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ถังน้ำมีน้ำเพียงพอก่อนชง\n",
      "- ทาความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็นประจำ\n",
      "- ใช้เมล็ดกา...\n",
      "==============================\n",
      "md6: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สำหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สำหรับทำฟองนมเช่น คาปูชิโน่ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สำหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สำหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ถังน้ำมีน้ำเพียงพอก่อนชง\n",
      "- ทำความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็นประจำ\n",
      "- ใช้เมล็ดกา...\n",
      "==============================\n",
      "md7: ## คู่มือการใช้งานเครื่องชงกาแฟสำนักงาน\n",
      "\n",
      "## 1. ภาพรวม\n",
      "\n",
      "คู่มือนี้อธิบายขั้นตอนการใช้งานเครื่องชงกาแฟในสำนักงาน ตั้งแต่การรู้จัก ส่วนประกอบ การเตรียมเครื่องไปจนถึงการชงกาแฟ\n",
      "\n",
      "## 2. ส่วนประกอบของเครื่อง\n",
      "\n",
      "รู้จักกับส่วนสำคัญของเครื่องก่อนใช้งาน:\n",
      "\n",
      "- จอแสดงเมนู ( Menu Display) -แสดงตัวเลือกเครื่องดื่มและสถานะการชง\n",
      "- ช่องใส่เมล็ดกาแฟ ( Fill Coffee Bean) -สำหรับใส่เมล็ดกาแฟ\n",
      "- แก้วตีฟองนม ( Milk Frother) -สำหรับทำฟองนมเช่น คาปูชิโน่ลาเต้\n",
      "- ถังน้ำ ( Water Tank) -สำหรับเก็บน้ำที่ใช้ชงกาแฟ\n",
      "- ถาดรองน้ำทิ้ง ( Water Tray) -สำหรับรองน้ำล้นหรือเศษน้ำจากการชง\n",
      "\n",
      "## 3. ขั้นตอนการชงกาแฟ\n",
      "\n",
      "## 1. เติมเมล็ดกาแฟ\n",
      "\n",
      "- o เปิดช่องใส่เมล็ดกาแฟด้านบนเครื่อง\n",
      "- o ใส่เมล็ดกาแฟลงไป\n",
      "- o ปิดฝาให้สนิท\n",
      "\n",
      "## 2. เลือกเมนูเครื่องดื่ม\n",
      "\n",
      "- o ใช้จอแสดงเมนูเพื่อเลือกประเภทเครื่องดื่มที่ต้องการ\n",
      "- o เลือกกาแฟตามต้องการ\n",
      "\n",
      "## 3. เริ่มการชง\n",
      "\n",
      "- o กดปุ่มเริ่มชง ( Start/Brew)\n",
      "- o รอจนกระทั่งเครื่องชงเสร็จ\n",
      "\n",
      "## 4. เคล็ดลับเพื่อรสชาติกาแฟที่ดีที่สุด\n",
      "\n",
      "- ตรวจสอบให้ถังน้ำมีน้ำเพียงพอก่อนชง\n",
      "- ทำความสะอาด ถาดรองน้ำทิ้งและแก้วตีฟองนมเป็นประจำ\n",
      "- ใช้เมล็ดกา...\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fbe35-bc18-4d55-b451-61e7ebfe3dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ce3cb-ad1a-4e6b-beba-fabe7d1ba053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fb2b1cf-e165-4a0c-89dd-cc52f5dc898e",
   "metadata": {},
   "source": [
    "# ======================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a418b83-cb1e-4d04-9241-989fbd5936ac",
   "metadata": {},
   "source": [
    "# Testing “Docling Parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3833038e-2a1c-49c8-91ee-9f1a8f7f8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from docling_core.types.doc.page import TextCellUnit\n",
    "# from docling_parse.pdf_parser import DoclingPdfParser, PdfDocument\n",
    "\n",
    "# parser = DoclingPdfParser()\n",
    "\n",
    "# pdf_doc: PdfDocument = parser.load(\n",
    "#     path_or_stream=\"ExampleOCR/NLP-CV-NachaiLim.pdf\"\n",
    "# )\n",
    "\n",
    "# # PdfDocument.iterate_pages() will automatically populate pages as they are yielded.\n",
    "# for page_no, pred_page in pdf_doc.iterate_pages():\n",
    "\n",
    "#     # iterate over the word-cells\n",
    "#     for word in pred_page.iterate_cells(unit_type=TextCellUnit.WORD):\n",
    "#         print(word.rect, \": \", word.text)\n",
    "\n",
    "#         # create a PIL image with the char cells\n",
    "#     img = pred_page.render_as_image(cell_unit=TextCellUnit.CHAR)\n",
    "#     img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6769f64-7937-4586-b237-67ce36b3b8c8",
   "metadata": {},
   "source": [
    "# Testing \"SmolDocling\" (Not very good with Thai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edd78a2a-e838-4a15-a78c-f6a62d328276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from docling_core.types.doc import DoclingDocument\n",
    "# from docling_core.types.doc.document import DocTagsDocument\n",
    "# from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "# from transformers.image_utils import load_image\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load images\n",
    "# image = load_image(\"ExampleOCR/ocr_coffee2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07f1a99c-0096-4f50-a999-711af9832b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize processor and model\n",
    "# processor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\n",
    "# model = AutoModelForVision2Seq.from_pretrained(\n",
    "#     \"ds4sd/SmolDocling-256M-preview\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     _attn_implementation=\"eager\",  # for gpu that does not supports flash attention\n",
    "# ).to(DEVICE)\n",
    "\n",
    "\n",
    "# # Create input messages\n",
    "# prompt = \"extract the data with locations if available\"\n",
    "# prompt = \"ocr the given document, the text can be in English or Thai\"\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": prompt}\n",
    "#         ]\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # Prepare inputs\n",
    "# prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "# inputs = inputs.to(DEVICE)\n",
    "\n",
    "# # Generate outputs\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
    "# prompt_length = inputs.input_ids.shape[1]\n",
    "# trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
    "# doctags = processor.batch_decode(\n",
    "#     trimmed_generated_ids,\n",
    "#     skip_special_tokens=False,\n",
    "# )[0].lstrip()\n",
    "\n",
    "# # Populate document\n",
    "# doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
    "# print(doctags)\n",
    "# # create a docling document\n",
    "# doc = DoclingDocument(name=\"Document\")\n",
    "# doc.load_from_doctags(doctags_doc)\n",
    "\n",
    "# print(doc.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4405a0c-02d9-4564-a45d-e3df47be18bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
