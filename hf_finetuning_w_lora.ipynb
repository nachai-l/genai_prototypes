{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3d0de1-c614-4195-89db-a7bdda69160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0901 17:04:45.527000 94866 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841c2d8-234b-482b-80c3-39f5e1c64101",
   "metadata": {},
   "source": [
    "# Loading HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d82d69-f907-4f1e-b9b0-88a72d3f9fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebf641d-784f-4a0f-b60d-d62f08c51f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: What's wrong with you, Mr. Polly?\n",
      "#Person2#: What's wrong? I want a break from this horrible job.\n",
      "#Person1#: Then, buy a bottle of soft drink.\n",
      "#Person2#: Would you like to buy a bottle for me in the shop?\n",
      "#Person1#: It's a problem, because my boss is in that shop now.\n",
      "#Person2#: Ok, I will go there myself.\n",
      "#Person1#: Sorry, Mr. Polly.\n",
      "#Person2#: It doesn't matter. Oh, God, I have only four dollars in my wallet. Is that possible for me to buy one?\n",
      "#Person1#: Have a try.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Mr. Polly is tired and wants a break from work. #Person1# cannot buy a bottle of soft drink for him.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "#Person1#: Mum, have we got any fruit to take on the picnic? I thought there were some oranges and bananas.\n",
      "#Person2#: Here are the bananas. Let's take them. Oh the oranges have all gone. What about taking some grapes or apples as well?\n",
      "#Person1#: OK, let's take the grapes. That'll be enough.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# and #Person1#'s mother are preparing the fruits they are going to take to the picnic.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [90, 270]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f8809-24f8-4f7d-afcf-d84c1c84bac1",
   "metadata": {},
   "source": [
    "# Loading base hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d2c53b-c61a-45cd-9bac-cd78320907a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model_name='google/flan-t5-small'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32, \n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89e429-5f9b-48cd-bd69-f7fdaa971fc8",
   "metadata": {},
   "source": [
    "# Summarization with base hf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4654de49-d755-47c7-af8e-fd08129ec277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "How would you like to upgrade your computer?\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d3ba5-ffa4-458b-a438-9e37486db048",
   "metadata": {},
   "source": [
    "# Tokenizing the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d050e3ee-8e54-4f88-9e11-8443f4b17adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (1246, 2)\n",
      "Test: (150, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1246\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token).\n",
    "def tokenize_function(example):\n",
    "    \n",
    "    start_prompt = 'Summarize the following article.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for story in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "\n",
    "#To save some time in the lab, you will subsample the dataset\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 10 == 0, with_indices=True)\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88242d81-d1d9-492b-b02a-54dbdcce32ca",
   "metadata": {},
   "source": [
    "# Full Fine-Tuning of the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af43ce09-7e41-48b0-aa4a-ba0a4e302472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 03:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>57.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>53.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>51.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>48.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>47.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>47.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>46.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>44.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>43.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>43.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>42.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>43.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>41.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>41.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>41.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>41.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>42.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>41.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>41.578400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=45.55965805053711, metrics={'train_runtime': 221.5144, 'train_samples_per_second': 0.722, 'train_steps_per_second': 0.09, 'total_flos': 29742480752640.0, 'train_loss': 45.55965805053711, 'epoch': 0.1282051282051282})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully fine-tuning the entire original_model\n",
    "fully_output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=fully_output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=20,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "trainer.train() #this will throw out of memory error due to memory constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c269759-4181-4fa6-baba-b398f6ab7d76",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning of the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523bb1a0-4fbf-46e6-967e-1431058764cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 344,064 || all params: 77,305,216 || trainable%: 0.4451\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "\n",
    "# Define a LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],  # for T5, \"q\" and \"v\" usually work well\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "# Wrap your base model with LoRA\n",
    "model_with_lora = get_peft_model(original_model, lora_config)\n",
    "model_with_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a8054-8f9f-4c56-8def-f49fb05769b0",
   "metadata": {},
   "source": [
    "# LoRA params setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee022a-db13-4e57-8332-e20445eb08c7",
   "metadata": {},
   "source": [
    "| Parameter                             | Description                                                                                                                                                                                                                                                                                                          |\n",
    "| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`r=8`**                             | **Rank of the LoRA matrix.** This determines the size of the low-rank adaptation. A smaller `r` means fewer trainable parameters. Higher values can give better performance but increase memory use.                                                                                                                 |\n",
    "| **`lora_alpha=32`**                   | **Scaling factor** for the LoRA weights. The actual adaptation is scaled by `lora_alpha / r`. This controls how much the LoRA weights affect the model.                                                                                                                                                              |\n",
    "| **`target_modules=[\"q\", \"v\"]`**       | **Names of submodules in the base model to which LoRA will be applied.** <br>For T5 (or similar Transformer models), `\"q\"` = query projection, `\"v\"` = value projection in attention. These are often most effective for adaptation.                                                                                 |\n",
    "| **`lora_dropout=0.1`**                | **Dropout rate applied to LoRA layers during training.** Helps prevent overfitting. This dropout is **only applied to the LoRA weights**, not the full model.                                                                                                                                                        |\n",
    "| **`bias=\"none\"`**                     | Controls how biases are handled. Options: <br>– `\"none\"` (no bias adaptation) <br>– `\"all\"` (adapt all biases) <br>– `\"lora_only\"` (adapt only biases in LoRA layers). `\"none\"` is often sufficient.                                                                                                                 |\n",
    "| **`task_type=TaskType.SEQ_2_SEQ_LM`** | Specifies the **type of task**, which tells PEFT how to apply LoRA properly. Options include: <br>– `SEQ_CLS` (sequence classification) <br>– `SEQ_2_SEQ_LM` (e.g., T5, BART) <br>– `CAUSAL_LM` (e.g., GPT) <br>– `TOKEN_CLS`, `QUESTION_ANS`, etc. <br>This ensures LoRA adapts the right components for your task. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6decf8-d709-463c-a513-cc8c479f69a9",
   "metadata": {},
   "source": [
    "| Parameter            | Typical Range / Advice                                                                                    | When to Increase                                                      | When to Decrease                                                  |\n",
    "| -------------------- | --------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| **`r`** (rank)       | `4 – 16` (default: 8) <br> `8` is a good balance for most T5-based models.                                | If your model is large and your dataset is complex.                   | If you're memory-constrained or dataset is small.                 |\n",
    "| **`lora_alpha`**     | `r × 4` (default: 32 when `r=8`) <br> Rule of thumb: set it proportional to `r`.                          | If the model underfits (too conservative changes).                    | If you observe overfitting or instability.                        |\n",
    "| **`target_modules`** | For **T5**, use `[\"q\", \"v\"]` (query, value). <br> Optional: include `\"k\"` and `\"o\"` for more flexibility. | If LoRA impact is too small. Try `[\"q\", \"k\", \"v\", \"o\"]`.              | Keep minimal (`[\"q\", \"v\"]`) to save memory and avoid overfitting. |\n",
    "| **`lora_dropout`**   | `0.0 – 0.3` <br> Default: `0.1`. Use small dropout for stable training.                                   | If dataset is noisy or very small (to prevent overfitting).           | If underfitting on clean and large datasets.                      |\n",
    "| **`bias`**           | `\"none\"` (most common) <br> `\"lora_only\"` if you suspect bias is important.                               | Rarely needed. Try `\"all\"` if performance is poor.                    | Stick with `\"none\"` to avoid unnecessary complexity.              |\n",
    "| **`task_type`**      | Depends on model type: <br>Use `SEQ_2_SEQ_LM` for T5, BART; <br>`CAUSAL_LM` for GPT.                      | ❌ Never change this arbitrarily — must match your model architecture. | N/A                                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cee7e7-9c1f-491e-80e1-558121101fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/anaconda3/envs/conda_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 03:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>39.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>38.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>36.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>34.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>33.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>32.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>31.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>30.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>28.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>27.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>26.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>26.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>26.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>24.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>23.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>23.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>23.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>22.547800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=29.77527208328247, metrics={'train_runtime': 229.1088, 'train_samples_per_second': 0.698, 'train_steps_per_second': 0.087, 'total_flos': 29911595089920.0, 'train_loss': 29.77527208328247, 'epoch': 0.1282051282051282})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=peft_output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=25,\n",
    "    logging_steps=1,\n",
    "    max_steps=20    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=model_with_lora,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e66830-d1f0-4c74-b530-70e2626faae5",
   "metadata": {},
   "source": [
    "# Comparision between Original vs Full Fine-Tuning vs LoRA Fine-Tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3193fb-e93e-4fa4-b3f5-3f1219fbfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Employees should be allowed to use instant messaging and instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to use instant messaging to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed to communicate with clients. Employees should be allowed\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "Is this the first time you have a dictation for me?\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: \n",
      "I am going to send you a memo to all employees.\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "###################\n",
    "# input Processing \n",
    "###################\n",
    "input_ids = tokenizer(prompt, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "###############################\n",
    "# Original Model summarization\n",
    "###############################\n",
    "device = torch.device(\"cpu\")\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "original_model_outputs = original_model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=200,\n",
    ")\n",
    "original_model_text_output = tokenizer.decode(\n",
    "    original_model_outputs[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#######################################\n",
    "# (Fully) Instruct Model summarization\n",
    "#######################################\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    f\"./{fully_output_dir}/checkpoint-20\",\n",
    "    torch_dtype=torch.float32,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_new_tokens=200,\n",
    ")\n",
    "instruct_model_text_output = tokenizer.decode(\n",
    "    instruct_model_outputs[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "#######################################\n",
    "# (LoRA) Instruct Model summarization\n",
    "#######################################\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32, \n",
    "    device_map='auto'\n",
    ")\n",
    "device = torch.device(\"cpu\")\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    f'./{peft_output_dir}/checkpoint-20', \n",
    "    torch_dtype=torch.float32,\n",
    "    is_trainable=False\n",
    ")\n",
    "peft_model_outputs = peft_model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_new_tokens=200,\n",
    ")\n",
    "peft_model_text_output = tokenizer.decode(\n",
    "    peft_model_outputs[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: \\n{peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae01970f-7485-4127-9722-afaefc1d089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>I'm apprehensive about the new policy.</td>\n",
       "      <td>Is this the first time you have a dictation fo...</td>\n",
       "      <td>I am going to send you a memo to all employees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Taking a dictation for the department heads.</td>\n",
       "      <td>Is this the first time you have a dictation fo...</td>\n",
       "      <td>I am going to send you a memo to all employees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Ms. Dawson, I need you to take a dictation for...</td>\n",
       "      <td>Is this the first time you have a dictation fo...</td>\n",
       "      <td>I am going to send you a memo to all employees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>Taking the subway would be a good idea for me.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>I'm a little worried about the traffic jam.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>A lot of people are driving to work.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Kate, you know, I'm not sure.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>I'm not sure if they're going to divorce the n...</td>\n",
       "      <td>Kate, you know, I'm not sure.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>Kate, you know, I'm not sure.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Happy Birthday, Brian.</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Person1# attends Brian's birthday party. Bria...</td>\n",
       "      <td>Happy Birthday, Brian.</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Person1# has a dance with Brian at Brian's bi...</td>\n",
       "      <td>Brian, I'm so happy to have you celebrate with...</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#Person1# is surprised at the Olympic Stadium'...</td>\n",
       "      <td>- I'm a sexy, a sexy, a sexy, a sexy, a sexy, ...</td>\n",
       "      <td>The Olympic Park is going to be finished in June.</td>\n",
       "      <td>A man is walking down the track.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Person2# shows #Person1# around the construct...</td>\n",
       "      <td>We are in the Olympic park.</td>\n",
       "      <td>The Olympic Park is going to be finished in June.</td>\n",
       "      <td>A man is walking down the track.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#Person2# introduces the Olympic Stadium's fin...</td>\n",
       "      <td>The Olympic stadium is going to be finished in...</td>\n",
       "      <td>The Olympic Park is going to be finished in June.</td>\n",
       "      <td>A man is walking down the track.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#Person1# wants to create a company and is goi...</td>\n",
       "      <td>I am done working for a company that is taking...</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to a friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Person1# abandons the idea of creating a comp...</td>\n",
       "      <td>- I am a businessperson and I am a businessper...</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to a friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#Person1# wants to start #Person1#'s own busin...</td>\n",
       "      <td>...</td>\n",
       "      <td>Talk to your boss.</td>\n",
       "      <td>Talk to a friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#Person2# feels itchy. #Person1# doubts it is ...</td>\n",
       "      <td>I think you're a rash.</td>\n",
       "      <td>Talk to your doctor.</td>\n",
       "      <td>Talk to your doctor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#Person1# suspects that #Person2# has chicken ...</td>\n",
       "      <td>I've been a lot of people have scratched my te...</td>\n",
       "      <td>Talk to your doctor.</td>\n",
       "      <td>Talk to your doctor.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             human_baseline_summaries  \\\n",
       "0   Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1   In order to prevent employees from wasting tim...   \n",
       "2   Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3   #Person2# arrives late because of traffic jam....   \n",
       "4   #Person2# decides to follow #Person1#'s sugges...   \n",
       "5   #Person2# complains to #Person1# about the tra...   \n",
       "6   #Person1# tells Kate that Masha and Hero get d...   \n",
       "7   #Person1# tells Kate that Masha and Hero are g...   \n",
       "8   #Person1# and Kate talk about the divorce betw...   \n",
       "9   #Person1# and Brian are at the birthday party ...   \n",
       "10  #Person1# attends Brian's birthday party. Bria...   \n",
       "11  #Person1# has a dance with Brian at Brian's bi...   \n",
       "12  #Person1# is surprised at the Olympic Stadium'...   \n",
       "13  #Person2# shows #Person1# around the construct...   \n",
       "14  #Person2# introduces the Olympic Stadium's fin...   \n",
       "15  #Person1# wants to create a company and is goi...   \n",
       "16  #Person1# abandons the idea of creating a comp...   \n",
       "17  #Person1# wants to start #Person1#'s own busin...   \n",
       "18  #Person2# feels itchy. #Person1# doubts it is ...   \n",
       "19  #Person1# suspects that #Person2# has chicken ...   \n",
       "\n",
       "                             original_model_summaries  \\\n",
       "0              I'm apprehensive about the new policy.   \n",
       "1        Taking a dictation for the department heads.   \n",
       "2   Ms. Dawson, I need you to take a dictation for...   \n",
       "3      Taking the subway would be a good idea for me.   \n",
       "4         I'm a little worried about the traffic jam.   \n",
       "5                A lot of people are driving to work.   \n",
       "6                Masha and Hero are getting divorced.   \n",
       "7   I'm not sure if they're going to divorce the n...   \n",
       "8                        Masha and Hero are divorced.   \n",
       "9                              Happy Birthday, Brian.   \n",
       "10                             Happy Birthday, Brian.   \n",
       "11  Brian, I'm so happy to have you celebrate with...   \n",
       "12  - I'm a sexy, a sexy, a sexy, a sexy, a sexy, ...   \n",
       "13                        We are in the Olympic park.   \n",
       "14  The Olympic stadium is going to be finished in...   \n",
       "15  I am done working for a company that is taking...   \n",
       "16  - I am a businessperson and I am a businessper...   \n",
       "17                                                ...   \n",
       "18                             I think you're a rash.   \n",
       "19  I've been a lot of people have scratched my te...   \n",
       "\n",
       "                             instruct_model_summaries  \\\n",
       "0   Is this the first time you have a dictation fo...   \n",
       "1   Is this the first time you have a dictation fo...   \n",
       "2   Is this the first time you have a dictation fo...   \n",
       "3                                  Talk to your boss.   \n",
       "4                                  Talk to your boss.   \n",
       "5                                  Talk to your boss.   \n",
       "6                       Kate, you know, I'm not sure.   \n",
       "7                       Kate, you know, I'm not sure.   \n",
       "8                       Kate, you know, I'm not sure.   \n",
       "9                                 Brian, how are you?   \n",
       "10                                Brian, how are you?   \n",
       "11                                Brian, how are you?   \n",
       "12  The Olympic Park is going to be finished in June.   \n",
       "13  The Olympic Park is going to be finished in June.   \n",
       "14  The Olympic Park is going to be finished in June.   \n",
       "15                                 Talk to your boss.   \n",
       "16                                 Talk to your boss.   \n",
       "17                                 Talk to your boss.   \n",
       "18                               Talk to your doctor.   \n",
       "19                               Talk to your doctor.   \n",
       "\n",
       "                               peft_model_summaries  \n",
       "0   I am going to send you a memo to all employees.  \n",
       "1   I am going to send you a memo to all employees.  \n",
       "2   I am going to send you a memo to all employees.  \n",
       "3                                Talk to your boss.  \n",
       "4                                Talk to your boss.  \n",
       "5                                Talk to your boss.  \n",
       "6              Masha and Hero are getting divorced.  \n",
       "7              Masha and Hero are getting divorced.  \n",
       "8              Masha and Hero are getting divorced.  \n",
       "9                               Brian, how are you?  \n",
       "10                              Brian, how are you?  \n",
       "11                              Brian, how are you?  \n",
       "12                 A man is walking down the track.  \n",
       "13                 A man is walking down the track.  \n",
       "14                 A man is walking down the track.  \n",
       "15                                Talk to a friend.  \n",
       "16                                Talk to a friend.  \n",
       "17                                Talk to a friend.  \n",
       "18                             Talk to your doctor.  \n",
       "19                             Talk to your doctor.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "dialogues = dataset['test'][0:20]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:20]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Original model\n",
    "    original_model_outputs = original_model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    # Fully Instruct model\n",
    "    instruct_model_outputs = instruct_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "\n",
    "    # PEFT model\n",
    "    peft_model_outputs = peft_model.generate(\n",
    "        input_ids=input_ids, \n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "    \n",
    "# Combine summaries\n",
    "zipped_summaries = list(zip(\n",
    "    human_baseline_summaries,\n",
    "    original_model_summaries,\n",
    "    instruct_model_summaries,\n",
    "    peft_model_summaries\n",
    "))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    zipped_summaries,\n",
    "    columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664bfc5e-4c13-4963-b0a4-291b03efff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': np.float64(0.16791136583794442), 'rouge2': np.float64(0.04915458937198068), 'rougeL': np.float64(0.14176375516917944), 'rougeLsum': np.float64(0.14187898607250704)}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': np.float64(0.12371533120463277), 'rouge2': np.float64(0.014646464646464647), 'rougeL': np.float64(0.1051203074619996), 'rougeLsum': np.float64(0.10536452327395669)}\n",
      "PEFT MODEL:\n",
      "{'rouge1': np.float64(0.16548854096663573), 'rouge2': np.float64(0.05218840579710145), 'rougeL': np.float64(0.15946482954824476), 'rougeLsum': np.float64(0.15938968175376172)}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print(\"ORIGINAL MODEL:\")\n",
    "print(original_model_results)\n",
    "\n",
    "print(\"INSTRUCT MODEL:\")\n",
    "print(instruct_model_results)\n",
    "\n",
    "print(\"PEFT MODEL:\")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46460b-d2f7-432c-8b88-0c6c0d6bc49d",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d40e95-fefc-4d16-808a-6b46b72390cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d788193acc49468c2539049bbb0763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I love machine learning.\",\n",
    "    \"FAISS is great for vector similarity search.\",\n",
    "    \"Let's store text embeddings in a database.\",\n",
    "    \"Python is a versatile language.\",\n",
    "]\n",
    "\n",
    "# Convert to vectors\n",
    "embeddings = model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e78292f-57bd-403a-9e25-79760dbf9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "dimension = embeddings.shape[1]  # e.g., 384\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 = Euclidean distance\n",
    "\n",
    "# Add vectors to index\n",
    "index.add(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeabc5ba-47da-47a8-8b91-95f8623cf529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I find similar texts?\n",
      "\n",
      "Top matches:\n",
      "1. FAISS is great for vector similarity search. (Distance: 0.9444)\n",
      "2. Let's store text embeddings in a database. (Distance: 1.2533)\n",
      "3. I love machine learning. (Distance: 1.7097)\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I find similar texts?\"\n",
    "query_vector = model.encode([query])\n",
    "\n",
    "# Search for top 3 similar texts\n",
    "distances, indices = index.search(query_vector, k=3)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop matches:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}. {texts[idx]} (Distance: {distances[0][i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9299a52-45bf-4a58-8e79-bbe5082e55e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to embedded data?\n",
      "\n",
      "Top matches:\n",
      "1. Let's store text embeddings in a database. (Distance: 0.9141)\n",
      "2. I love machine learning. (Distance: 1.7842)\n",
      "3. FAISS is great for vector similarity search. (Distance: 1.8112)\n"
     ]
    }
   ],
   "source": [
    "query = \"How to embedded data?\"\n",
    "query_vector = model.encode([query])\n",
    "\n",
    "# Search for top 3 similar texts\n",
    "distances, indices = index.search(query_vector, k=3)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop matches:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}. {texts[idx]} (Distance: {distances[0][i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745bf637-6bf9-43b6-8fb9-b46352ff4af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love machine learning.',\n",
       " 'FAISS is great for vector similarity search.',\n",
       " \"Let's store text embeddings in a database.\",\n",
       " 'Python is a versatile language.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8dc2c75-2feb-41e5-b50c-d2d60ada4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index\n",
    "faiss.write_index(index, \"vector_and_bm25_dbs/text_index.faiss\")\n",
    "\n",
    "# Later load it\n",
    "index = faiss.read_index(\"vector_and_bm25_dbs/text_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d1206-2bea-4068-a676-a57233efc758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
