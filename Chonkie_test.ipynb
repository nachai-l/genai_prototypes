{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553954f-7d3d-4324-96b2-8b5624350e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chonkie Python Examples - Text Chunking for RAG Applications\n",
    "\n",
    "# First, install Chonkie:\n",
    "# pip install chonkie\n",
    "# pip install chonkie[tokenizers]  # For tokenizer support\n",
    "\n",
    "# Basic example with TokenChunker\n",
    "from chonkie import TokenChunker\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create chunker\n",
    "chunker = TokenChunker(tokenizer=tokenizer, chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "# Process text\n",
    "text = \"\"\"\n",
    "Chonkie is a revolutionary text chunking library designed for RAG applications.\n",
    "It provides lightning-fast performance with minimal overhead, making it perfect\n",
    "for production environments. The library offers multiple chunking strategies\n",
    "including token-based, sentence-based, and semantic chunking approaches.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.text[:100]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# Different Chunking Strategies\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Sentence-based chunking\n",
    "from chonkie import SentenceChunker\n",
    "\n",
    "sentence_chunker = SentenceChunker(chunk_size=2)  # 2 sentences per chunk\n",
    "sentence_chunks = sentence_chunker.chunk(text)\n",
    "\n",
    "print(\"\\n--- Sentence Chunking ---\")\n",
    "for i, chunk in enumerate(sentence_chunks):\n",
    "    print(f\"Sentence Chunk {i+1}: {chunk.text}\")\n",
    "\n",
    "# 2. Semantic chunking (if available)\n",
    "try:\n",
    "    from chonkie import SemanticChunker\n",
    "    \n",
    "    semantic_chunker = SemanticChunker()\n",
    "    semantic_chunks = semantic_chunker.chunk(text)\n",
    "    \n",
    "    print(\"\\n--- Semantic Chunking ---\")\n",
    "    for i, chunk in enumerate(semantic_chunks):\n",
    "        print(f\"Semantic Chunk {i+1}: {chunk.text[:100]}...\")\n",
    "except ImportError:\n",
    "    print(\"\\nSemantic chunking requires additional dependencies\")\n",
    "\n",
    "# 3. Hierarchical chunking\n",
    "try:\n",
    "    from chonkie import HierarchicalChunker\n",
    "    \n",
    "    hierarchical_chunker = HierarchicalChunker()\n",
    "    hierarchical_chunks = hierarchical_chunker.chunk(text)\n",
    "    \n",
    "    print(\"\\n--- Hierarchical Chunking ---\")\n",
    "    for i, chunk in enumerate(hierarchical_chunks):\n",
    "        print(f\"Hierarchical Chunk {i+1}: {chunk.text[:100]}...\")\n",
    "except ImportError:\n",
    "    print(\"\\nHierarchical chunking requires additional dependencies\")\n",
    "\n",
    "# ============================================================================\n",
    "# Working with longer texts and files\n",
    "# ============================================================================\n",
    "\n",
    "def process_document(file_path, chunk_size=512, overlap=50):\n",
    "    \"\"\"Process a text document using Chonkie\"\"\"\n",
    "    \n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Initialize chunker\n",
    "    tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "    chunker = TokenChunker(tokenizer=tokenizer, chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    \n",
    "    # Chunk the content\n",
    "    chunks = chunker.chunk(content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage (uncomment if you have a text file):\n",
    "# chunks = process_document(\"sample_document.txt\")\n",
    "# print(f\"Processed document into {len(chunks)} chunks\")\n",
    "\n",
    "# ============================================================================\n",
    "# Advanced Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Custom tokenizer configuration\n",
    "custom_tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "advanced_chunker = TokenChunker(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=25,\n",
    "    min_chunk_size=50  # Minimum chunk size\n",
    ")\n",
    "\n",
    "# Process with custom settings\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence \n",
    "that focuses on the interaction between computers and humans using natural language.\n",
    "The ultimate objective of NLP is to read, decipher, understand, and make sense \n",
    "of human languages in a manner that is valuable. Most NLP techniques rely on \n",
    "machine learning to derive meaning from human languages.\n",
    "\"\"\"\n",
    "\n",
    "advanced_chunks = advanced_chunker.chunk(sample_text)\n",
    "print(f\"\\n--- Advanced Configuration ---\")\n",
    "print(f\"Created {len(advanced_chunks)} chunks with custom settings\")\n",
    "\n",
    "# ============================================================================\n",
    "# Integration with RAG Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_for_rag(text, chunk_size=400, overlap=40):\n",
    "    \"\"\"Prepare text for RAG by chunking and adding metadata\"\"\"\n",
    "    \n",
    "    tokenizer = Tokenizer.from_pretrained(\"gpt2\")\n",
    "    chunker = TokenChunker(tokenizer=tokenizer, chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    \n",
    "    chunks = chunker.chunk(text)\n",
    "    \n",
    "    # Add metadata for each chunk\n",
    "    prepared_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            'id': f\"chunk_{i}\",\n",
    "            'text': chunk.text,\n",
    "            'start_index': chunk.start_index,\n",
    "            'end_index': chunk.end_index,\n",
    "            'token_count': len(tokenizer.encode(chunk.text).ids),\n",
    "        }\n",
    "        prepared_chunks.append(chunk_data)\n",
    "    \n",
    "    return prepared_chunks\n",
    "\n",
    "# Example RAG preparation\n",
    "rag_chunks = prepare_for_rag(sample_text)\n",
    "print(f\"\\n--- RAG Preparation ---\")\n",
    "for chunk in rag_chunks:\n",
    "    print(f\"ID: {chunk['id']}, Tokens: {chunk['token_count']}, Text: {chunk['text'][:60]}...\")\n",
    "\n",
    "print(\"\\nðŸ¦› Chonkie makes text chunking simple and fast!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
