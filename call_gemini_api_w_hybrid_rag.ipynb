{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a122010-af18-42a8-91ef-24e404f5e45f",
   "metadata": {},
   "source": [
    "# Gemini API [DO COST $$$]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7dea1-efdc-4988-aa2f-2747071703fc",
   "metadata": {},
   "source": [
    "# >> V0.5: Production Code version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db91f03c-579d-4147-b836-1a084b940015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pprint\n",
    "import google.genai as genai\n",
    "from google.genai import types\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60c6b79-1875-46d1-801f-77dc8a41daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genai_functions.gemini_usage_logging as gemini_log_functs\n",
    "import genai_functions.helper_functions as helper_functs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4757818-9770-490a-a80f-ae2fdf32a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functs.enable_notebook_logging(logger_name=\"hr_llm_functs\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cbf640-0d84-4595-b865-4b9f58f9e6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 99 existing logs (persist=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens_per_request': 3407.0,\n",
      " 'date_range': (Timestamp('2025-09-08 07:22:02.188114+0000', tz='UTC'),\n",
      "                Timestamp('2025-09-09 09:12:38.931943+0000', tz='UTC')),\n",
      " 'finish_reasons': {'MAX_TOKENS': 4, 'STOP': 87},\n",
      " 'total_requests': 99,\n",
      " 'total_tokens': 337293}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-09-09 09:07:52.901235+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: Can you recommend ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is the candidate suitable for a senior da...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>3565</td>\n",
       "      <td>716</td>\n",
       "      <td>4374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-09-09 09:09:26.008771+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: Can you recommend ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is the candidate with experience in the t...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>3808</td>\n",
       "      <td>391</td>\n",
       "      <td>4250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-09-09 09:09:52.227635+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: Can you recommend ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>At True Corporation Public Company Limited, Na...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>1017</td>\n",
       "      <td>30</td>\n",
       "      <td>3998</td>\n",
       "      <td>150</td>\n",
       "      <td>4178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-09-09 09:10:53.766114+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: Can you give me ed...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here are the candidates with good management s...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152</td>\n",
       "      <td>3932</td>\n",
       "      <td>350</td>\n",
       "      <td>4434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2025-09-09 09:12:38.931943+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: What did Oranid do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here are the candidates who may be suitable fo...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115</td>\n",
       "      <td>3839</td>\n",
       "      <td>302</td>\n",
       "      <td>4256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  \\\n",
       "94 2025-09-09 09:07:52.901235+00:00   \n",
       "95 2025-09-09 09:09:26.008771+00:00   \n",
       "96 2025-09-09 09:09:52.227635+00:00   \n",
       "97 2025-09-09 09:10:53.766114+00:00   \n",
       "98 2025-09-09 09:12:38.931943+00:00   \n",
       "\n",
       "                                                query uploaded_file  \\\n",
       "94  Conversation so far:\\nUser: Can you recommend ...           NaN   \n",
       "95  Conversation so far:\\nUser: Can you recommend ...           NaN   \n",
       "96  Conversation so far:\\nUser: Can you recommend ...           NaN   \n",
       "97  Conversation so far:\\nUser: Can you give me ed...           NaN   \n",
       "98  Conversation so far:\\nUser: What did Oranid do...           NaN   \n",
       "\n",
       "                                        response_text finish_reason  \\\n",
       "94  Here is the candidate suitable for a senior da...          STOP   \n",
       "95  Here is the candidate with experience in the t...          STOP   \n",
       "96  At True Corporation Public Company Limited, Na...          STOP   \n",
       "97  Here are the candidates with good management s...          STOP   \n",
       "98  Here are the candidates who may be suitable fo...          STOP   \n",
       "\n",
       "   cached_content_token_count candidates_token_count prompt_token_count  \\\n",
       "94                        NaN                     93               3565   \n",
       "95                        NaN                     51               3808   \n",
       "96                       1017                     30               3998   \n",
       "97                        NaN                    152               3932   \n",
       "98                        NaN                    115               3839   \n",
       "\n",
       "   thoughts_token_count total_token_count  \n",
       "94                  716              4374  \n",
       "95                  391              4250  \n",
       "96                  150              4178  \n",
       "97                  350              4434  \n",
       "98                  302              4256  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_logger = gemini_log_functs.GeminiUsageLogger(log_path=\"logs/gemini_usage.csv\")\n",
    "pprint.pprint(gemini_logger.get_usage_summary())\n",
    "logs_df = gemini_logger.get_logs_dataframe()\n",
    "logs_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43625080-4a28-430e-bbe7-af555be2cf64",
   "metadata": {},
   "source": [
    "========================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201f10a7-2d64-4176-b42c-555c797ca543",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyDHOSjzr-AedFPftuIK7iiZ0yTqaTkSDYQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae63b904-dd1a-45fa-b68e-7f95cc6b9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploaded_filename = \"ExampleCV/NLP-CV-NachaiLim.pdf\"\n",
    "# uploaded_filename = \"ExampleCV/CV-Oranid.pdf\"\n",
    "# uploaded_filename = \"ExampleCV/Natthaporn_CV2022.pdf\"\n",
    "uploaded_filename = \"ExampleCV/CV-Tun.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0a690a-a991-4f07-a732-54bbf8696c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_prompt_text  = \"\"\"\n",
    "Analyze the uploaded CV and provide the following:\n",
    "\n",
    "1. Identify the individual (full name).\n",
    "2. Summarize education and work experience in **10â€“15 concise bullet points**, including years of experience for each role.\n",
    "3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "\"\"\"\n",
    "formatted_today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "cv_system_text  =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\"\n",
    "\n",
    "log_path          = \"logs/gemini_usage.csv\"\n",
    "max_output_tokens = 18432\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec34ca2-232f-4c76-a9ec-9ea72dea54fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "import genai_functions.gemini_hr_llm_functions as hr_llm_functs\n",
    "from genai_functions.gemini_usage_logging import GeminiUsageLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af3a0bb-8a71-4ff0-b1ef-4c595bb98cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 99 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH887iOdmPGFFQcCKxsO0kbUAwdvcXpKnNua57pMh5VPhlRE_Lw4dsdXx6JFFszB6KZ5sWJYnoD3XMbmO6ttbJECJvcmL0DMGffdsl8IRQg&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Uploaded the CV file to Gemini\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=437, total_tokens=5901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the uploaded CV:\n",
      "\n",
      "### 1. Individual Identification\n",
      "**Full Name:** TUN KEDSARO\n",
      "\n",
      "### 2. Education and Work Experience Summary (13 concise bullet points)\n",
      "\n",
      "*   **Education:** Bachelor of Engineering in Electrical (B.ENG) from Kasetsart University (July 2017 â€“ June 2021, 4 years).\n",
      "*  \n"
     ]
    }
   ],
   "source": [
    "cv_resp, logs_df = hr_llm_functs.master_gemini_upload_cv_prompt_w_log(\n",
    "    uploaded_filename = uploaded_filename,\n",
    "    prompt_text       = cv_prompt_text,\n",
    "    system_text       = cv_system_text,\n",
    "    client            = client,                     \n",
    "    log_path          = log_path,  # enable persistence\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p,\n",
    "    top_k             = top_k,\n",
    ")\n",
    "print(cv_resp.text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c514e0-ed31-4f75-9d9c-db8a166ceee2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "jd_prompt_text = f\"\"\"\n",
    "Senior Data Scientist\n",
    "\n",
    "à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸žà¸·à¹‰à¸™à¸à¸²à¸™ 5-10 à¸›à¸µ à¸›à¸£à¸´à¸à¸à¸²à¸•à¸£à¸µà¸‚à¸¶à¹‰à¸™à¹„à¸›\n",
    "\n",
    "à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‡à¸²à¸™à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™ Key responsibilities: â€¢ Interpret data, and analyze results using statistical methods â€¢ Prepare and deliver business reports that effectively communicate trends, patterns, risks, and insights using data â€¢ Research, design, and develop machine-learning/artificial-intelligence/computer-vision systems to address key business challenges â€¢ Perform data quality testing, validation, and assurance as a part of designing, and implementing scalable data solutions â€¢ Support identification, triage, and remediation of data quality issues across the data and technology organizations â€¢ Effectively manage and develop small data analytics teams â€¢ Effectively engage and partner with team members across data analytics, technology development, and business strategy to drive outcomes and impact â€¢ Ensure end user requirements of data solutions are effectively met â€¢ Develop user friendly documentation to communicate the use and value of data solutions â€¢ Identify and push forward process improvement opportunities and solutions â€¢ Review and keep up to date with developments in the data analytics and ML fields\n",
    "\n",
    "Education: â€¢ B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science\n",
    "\n",
    "Relevant experience required: â€¢ 5+ years hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health / monitoring frameworks â€¢ Proficiency with one or more programming languages or data engineering frameworks, such as Python, SQL, Spark, Java, C++, TypeScript/JavaScript, or similar â€¢ Proficiency with any of the major machine learning and computer vision frameworks preferred â€¢ Hands-on experience on BI solutions (e.g. Tableau, Power BI, Qlik, Looker) â€¢ Ability to work well with cross-disciplinary teams â€¢ Strong numerical and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy â€¢ Communicate in English fluently\n",
    "\n",
    "ðŸ˜† Our Benefits: Â· Free lunch every Tuesday and Friday Â· WFH/WFA (Work From Home/Work From Anywhere) Â· Group insurance Â· Retirement savings fund Â· Well-stocked Snack Bar with snacks and beverages Â· Dress in your own style at work Â· Annual vacation 15 days\n",
    "\"\"\"\n",
    "\n",
    "jd_system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "make a summarization of the job description in a list of Key responsibilities, Mandaotry experiences & skills, Prefer experiences & skills.\n",
    "\"\"\"\n",
    "max_output_tokens = 4096\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c628e7-66d0-48fe-890b-45e05718f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 100 existing logs (persist=True)\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:JD generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=2353, total_tokens=2580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summarization of the Senior Data Scientist job description:\n",
      "\n",
      "---\n",
      "\n",
      "**Senior Data Scientist**\n",
      "\n",
      "We are seeking an experienced Senior Data Scientist to lead data analysis, develop advanced ML/AI/CV systems, and drive data-driven insights. This role requires strong analytical skills, technical p\n"
     ]
    }
   ],
   "source": [
    "jd_resp, logs_df, jd_formatted_text = hr_llm_functs.master_gemini_jd_prompt_w_log(\n",
    "    prompt_text       = jd_prompt_text,\n",
    "    system_text       = jd_system_text,\n",
    "    client            = client,                     \n",
    "    log_path          = log_path,  # enable persistence\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p,\n",
    "    top_k             = top_k,\n",
    ")\n",
    "print(jd_formatted_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340102a6-6c7f-40e0-b547-96c44dd72df5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output_template = \"\"\"\n",
    "### JD Mandatory Requirements [âœ“: Pass | âœ—: Missing]\n",
    "1. Requirement 1 â€” [âœ“/âœ—]\n",
    "2. Requirement 2 â€” [âœ“/âœ—]\n",
    "\n",
    "### JD Preferred Requirements [âœ“: Pass | âœ—: Missing]\n",
    "1. Requirement 1 â€” [âœ“/âœ—]\n",
    "2. Requirement 2 â€” [âœ“/âœ—]\n",
    "\n",
    "### Candidate Strengths (1â€“5 bullets)\n",
    "1. Strength 1\n",
    "2. Strength 2\n",
    "\n",
    "### Candidate Weaknesses (1â€“5 bullets)\n",
    "1. Weakness 1\n",
    "2. Weakness 2\n",
    "\n",
    "### JD vs CV Matching Score  \n",
    "**Score:** X.X / 10.0  \n",
    "\n",
    "**Reasoning (3â€“5 bullets):**  \n",
    "1. Reason 1  \n",
    "2. Reason 2  \n",
    "3. Reason 3  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35cee75f-b841-4570-9528-ef4aaebc0be4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "Filling in the output_template below based on the given job description (JD) and summarized resume (CV).\n",
    "Only answer in the given output_template.\n",
    "\n",
    "# output_template:\n",
    "{output_template}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6967dee5-336d-4c4e-a2a2-f459d2e84d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 101 existing logs (persist=True)\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:CVÃ—JD matching generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=5586, total_tokens=5696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### JD Mandatory Requirements [âœ“: Pass | âœ—: Missing]\n",
      "1.  Education (Bachelor's, Master's, or Ph.D. in CS, Math, SE, Physics, or Data Science) â€” âœ“\n",
      "2.  5+ years of hands-on experience in Data mining and statistical analysis â€” âœ“\n",
      "3.  5+ years of hands-on experience in Distributed computing â€” âœ—\n",
      "4.  5+ ye\n"
     ]
    }
   ],
   "source": [
    "cvxjd_resp, logs_df, cvxjd_formatted_text, clean_output_md, parsed_output = hr_llm_functs.master_gemini_cvxjd_matching_w_log(\n",
    "    resp_jd           = jd_resp,\n",
    "    resp_cv           = cv_resp,\n",
    "    client            = client,\n",
    "    output_template   = output_template,\n",
    "    system_text       = system_text,\n",
    "    log_path          = log_path,  # enable persistence\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p,\n",
    "    top_k             = top_k,\n",
    ")\n",
    "print(clean_output_md[:300])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e172fd-ab46-41bd-81f4-41cecdb5ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_filenames_list = [\n",
    "    \"ExampleCV/NLP-CV-NachaiLim.pdf\",\n",
    "    \"ExampleCV/CV-Oranid.pdf\",\n",
    "    \"ExampleCV/Natthaporn_CV2022.pdf\",\n",
    "    \"ExampleCV/CV-Tun.pdf\",\n",
    "]\n",
    "applied_position = \"senior data scientist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696e88e1-9e31-489a-bef1-6a6ba9a5a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 102 existing logs (persist=True)\n",
      "INFO:genai_functions.gemini_hr_llm_functions:>>> Loaded & cleared FAISS store: vector_and_bm25_dbs/vector_index.faiss\n",
      "INFO:genai_functions.gemini_hr_llm_functions:>>> Loaded & cleared BM25 index: vector_and_bm25_dbs/bm25_metadata.jsonl\n",
      "INFO:genai_functions.gemini_hr_llm_functions:\n",
      "==== Processing CV 1/4: ExampleCV/NLP-CV-NachaiLim.pdf ====\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 102 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH8-zp8bvZSx0x7mbrsg0Cakxv21Hk9Qfj9TeCaIzQ9yCdxYVsMf8t-AalLPt65NSRgYMbMkdjX8iPjQ0BPa-Yjeq9gCmLiateFh7ZI0-Mw&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Uploaded the CV file to Gemini\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=465, total_tokens=4499\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:==== Completed CV 1/4: ExampleCV/NLP-CV-NachaiLim.pdf ====\n",
      "\n",
      "INFO:genai_functions.gemini_hr_llm_functions:\n",
      "==== Processing CV 2/4: ExampleCV/CV-Oranid.pdf ====\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 103 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH890A9YiRpqFS7OikLH5P8GAeztUXjLQ-DkcgNWwkO5LP_2qjrklmtFhB_4o1jsAtjlOSy9xrXLKeTOKHFss9C9GqfE4n1uf7HHoNVLLK2I&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Uploaded the CV file to Gemini\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=465, total_tokens=4146\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:==== Completed CV 2/4: ExampleCV/CV-Oranid.pdf ====\n",
      "\n",
      "INFO:genai_functions.gemini_hr_llm_functions:\n",
      "==== Processing CV 3/4: ExampleCV/Natthaporn_CV2022.pdf ====\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 104 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH888Fv9ehRmlbQ9QjIh0YExXBRMPwUL1X_aJbbnzH-KyuS_o0vri_Lc6xftM7Nz0euHp4K8nzuyjuxH2t4TIlicNQeD6G3n1_YcXv7z8IA&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Uploaded the CV file to Gemini\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=465, total_tokens=3900\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:==== Completed CV 3/4: ExampleCV/Natthaporn_CV2022.pdf ====\n",
      "\n",
      "INFO:genai_functions.gemini_hr_llm_functions:\n",
      "==== Processing CV 4/4: ExampleCV/CV-Tun.pdf ====\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 105 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/upload/v1beta/files?upload_id=ABgVH89o3FY2Md1i8ohx-AenuxhGQvXBChz04p8v6wFSIM-VNjZJOPJu3m0MTRZNYbIAiehItxsS3fZ7lezNwrtCg1314pkmjn4i-HUpYq_6Xw&upload_protocol=resumable \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Uploaded the CV file to Gemini\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Generation succeeded with model=gemini-2.5-flash\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=465, total_tokens=4919\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:genai_functions.gemini_hr_llm_functions:==== Completed CV 4/4: ExampleCV/CV-Tun.pdf ====\n",
      "\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Saved FAISS store & metadata: vector_and_bm25_dbs/vector_index.faiss | vector_and_bm25_dbs/vector_metadata.jsonl\n",
      "INFO:genai_functions.gemini_hr_llm_functions:Saved BM25 metadata: vector_and_bm25_dbs/bm25_metadata.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Final Summary =======\n",
      "num_inputs:  4\n",
      "num_success: 4\n",
      "num_failed:  0\n",
      ">>> failed_files:  []\n"
     ]
    }
   ],
   "source": [
    "result = hr_llm_functs.master_loading_cvs_to_faiss_and_bm25(\n",
    "    client,\n",
    "    uploaded_filenames_list, \n",
    "    applied_position,\n",
    "    embedding_modelname = \"text-embedding-004\",\n",
    "    vector_dbname       = \"vector_and_bm25_dbs/vector_index.faiss\",\n",
    "    vector_dbmeta       = \"vector_and_bm25_dbs/vector_metadata.jsonl\",\n",
    "    bm25_dbmeta         = \"vector_and_bm25_dbs/bm25_metadata.jsonl\" ,\n",
    "    cv_system_text      =  None, # Using default sysyem text\n",
    "    cv_prompt_text      = \"\"\"\n",
    "    Analyze the uploaded CV and provide the following:\n",
    "    \n",
    "    1. Identify the individual (full name).\n",
    "    2. Summarize education and work experience in **10â€“15 concise bullet points**, including years of experience for each role.\n",
    "    3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "    4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "    \"\"\",\n",
    "    log_path            = \"logs/gemini_usage.csv\",\n",
    "    max_output_tokens   = 12288,\n",
    "    temperature         = 0.1,\n",
    "    top_p               = 0.9, \n",
    "    top_k               = 40,  # At each step, the model only considers the top_k most likely tokens.\n",
    "    clear_initial_faiss = True,\n",
    "    clear_initial_bm25  = True,\n",
    ")\n",
    "print(\"\\n======= Final Summary =======\")\n",
    "print(f\"num_inputs:  {result['num_inputs']}\")\n",
    "print(f\"num_success: {result['num_success']}\")\n",
    "print(f\"num_failed:  {result['num_failed']}\")\n",
    "print(f\">>> failed_files:  {result['failed_files']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af8f340-8159-4313-a660-f1d8adbeceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genai_functions.hybrid_vectordb_functions as vectordb_functs\n",
    "from genai_functions.hybrid_vectordb_functions import FaissVectorStore, BM25Index, embed_texts_with_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4600c8c4-12d4-4867-9602-b5327568a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vector Search ===\n",
      "[score=0.4252] ExampleCV/CV-Oranid.pdf\n",
      "[score=0.3596] ExampleCV/CV-Tun.pdf\n",
      "[score=0.3431] ExampleCV/Natthaporn_CV2022.pdf\n",
      "\n",
      "=== BM25 Search ===\n",
      "[score=0.2542] ExampleCV/Natthaporn_CV2022.pdf\n",
      "[score=0.2356] ExampleCV/CV-Oranid.pdf\n",
      "[score=0.2236] ExampleCV/CV-Tun.pdf\n"
     ]
    }
   ],
   "source": [
    "# ========= Example query after loop =========\n",
    "query = \"Study in Australia\"\n",
    "q_emb, _ = vectordb_functs.embed_query_with_gemini(client, query, embedding_modelname = \"text-embedding-004\")\n",
    "\n",
    "loaded_store = FaissVectorStore.load(result['faiss_path'], result['faiss_meta'])\n",
    "bm25_index   = BM25Index.load(result['bm25_meta'])\n",
    "\n",
    "vec_hits = loaded_store.search(q_emb, k=3)\n",
    "bm25_hits = bm25_index.search(query, k=3)\n",
    "vectordb_functs.show_hits(vec_hits, \"Vector Search\")\n",
    "vectordb_functs.show_hits(bm25_hits, \"BM25 Search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b66aaa45-2879-4305-a57e-7e816ed1cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genai_functions.hybrid_rag_functions as rag_functs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e08005d-ac14-48b0-a4f1-792a45eb3fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 106 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=51, total_tokens=2087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer:\n",
      " Natthaporn Takpho has work experience in Singapore [1].\n",
      "*   Natthaporn currently serves as Assistant Manager, Research Division at Mitsui Chemicals Singapore R&D Centre [1].\n",
      "*   Natthaporn previously worked as a Researcher at Mitsui Chemicals Singapore R&D Centre [1].\n",
      "ExampleCV/Natthaporn_CV2022.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"Which candidates have work experience in Singapore?\"\n",
    "resp, logs_df, used_ctx = rag_functs.master_gemini_rag_bm25_answer_w_log(\n",
    "    client, query,\n",
    "    log_path = \"logs/gemini_usage.csv\",\n",
    "    vector_store = loaded_store,\n",
    "    bm25_index   = bm25_index,\n",
    "    k_vec = 3, k_bm25 = 3, k_final = 2, \n",
    "    rrf_k = 60, w_vec = 1.0, w_bm = 1.0, # equal weight between FAISS and BM25\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b93897c-4942-4fe5-b7f7-57575da96f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 107 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=36, total_tokens=1986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer:\n",
      " Oranid Yenradee studies in Australia, pursuing a Master of Commerce (Extension) in Data Analytics in Business and Global Logistics from the University of Sydney [1].\n",
      "ExampleCV/CV-Oranid.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"Which candidates study in Australia?\"\n",
    "resp, logs_df, used_ctx = rag_functs.master_gemini_rag_bm25_answer_w_log(\n",
    "    client, query,\n",
    "    log_path = \"logs/gemini_usage.csv\",\n",
    "    vector_store = loaded_store,\n",
    "    bm25_index   = bm25_index,\n",
    "    k_vec = 3, k_bm25 = 3, k_final = 2, \n",
    "    rrf_k = 60, w_vec = 1.0, w_bm = 1.0, # equal weight between FAISS and BM25\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "329fd844-788f-48b3-8903-8137ae76cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 108 existing logs (persist=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens_per_request': 3453.7685185185187,\n",
      " 'date_range': (Timestamp('2025-09-08 07:22:02.188114+0000', tz='UTC'),\n",
      "                Timestamp('2025-09-09 09:21:05.743144+0000', tz='UTC')),\n",
      " 'finish_reasons': {'MAX_TOKENS': 5, 'STOP': 95},\n",
      " 'total_requests': 108,\n",
      " 'total_tokens': 373007}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2025-09-09 09:20:15.611136+00:00</td>\n",
       "      <td>\\n    Analyze the uploaded CV and provide the ...</td>\n",
       "      <td>ExampleCV/CV-Oranid.pdf</td>\n",
       "      <td>Here's a summarization of Oranid Yenradee's CV...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>845</td>\n",
       "      <td>664</td>\n",
       "      <td>2637</td>\n",
       "      <td>4146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2025-09-09 09:20:33.691917+00:00</td>\n",
       "      <td>\\n    Analyze the uploaded CV and provide the ...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summary of the uploaded CV for Nattha...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>676</td>\n",
       "      <td>1180</td>\n",
       "      <td>2044</td>\n",
       "      <td>3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2025-09-09 09:20:57.960264+00:00</td>\n",
       "      <td>\\n    Analyze the uploaded CV and provide the ...</td>\n",
       "      <td>ExampleCV/CV-Tun.pdf</td>\n",
       "      <td>Here's a summary of the provided CV:\\n\\n### 1....</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>818</td>\n",
       "      <td>664</td>\n",
       "      <td>3437</td>\n",
       "      <td>4919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2025-09-09 09:21:02.846490+00:00</td>\n",
       "      <td>Which candidates have work experience in Singa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Natthaporn Takpho has work experience in Singa...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>1748</td>\n",
       "      <td>274</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2025-09-09 09:21:05.743144+00:00</td>\n",
       "      <td>Which candidates study in Australia?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oranid Yenradee studies in Australia, pursuing...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>1746</td>\n",
       "      <td>206</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           timestamp  \\\n",
       "103 2025-09-09 09:20:15.611136+00:00   \n",
       "104 2025-09-09 09:20:33.691917+00:00   \n",
       "105 2025-09-09 09:20:57.960264+00:00   \n",
       "106 2025-09-09 09:21:02.846490+00:00   \n",
       "107 2025-09-09 09:21:05.743144+00:00   \n",
       "\n",
       "                                                 query  \\\n",
       "103  \\n    Analyze the uploaded CV and provide the ...   \n",
       "104  \\n    Analyze the uploaded CV and provide the ...   \n",
       "105  \\n    Analyze the uploaded CV and provide the ...   \n",
       "106  Which candidates have work experience in Singa...   \n",
       "107               Which candidates study in Australia?   \n",
       "\n",
       "                       uploaded_file  \\\n",
       "103          ExampleCV/CV-Oranid.pdf   \n",
       "104  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "105             ExampleCV/CV-Tun.pdf   \n",
       "106                              NaN   \n",
       "107                              NaN   \n",
       "\n",
       "                                         response_text finish_reason  \\\n",
       "103  Here's a summarization of Oranid Yenradee's CV...          STOP   \n",
       "104  Here's a summary of the uploaded CV for Nattha...          STOP   \n",
       "105  Here's a summary of the provided CV:\\n\\n### 1....          STOP   \n",
       "106  Natthaporn Takpho has work experience in Singa...          STOP   \n",
       "107  Oranid Yenradee studies in Australia, pursuing...          STOP   \n",
       "\n",
       "    cached_content_token_count candidates_token_count prompt_token_count  \\\n",
       "103                        NaN                    845                664   \n",
       "104                        NaN                    676               1180   \n",
       "105                        NaN                    818                664   \n",
       "106                        NaN                     65               1748   \n",
       "107                        NaN                     34               1746   \n",
       "\n",
       "    thoughts_token_count total_token_count  \n",
       "103                 2637              4146  \n",
       "104                 2044              3900  \n",
       "105                 3437              4919  \n",
       "106                  274              2087  \n",
       "107                  206              1986  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_logger = gemini_log_functs.GeminiUsageLogger(log_path=\"logs/gemini_usage.csv\")\n",
    "pprint.pprint(gemini_logger.get_usage_summary())\n",
    "logs_df = gemini_logger.get_logs_dataframe()\n",
    "logs_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c561f-e875-4d6e-9585-64cb96914780",
   "metadata": {},
   "source": [
    "## Minimal Production Gradio Chatbot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb40088f-cca5-45c7-8e6e-8d86bda2abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://90f75b179c2a9628e6.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 108 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=138, total_tokens=4339\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 109 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=80, total_tokens=3784\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 110 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=80, total_tokens=3744\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 111 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=1292, total_tokens=3997\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 112 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=777, total_tokens=3700\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 113 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=1198, total_tokens=4160\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 114 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=1734, total_tokens=4266\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 115 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=2313, total_tokens=4990\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 116 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=2961, total_tokens=4778\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 117 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=2306, total_tokens=4409\n",
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 118 existing logs (persist=True)\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC remote call 1 is done.\n",
      "INFO:genai_functions.gemini_usage_logging:Added log entry: query_len=3406, total_tokens=5090\n"
     ]
    }
   ],
   "source": [
    "import genai_functions.gradio_chatbot_functions as gradio_functs\n",
    "import genai_functions.hybrid_vectordb_functions as vectordb_functs\n",
    "import google.genai as genai\n",
    "\n",
    "api_key = \"AIzaSyDHOSjzr-AedFPftuIK7iiZ0yTqaTkSDYQ\"\n",
    "vector_dbname = \"vector_and_bm25_dbs/vector_index.faiss\"\n",
    "vector_dbmeta = \"vector_and_bm25_dbs/vector_metadata.jsonl\"\n",
    "bm25_dbmeta   = \"vector_and_bm25_dbs/bm25_metadata.jsonl\" \n",
    "\n",
    "client = genai.Client(api_key = api_key)\n",
    "loaded_store = vectordb_functs.FaissVectorStore.load(vector_dbname, vector_dbmeta)\n",
    "bm25_index   = vectordb_functs.BM25Index.load(bm25_dbmeta)\n",
    "\n",
    "gradio_app = gradio_functs.make_rag_chat_w_gradio(\n",
    "    client       = client,\n",
    "    vector_store = loaded_store,\n",
    "    bm25_index   = bm25_index,\n",
    "    log_path     = \"logs/gemini_usage.csv\",\n",
    ")\n",
    "gradio_app.launch(inline=False, share=True, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fc84a2-82e9-4e78-a149-4f8a2894ffa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6beb94bc-ce16-4960-8a78-2b87a71d8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gradio_app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a04b0b20-dbea-4b2d-8d25-3ca3b352db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:genai_functions.gemini_usage_logging:GeminiUsageLogger initialized with 124 existing logs (persist=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens_per_request': 3619.8629032258063,\n",
      " 'date_range': (Timestamp('2025-09-08 07:22:02.188114+0000', tz='UTC'),\n",
      "                Timestamp('2025-09-09 09:38:41.239376+0000', tz='UTC')),\n",
      " 'finish_reasons': {'MAX_TOKENS': 6, 'STOP': 110},\n",
      " 'total_requests': 124,\n",
      " 'total_tokens': 448863}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2025-09-09 09:31:20.972491+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: I would like to hi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Given Oranid Yenradee's 4 months of direct Dat...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>173</td>\n",
       "      <td>4489</td>\n",
       "      <td>707</td>\n",
       "      <td>5369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2025-09-09 09:32:21.490754+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¸µà¹ˆ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Candidates with Cloud experience are as follow...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>4669</td>\n",
       "      <td>45</td>\n",
       "      <td>4835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2025-09-09 09:34:24.711845+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: I would like to hi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For the role of a Data Scientist focused on re...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187</td>\n",
       "      <td>4871</td>\n",
       "      <td>659</td>\n",
       "      <td>5717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2025-09-09 09:34:44.351970+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸«à¸²à¸„à¸™à¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Candidates matching your requirements are as f...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>358</td>\n",
       "      <td>4928</td>\n",
       "      <td>2146</td>\n",
       "      <td>7432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2025-09-09 09:38:41.239376+00:00</td>\n",
       "      <td>Conversation so far:\\nUser: How about signal p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not found in provided documents.</td>\n",
       "      <td>STOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5080</td>\n",
       "      <td>160</td>\n",
       "      <td>5246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           timestamp  \\\n",
       "119 2025-09-09 09:31:20.972491+00:00   \n",
       "120 2025-09-09 09:32:21.490754+00:00   \n",
       "121 2025-09-09 09:34:24.711845+00:00   \n",
       "122 2025-09-09 09:34:44.351970+00:00   \n",
       "123 2025-09-09 09:38:41.239376+00:00   \n",
       "\n",
       "                                                 query uploaded_file  \\\n",
       "119  Conversation so far:\\nUser: I would like to hi...           NaN   \n",
       "120  Conversation so far:\\nUser: à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¸µà¹ˆ...           NaN   \n",
       "121  Conversation so far:\\nUser: I would like to hi...           NaN   \n",
       "122  Conversation so far:\\nUser: à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸«à¸²à¸„à¸™à¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²...           NaN   \n",
       "123  Conversation so far:\\nUser: How about signal p...           NaN   \n",
       "\n",
       "                                         response_text finish_reason  \\\n",
       "119  Given Oranid Yenradee's 4 months of direct Dat...          STOP   \n",
       "120  Candidates with Cloud experience are as follow...          STOP   \n",
       "121  For the role of a Data Scientist focused on re...          STOP   \n",
       "122  Candidates matching your requirements are as f...          STOP   \n",
       "123                   Not found in provided documents.          STOP   \n",
       "\n",
       "    cached_content_token_count candidates_token_count prompt_token_count  \\\n",
       "119                        NaN                    173               4489   \n",
       "120                        NaN                    121               4669   \n",
       "121                        NaN                    187               4871   \n",
       "122                        NaN                    358               4928   \n",
       "123                        NaN                      6               5080   \n",
       "\n",
       "    thoughts_token_count total_token_count  \n",
       "119                  707              5369  \n",
       "120                   45              4835  \n",
       "121                  659              5717  \n",
       "122                 2146              7432  \n",
       "123                  160              5246  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_logger = gemini_log_functs.GeminiUsageLogger(log_path=\"logs/gemini_usage.csv\")\n",
    "pprint.pprint(gemini_logger.get_usage_summary())\n",
    "logs_df = gemini_logger.get_logs_dataframe()\n",
    "logs_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c09ec-c0c3-468e-866f-feb8288e348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x\n",
    "# Next add FAISS abd BM25\n",
    "# Add Hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b214b-aa2a-45ec-8f27-e6d63706cdbf",
   "metadata": {},
   "source": [
    "# >> V0: POC Code version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250050a-406d-4f6e-a6b3-9bcd24fdebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai as genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ca7cb-e3fa-4304-ae01-bd371c1b0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d90d2-c968-4091-80cc-44197240254b",
   "metadata": {},
   "source": [
    "### APIs Usage Log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8ea5b-ec72-4a9c-bc8b-7b83845a91c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "LOG_COLUMNS = [\n",
    "    \"timestamp\",\"query\",\"uploaded_file\",\"response_text\",\"finish_reason\",\n",
    "    \"cached_content_token_count\",\"candidates_token_count\",\n",
    "    \"prompt_token_count\",\"thoughts_token_count\",\"total_token_count\",\n",
    "]\n",
    "\n",
    "def ensure_logs_df(logs_df: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    if logs_df is None or logs_df.empty:\n",
    "        # Initialize with correct columns (no rows). Keep dtypes flexible.\n",
    "        logs_df = pd.DataFrame({c: pd.Series(dtype=\"object\") for c in LOG_COLUMNS})\n",
    "    # Guarantee column order/superset\n",
    "    for c in LOG_COLUMNS:\n",
    "        if c not in logs_df.columns:\n",
    "            logs_df[c] = pd.Series(dtype=\"object\")\n",
    "    return logs_df[LOG_COLUMNS]\n",
    "\n",
    "def append_usage_log(logs_df, query_text, uploaded_file, resp=None):\n",
    "    \"\"\"\n",
    "    Append a new usage log entry to an existing DataFrame (no FutureWarning).\n",
    "    \"\"\"\n",
    "    logs_df = ensure_logs_df(logs_df)\n",
    "\n",
    "    # Safe extraction\n",
    "    usage = getattr(resp, \"usage_metadata\", None) if resp else None\n",
    "    try:\n",
    "        finish_reason = resp.candidates[0].finish_reason.name if (resp and getattr(resp, \"candidates\", None)) else None\n",
    "    except Exception:\n",
    "        finish_reason = None\n",
    "    output_text = getattr(resp, \"text\", None) if resp else None\n",
    "\n",
    "    # Build row\n",
    "    new_row = {\n",
    "        \"timestamp\": datetime.now(timezone.utc),  # tz-aware\n",
    "        \"query\": query_text,\n",
    "        \"uploaded_file\": uploaded_file,\n",
    "        \"response_text\": output_text,\n",
    "        \"finish_reason\": finish_reason,\n",
    "        \"cached_content_token_count\": getattr(usage, \"cached_content_token_count\", None) if usage else None,\n",
    "        \"candidates_token_count\": getattr(usage, \"candidates_token_count\", None) if usage else None,\n",
    "        \"prompt_token_count\": getattr(usage, \"prompt_token_count\", None) if usage else None,\n",
    "        \"thoughts_token_count\": getattr(usage, \"thoughts_token_count\", None) if usage else None,\n",
    "        \"total_token_count\": getattr(usage, \"total_token_count\", None) if usage else None,\n",
    "    }\n",
    "\n",
    "    # Append without concat (avoids the deprecation)\n",
    "    logs_df.loc[len(logs_df)] = [new_row.get(c, None) for c in LOG_COLUMNS]\n",
    "    return logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4748cf-0a08-4850-b156-7d801489eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = pd.DataFrame({\n",
    "    \"timestamp\": pd.Series(dtype=\"datetime64[ns]\"),\n",
    "    \"query\": pd.Series(dtype=\"string\"),\n",
    "    \"uploaded_file\": pd.Series(dtype=\"string\"),\n",
    "    \"response_text\": pd.Series(dtype=\"string\"),\n",
    "    \"finish_reason\": pd.Series(dtype=\"string\"),\n",
    "    \"cached_content_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"candidates_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"prompt_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"thoughts_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"total_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "})\n",
    "logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c447d1e-69b5-4917-bb8c-06f1598de977",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyDHOSjzr-AedFPftuIK7iiZ0yTqaTkSDYQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d15e6b-f236-42ef-bdde-3ff92c3df7b6",
   "metadata": {},
   "source": [
    "### Fail to get response example due to 'MAX_TOKENS'>\n",
    "GenerateContentResponse(\n",
    "  automatic_function_calling_history=[],\n",
    "  candidates=[\n",
    "    Candidate(\n",
    "      content=Content(\n",
    "        role='model'\n",
    "      ),\n",
    "      finish_reason=<FinishReason.MAX_TOKENS: 'MAX_TOKENS'>,\n",
    "      index=0\n",
    "    ),\n",
    "  ],\n",
    "  model_version='gemini-2.5-flash',\n",
    "  response_id='tPemaLqfEaOtmtkPiLTW4QI',\n",
    "  sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "  ),\n",
    "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
    "    prompt_token_count=8,\n",
    "    prompt_tokens_details=[\n",
    "      ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=8\n",
    "      ),\n",
    "    ],\n",
    "    thoughts_token_count=199,\n",
    "    total_token_count=207\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fbe35-bc18-4d55-b451-61e7ebfe3dfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": \"Give 3 bullets about ADC.\"}]}],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.6,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Robust extractor\n",
    "def extract_text(r):\n",
    "    out = []\n",
    "    for c in getattr(r, \"candidates\", []) or []:\n",
    "        # finish_reason can be useful: types.FinishReason.STOP, SAFETY, etc.\n",
    "        # print(\"finish_reason:\", c.finish_reason)\n",
    "        content = getattr(c, \"content\", None)\n",
    "        if content:\n",
    "            for p in getattr(content, \"parts\", []) or []:\n",
    "                t = getattr(p, \"text\", None)\n",
    "                if t:\n",
    "                    out.append(t)\n",
    "    # Fallback to r.text if present\n",
    "    return \"\\n\".join(out) or getattr(r, \"text\", \"\") or \"\"\n",
    "\n",
    "print(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b7b04-5d66-4954-b65a-ffed89e9a7b9",
   "metadata": {},
   "source": [
    "### PDF upload with text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bee6c1-dad8-441d-88ac-313bea78292c",
   "metadata": {},
   "source": [
    "#### Step1: CV Extraction and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b178347-6027-4071-9110-d153be1f9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the datetime object into a string\n",
    "formatted_today = datetime.utcnow().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98f352-b2e9-4323-bd1a-4b203db2a8cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def master_gemini_upload_cv_prompt_w_log(\n",
    "    uploaded_filename, prompt_text, logs_df,\n",
    "    system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\",\n",
    "    max_output_tokens = 4096,\n",
    "    temperature       = 0.1,\n",
    "    top_p             = 0.9, # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "    top_k             = 40,  # At each step, the model only considers the top_k most likely tokens.\n",
    "):\n",
    "    # Step 1: Upload the PDF\n",
    "    uploaded_file = client.files.upload(\n",
    "        file=uploaded_filename, \n",
    "        config={\"display_name\": \"CV\"}\n",
    "    )\n",
    "\n",
    "    # Step 2: Pass the file reference into the request\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt_text},\n",
    "                    {\"file_data\": {\"file_uri\": uploaded_file.uri}}\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            top_k = top_k,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"resp.text:\\n {resp.text}\")\n",
    "        size_info = calc_resp_text_size(resp)\n",
    "        print(f\"[resp.text size] chars={size_info['char_length']}, tokensâ‰ˆ{size_info['token_est']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"resp.text: Not Found ({e})\")\n",
    "        size_info = {\"char_length\": 0, \"token_est\": 0}\n",
    "        \n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=prompt_text,\n",
    "        uploaded_file=uploaded_filename,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df\n",
    "\n",
    "def calc_resp_text_size(resp) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the size of resp.text in characters and estimated tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    resp : object\n",
    "        Gemini response object (with .text)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : {\"char_length\": int, \"token_est\": int}\n",
    "    \"\"\"\n",
    "    if not hasattr(resp, \"text\") or resp.text is None:\n",
    "        return {\"char_length\": 0, \"token_est\": 0}\n",
    "    \n",
    "    txt = resp.text\n",
    "    char_length = len(txt)\n",
    "    token_est   = len(txt.split())  # rough estimate\n",
    "    \n",
    "    return {\"char_length\": char_length, \"token_est\": token_est}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a462b-c41a-4aaf-a039-e529727a474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploaded_filename = \"ExampleCV/NLP-CV-NachaiLim.pdf\"\n",
    "# uploaded_filename = \"ExampleCV/CV-Oranid.pdf\"\n",
    "uploaded_filename = \"ExampleCV/Natthaporn_CV2022.pdf\"\n",
    "system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\"\n",
    "# prompt_text       = \"Who is the individual in the CV? Summarize the uploaded CV's education and work experience in 5-10 concise bullet points with experience years. Then list his/her skills in python list.\"\n",
    "prompt_text       = \"\"\"\n",
    "Analyze the uploaded CV and provide the following:\n",
    "\n",
    "1. Identify the individual (full name).\n",
    "2. Summarize education and work experience in **10â€“15 concise bullet points**, including years of experience for each role.\n",
    "3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "\"\"\"\n",
    "\n",
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cd0d6-b315-4a03-9561-d56575dadbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_cv, logs_df = master_gemini_upload_cv_prompt_w_log(\n",
    "    uploaded_filename, prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a380769-0806-49f6-ad39-c0f3ef82923a",
   "metadata": {},
   "source": [
    "#### Step2: JD Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b5b0c-734c-4291-a294-ac39fbc5e704",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def master_gemini_jd_prompt_w_log(\n",
    "    prompt_text, logs_df,\n",
    "    system_text       = f\"\"\"\n",
    "    You are an expert HR assistant, \n",
    "    make a summarization of the job description in a list of Key responsibilities, Mandaotry experiences & skills, Prefer experiences & skills.\n",
    "    \"\"\",\n",
    "    max_output_tokens = 4096,\n",
    "    temperature       = 0.1,\n",
    "    top_p             = 0.9, # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "    top_k             = 40,  # At each step, the model only considers the top_k most likely tokens.\n",
    "):\n",
    "\n",
    "    # Step 1: Pass the file reference into the request\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt_text},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            top_k = top_k,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"resp.text:\\n {format_job_description(resp.text)}\")\n",
    "    except:\n",
    "        print(f\"resp.text:\\n Not Found\")\n",
    "        \n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=prompt_text,\n",
    "        uploaded_file=uploaded_filename,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df\n",
    "\n",
    "def format_job_description(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and format a job description string into a more readable format.\n",
    "    Supports Markdown-style output for readability.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from textwrap import dedent\n",
    "\n",
    "    # Step 1: Replace \\n escape sequences with real line breaks\n",
    "    formatted = raw_text.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Step 2: Remove leading/trailing quotes or boilerplate\n",
    "    formatted = re.sub(r'^\"|\"$', '', formatted.strip())\n",
    "\n",
    "    # Step 3: Normalize multiple blank lines\n",
    "    formatted = re.sub(r'\\n{3,}', '\\n\\n', formatted)\n",
    "\n",
    "    # Step 4: Dedent to align properly\n",
    "    formatted = dedent(formatted)\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67911427-9d25-4f8b-8b05-47f2ea8f8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = f\"\"\"\n",
    "Senior Data Scientist\n",
    "\n",
    "à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸žà¸·à¹‰à¸™à¸à¸²à¸™ 5-10 à¸›à¸µ à¸›à¸£à¸´à¸à¸à¸²à¸•à¸£à¸µà¸‚à¸¶à¹‰à¸™à¹„à¸›\n",
    "\n",
    "à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‡à¸²à¸™à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™ Key responsibilities: â€¢ Interpret data, and analyze results using statistical methods â€¢ Prepare and deliver business reports that effectively communicate trends, patterns, risks, and insights using data â€¢ Research, design, and develop machine-learning/artificial-intelligence/computer-vision systems to address key business challenges â€¢ Perform data quality testing, validation, and assurance as a part of designing, and implementing scalable data solutions â€¢ Support identification, triage, and remediation of data quality issues across the data and technology organizations â€¢ Effectively manage and develop small data analytics teams â€¢ Effectively engage and partner with team members across data analytics, technology development, and business strategy to drive outcomes and impact â€¢ Ensure end user requirements of data solutions are effectively met â€¢ Develop user friendly documentation to communicate the use and value of data solutions â€¢ Identify and push forward process improvement opportunities and solutions â€¢ Review and keep up to date with developments in the data analytics and ML fields\n",
    "\n",
    "Education: â€¢ B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science\n",
    "\n",
    "Relevant experience required: â€¢ 5+ years hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health / monitoring frameworks â€¢ Proficiency with one or more programming languages or data engineering frameworks, such as Python, SQL, Spark, Java, C++, TypeScript/JavaScript, or similar â€¢ Proficiency with any of the major machine learning and computer vision frameworks preferred â€¢ Hands-on experience on BI solutions (e.g. Tableau, Power BI, Qlik, Looker) â€¢ Ability to work well with cross-disciplinary teams â€¢ Strong numerical and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy â€¢ Communicate in English fluently\n",
    "\n",
    "ðŸ˜† Our Benefits: Â· Free lunch every Tuesday and Friday Â· WFH/WFA (Work From Home/Work From Anywhere) Â· Group insurance Â· Retirement savings fund Â· Well-stocked Snack Bar with snacks and beverages Â· Dress in your own style at work Â· Annual vacation 15 days\n",
    "\"\"\"\n",
    "\n",
    "system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "make a summarization of the job description in a list of Key responsibilities, Mandaotry experiences & skills, Prefer experiences & skills.\n",
    "\"\"\"\n",
    "max_output_tokens = 4096\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb5eae-e2b5-48e3-9e04-536f799927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_jd, logs_df = master_gemini_jd_prompt_w_log(    \n",
    "    prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1e24e-e46c-4997-ba64-2f0b5e396a00",
   "metadata": {},
   "source": [
    "#### Step3: JD vs CV scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5523570-ea8d-4167-9c61-56c9579366d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_jd.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c093a28-63e6-4cff-b0e7-d1509a5a688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_cv.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa82b9-a129-42e1-aa02-0ac2068d194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_template = \"\"\"\n",
    "### JD Mandatory Requirements [âœ“: Pass | âœ—: Missing]\n",
    "1. Requirement 1 â€” [âœ“/âœ—]\n",
    "2. Requirement 2 â€” [âœ“/âœ—]\n",
    "\n",
    "### JD Preferred Requirements [âœ“: Pass | âœ—: Missing]\n",
    "1. Requirement 1 â€” [âœ“/âœ—]\n",
    "2. Requirement 2 â€” [âœ“/âœ—]\n",
    "\n",
    "### Candidate Strengths (1â€“5 bullets)\n",
    "1. Strength 1\n",
    "2. Strength 2\n",
    "\n",
    "### Candidate Weaknesses (1â€“5 bullets)\n",
    "1. Weakness 1\n",
    "2. Weakness 2\n",
    "\n",
    "### JD vs CV Matching Score  \n",
    "**Score:** X.X / 10.0  \n",
    "\n",
    "**Reasoning (3â€“5 bullets):**  \n",
    "1. Reason 1  \n",
    "2. Reason 2  \n",
    "3. Reason 3  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dbbad-e072-4382-9db4-8c40db9b4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "Filling in the output_template below based on the given job description (JD) and summarized resume (CV).\n",
    "Only answer in the given output_template.\n",
    "\n",
    "# output_template:\n",
    "{output_template}\n",
    "\"\"\"\n",
    "\n",
    "prompt_text = f\"\"\"\n",
    "# job description (JD):\n",
    "{format_job_description(resp_jd.text)}\n",
    "\n",
    "# summarized resume (CV):\n",
    "{resp_cv.text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e4fcf-100e-4dd4-a813-e0f2f5976192",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04d81b-15bf-43db-bf0c-ce4590f08ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_summary, logs_df = master_gemini_jd_prompt_w_log(    \n",
    "    prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35925d7-d2e4-42cf-93b9-8455365d04e2",
   "metadata": {},
   "source": [
    "## Terra job description example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b441f-f176-429a-a826-ef7fc5adccfa",
   "metadata": {},
   "source": [
    "source: https://www.jobfinfin.com/job/659cf72b64805d54e65dc789\n",
    "\n",
    "Senior Data Scientist\n",
    "\n",
    "à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸žà¸·à¹‰à¸™à¸à¸²à¸™\n",
    "5-10 à¸›à¸µ\n",
    "à¸›à¸£à¸´à¸à¸à¸²à¸•à¸£à¸µà¸‚à¸¶à¹‰à¸™à¹„à¸›\n",
    "\n",
    "à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸‡à¸²à¸™à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™\n",
    "Key responsibilities:\n",
    "â€¢ Interpret data, and analyze results using statistical methods\n",
    "â€¢ Prepare and deliver business reports that effectively communicate trends, patterns, risks, and insights using data\n",
    "â€¢ Research, design, and develop machine-learning/artificial-intelligence/computer-vision systems to address key business challenges \n",
    "â€¢ Perform data quality testing, validation, and assurance as a part of designing, and implementing scalable data solutions\n",
    "â€¢ Support identification, triage, and remediation of data quality issues across the data and technology organizations\n",
    "â€¢ Effectively manage and develop small data analytics teams\n",
    "â€¢ Effectively engage and partner with team members across data analytics, technology development, and business strategy to drive outcomes and impact\n",
    "â€¢ Ensure end user requirements of data solutions are effectively met\n",
    "â€¢ Develop user friendly documentation to communicate the use and value of data solutions\n",
    "â€¢ Identify and push forward process improvement opportunities and solutions\n",
    "â€¢ Review and keep up to date with developments in the data analytics and ML fields\n",
    " \n",
    "Education:\n",
    "â€¢ B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science\n",
    " \n",
    "Relevant experience required:\n",
    "â€¢ 5+ years hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health / monitoring frameworks\n",
    "â€¢ Proficiency with one or more programming languages or data engineering frameworks, such as Python, SQL, Spark, Java, C++, TypeScript/JavaScript, or similar\n",
    "â€¢ Proficiency with any of the major machine learning and computer vision frameworks preferred\n",
    "â€¢ Hands-on experience on BI solutions (e.g. Tableau, Power BI, Qlik, Looker)\n",
    "â€¢ Ability to work well with cross-disciplinary teams\n",
    "â€¢ Strong numerical and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy\n",
    "â€¢ Communicate in English fluently\n",
    "\n",
    "ðŸ˜† Our Benefits:\n",
    "Â· Free lunch every Tuesday and Friday\n",
    "Â· WFH/WFA (Work From Home/Work From Anywhere)\n",
    "Â· Group insurance\n",
    "Â· Retirement savings fund\n",
    "Â· Well-stocked Snack Bar with snacks and beverages\n",
    "Â· Dress in your own style at work\n",
    "Â· Annual vacation 15 days "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c76734-3408-4d43-9d3e-83c4ea8bc6cd",
   "metadata": {},
   "source": [
    "# Text Embedding for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe52ad-9f41-4256-8977-246ac1db178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from google import genai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffef4c6-0271-406b-a7aa-feea7cd9b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Cloud adoption helps startups scale infrastructure with low upfront costs.\",\n",
    "    \"Hybrid cloud combines private and public resources for flexibility.\",\n",
    "    \"On-premise solutions give companies more control but higher costs.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf82ca7-92d5-4951-8c22-516f6fbe07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embeddings = [\n",
    "    client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": doc}]}]\n",
    "    ).embeddings[0].values\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "# Convert to NumPy array for FAISS\n",
    "emb_matrix = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index (cosine similarity â†’ use inner product + normalized vectors)\n",
    "faiss.normalize_L2(emb_matrix)\n",
    "index = faiss.IndexFlatIP(emb_matrix.shape[1])\n",
    "index.add(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefc1fa-fdbe-4558-a9ed-8a55dcb1803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for nearest docs\n",
    "query = \"What are the benefits of cloud computing?\"\n",
    "query_emb = client.models.embed_content(\n",
    "    model=\"text-embedding-004\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": query}]}]\n",
    ").embeddings[0].values\n",
    "\n",
    "query_emb = np.array([query_emb]).astype(\"float32\")\n",
    "faiss.normalize_L2(query_emb)  # only if using cosine/IP index\n",
    "\n",
    "D, I = index.search(query_emb, k=2)  # distances, indices\n",
    "retrieved_docs = [docs[i] for i in I[0]]\n",
    "print(\"Retrieved docs:\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28690e-d8a2-40cc-bd66-4bab7adf276a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_token_count(text: str) -> int:\n",
    "    \"\"\"Very rough token estimate (whitespace split).\n",
    "    Replace with your tokenizer if needed.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "class FaissVectorStore:\n",
    "    \"\"\"\n",
    "    Minimal FAISS + metadata store with cosine similarity search.\n",
    "    - Stores vectors in FAISS (IndexFlatIP) with L2-normalized vectors (so IP == cosine sim)\n",
    "    - Stores metadata separately (JSONL or pickle)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatIP(dim)  # inner product (use with normalized vectors)\n",
    "        self._vectors = []                  # keep in RAM until saved (optional)\n",
    "        self._metadata: List[Dict] = []     # [{\"text\":..., \"timestamp\":..., \"token_length\":...}, ...]\n",
    "\n",
    "    # ---------- Embedding helpers (plug your embedding client here) ----------\n",
    "    @staticmethod\n",
    "    def _to_float32(arr) -> np.ndarray:\n",
    "        return np.array(arr, dtype=\"float32\")\n",
    "\n",
    "    def add_texts_and_metadata(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadata_list: Optional[List[Dict]] = None,\n",
    "        default_metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add texts with precomputed embeddings + flexible per-item metadata.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        texts : List[str]\n",
    "            Plaintext chunks to index.\n",
    "        embeddings : List[List[float]]\n",
    "            Precomputed embeddings matching each text (dim == self.dim).\n",
    "            These will be L2-normalized for cosine similarity with IndexFlatIP.\n",
    "        metadata_list : Optional[List[Dict]]\n",
    "            A list of metadata dicts (one per text). Each dict can contain any keys\n",
    "            (JSONL-friendly). Missing keys are allowed and will be auto-filled where applicable.\n",
    "            If None, minimal metadata will be generated.\n",
    "        default_metadata : Optional[Dict]\n",
    "            Default metadata merged into each item *before* item-specific overrides.\n",
    "            Example: {\"source\": \"my_corpus\", \"tag\": \"v1\"}.\n",
    "    \n",
    "        Behavior\n",
    "        --------\n",
    "        - For each item i, final metadata = {**default_metadata, **metadata_list[i]} (if provided)\n",
    "        - Auto-fill:\n",
    "            * \"timestamp\" (UTC ISO 8601) if not present\n",
    "            * \"token_length\" via simple_token_count(text) if not present\n",
    "        - All keys are preserved as-is to support JSONL dumps without schema constraints.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If lengths of texts, embeddings, and metadata_list (when provided) mismatch.\n",
    "        \"\"\"\n",
    "        n = len(texts)\n",
    "        if len(embeddings) != n:\n",
    "            raise ValueError(\"embeddings length must match texts length\")\n",
    "    \n",
    "        if metadata_list is not None and len(metadata_list) != n:\n",
    "            raise ValueError(\"metadata_list length must match texts length when provided\")\n",
    "    \n",
    "        # Normalize and add vectors\n",
    "        vecs = np.asarray(embeddings, dtype=\"float32\")\n",
    "        faiss.normalize_L2(vecs)\n",
    "        self.index.add(vecs)\n",
    "        self._vectors.extend(vecs)\n",
    "    \n",
    "        # Prepare defaults\n",
    "        default_metadata = default_metadata or {}\n",
    "    \n",
    "        # Build metadata rows JSONL-friendly (arbitrary keys preserved)\n",
    "        now_iso = datetime.utcnow().isoformat()\n",
    "        for i, t in enumerate(texts):\n",
    "            item_meta = metadata_list[i] if metadata_list is not None else {}\n",
    "            # Merge: defaults first, then item-specific override\n",
    "            meta = {**default_metadata, **item_meta}\n",
    "    \n",
    "            # Auto-fill timestamp if missing\n",
    "            meta.setdefault(\"timestamp\", now_iso)\n",
    "            # Auto-fill token_length if missing (won't overwrite if provided)\n",
    "            meta.setdefault(\"token_length\", simple_token_count(t))\n",
    "            # Always store the raw text (if you want to make it optional, remove this line)\n",
    "            meta.setdefault(\"text\", t)\n",
    "    \n",
    "            self._metadata.append(meta)\n",
    "            \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clear all vectors and metadata from the store.\n",
    "        This reinitializes the FAISS index and empties metadata.\n",
    "        \"\"\"\n",
    "        # Recreate a fresh FAISS index with the same dimension\n",
    "        self.index = faiss.IndexFlatIP(self.dim)  # cosine sim (normalized IP)\n",
    "        \n",
    "        # Reset vectors and metadata\n",
    "        self._vectors = []\n",
    "        self._metadata = []         \n",
    "\n",
    "    def search(self, query_embedding: List[float], k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Search the index by embedding. Returns top-k as [(metadata, similarity), ...].\n",
    "        Similarity is cosine similarity (since we normalized and use IP index).\n",
    "        \"\"\"\n",
    "        q = self._to_float32([query_embedding])\n",
    "        faiss.normalize_L2(q)  # normalize query for cosine/IP\n",
    "        D, I = self.index.search(q, k)\n",
    "        hits = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1:  # no result\n",
    "                continue\n",
    "            hits.append((self._metadata[idx], float(score)))  # score âˆˆ [-1, 1]\n",
    "        return hits\n",
    "\n",
    "    # ---------- Persistence ----------\n",
    "    def save(self, index_path: str, metadata_path: str):\n",
    "        \"\"\"Save FAISS index and metadata (JSONL if .jsonl else pickle).\"\"\"\n",
    "        faiss.write_index(self.index, index_path)\n",
    "\n",
    "        # Save metadata (choose JSONL by default for readability)\n",
    "        if metadata_path.endswith(\".jsonl\"):\n",
    "            with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for row in self._metadata:\n",
    "                    f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "        else:\n",
    "            with open(metadata_path, \"wb\") as f:\n",
    "                pickle.dump(self._metadata, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, index_path: str, metadata_path: str):\n",
    "        \"\"\"Load FAISS index and metadata; returns an initialized store.\"\"\"\n",
    "        index = faiss.read_index(index_path)\n",
    "\n",
    "        # Load metadata\n",
    "        if metadata_path.endswith(\".jsonl\"):\n",
    "            metadata = []\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    metadata.append(json.loads(line))\n",
    "        else:\n",
    "            with open(metadata_path, \"rb\") as f:\n",
    "                metadata = pickle.load(f)\n",
    "\n",
    "        dim = index.d  # read dimension from index\n",
    "        store = cls(dim)\n",
    "        store.index = index\n",
    "        store._metadata = metadata\n",
    "        return store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a773b-ce50-44ae-bdb2-5b95bd32a4a2",
   "metadata": {},
   "source": [
    "# BM25 for keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a75061-de4f-4d84-84e1-cffcb3f88ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd12173-6b6e-4529-ab2a-29990ebe97c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    \"\"\"\n",
    "    Persistent BM25 index with UNIQUE docs keyed by doc_id.\n",
    "    - doc_id := doc['id'] if present, else doc['uploaded_filename']\n",
    "    - add_or_replace(): replaces existing doc with same doc_id\n",
    "    - load(): dedupes by doc_id (last one wins)\n",
    "    \"\"\"\n",
    "    def __init__(self, by_id=None):\n",
    "        self.by_id = by_id or {}  # doc_id -> meta (includes 'text')\n",
    "        self._bm25 = None\n",
    "        self._ids = []     # order aligned with _tokens\n",
    "        self._tokens = []  # tokenized texts in same order\n",
    "        self._docs = {} \n",
    "\n",
    "    # ---------- persistence ----------\n",
    "    @classmethod\n",
    "    def load(cls, meta_path: str):\n",
    "        by_id = {}\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    d = json.loads(line)\n",
    "                    doc_id = d.get(\"id\") or d.get(\"uploaded_filename\")\n",
    "                    if not doc_id:\n",
    "                        # skip malformed rows\n",
    "                        continue\n",
    "                    # normalize stored id & text\n",
    "                    d[\"id\"] = doc_id\n",
    "                    d[\"text\"] = (d.get(\"text\") or \"\").strip()\n",
    "                    # last one wins for same doc_id\n",
    "                    by_id[doc_id] = d\n",
    "        return cls(by_id)\n",
    "\n",
    "    def save(self, meta_path: str):\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc_id, d in self.by_id.items():\n",
    "                f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def clear(self):\n",
    "        self.by_id.clear()\n",
    "        self._bm25 = None\n",
    "        self._ids = []\n",
    "        self._tokens = []\n",
    "\n",
    "    # ---------- core ops ----------\n",
    "    def _rebuild(self):\n",
    "        # build arrays in a stable order\n",
    "        self._ids = list(self.by_id.keys())\n",
    "        self._tokens = [_tokenize(self.by_id[i].get(\"text\", \"\")) for i in self._ids]\n",
    "        self._bm25 = BM25Okapi(self._tokens)\n",
    "\n",
    "    def add_or_replace(self, doc_meta: dict):\n",
    "        doc_id = doc_meta.get(\"id\") or doc_meta.get(\"uploaded_filename\")\n",
    "        if not doc_id:\n",
    "            raise ValueError(\"doc_meta must include 'id' or 'uploaded_filename'\")\n",
    "        # normalize\n",
    "        doc_meta = {**doc_meta, \"id\": doc_id, \"text\": (doc_meta.get(\"text\") or \"\").strip()}\n",
    "        self.by_id[doc_id] = doc_meta\n",
    "        self._bm25 = None  # mark dirty (lazy rebuild)\n",
    "\n",
    "    def search(self, query: str, k: int = 5):\n",
    "        if not self.by_id:\n",
    "            return []\n",
    "        if self._bm25 is None:\n",
    "            self._rebuild()\n",
    "        q_tokens = _tokenize(query or \"\")\n",
    "        scores = self._bm25.get_scores(q_tokens)\n",
    "        order = np.argsort(scores)[::-1]\n",
    "        hits = []\n",
    "        for i in order:\n",
    "            doc_id = self._ids[i]\n",
    "            meta = self.by_id[doc_id]\n",
    "            hits.append((meta, float(scores[i])))\n",
    "            if len(hits) >= k:\n",
    "                break\n",
    "        return hits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._docs)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self._docs)\n",
    "\n",
    "def _tokenize(txt: str):\n",
    "    return re.findall(r\"[A-Za-z0-9\\-]+\", (txt or \"\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bdc3e-3f20-4d1c-8235-7745b0c44990",
   "metadata": {},
   "source": [
    "# Creating Hybrid index with RAG and BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a21cc-5b2d-4f0e-ac89-0c836b3da839",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def embed_texts_with_gemini(client, texts: List[str], embedding_modelname:str =\"text-embedding-004\") -> List[List[float]]:\n",
    "    emb_list = []\n",
    "    for t in texts:\n",
    "        emb = client.models.embed_content(\n",
    "            model=embedding_modelname,\n",
    "            contents=[{\"role\": \"user\", \"parts\": [{\"text\": t}]}]\n",
    "        ).embeddings[0].values\n",
    "        emb_list.append(emb)\n",
    "    return emb_list\n",
    "\n",
    "def embed_query_with_gemini(client, query: str, embedding_modelname:str =\"text-embedding-004\") -> List[float]:\n",
    "    resp = client.models.embed_content(\n",
    "        model=embedding_modelname,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": query}]}]\n",
    "    )\n",
    "    embed_query = resp.embeddings[0].values\n",
    "    \n",
    "    return embed_query, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafb3a7-0fdb-44f6-998e-07b0445471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the datetime object into a string\n",
    "formatted_today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\"\n",
    "prompt_text       = \"\"\"\n",
    "Analyze the uploaded CV and provide the following:\n",
    "\n",
    "1. Identify the individual (full name).\n",
    "2. Summarize education and work experience in **10â€“15 concise bullet points**, including years of experience for each role.\n",
    "3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "\"\"\"\n",
    "\n",
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability â‰¥ top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55d6ac-df91-4dd3-9ce6-c00bac717f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_filenames_list = [\n",
    "    \"ExampleCV/NLP-CV-NachaiLim.pdf\",\n",
    "    \"ExampleCV/CV-Oranid.pdf\",\n",
    "    \"ExampleCV/Natthaporn_CV2022.pdf\",\n",
    "]\n",
    "applied_position = \"senior data scientist\"\n",
    "embedding_modelname = \"text-embedding-004\"\n",
    "vector_dbname = \"vector_and_bm25_dbs/vector_index.faiss\"\n",
    "vector_dbmeta = \"vector_and_bm25_dbs/vector_metadata.jsonl\"\n",
    "bm25_dbmeta   = \"vector_and_bm25_dbs/bm25_metadata.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f37ae-1a5b-4fa2-aaa6-efd6b82145cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Load / init FAISS (and clear) =========\n",
    "try:\n",
    "    loaded_store = FaissVectorStore.load(vector_dbname, vector_dbmeta)\n",
    "    loaded_store.clear()\n",
    "    loaded_store.save(vector_dbname, vector_dbmeta)  # persist cleared index\n",
    "except Exception:\n",
    "    print(f\"Fail to load '{vector_dbname}' & '{vector_dbmeta}', creating new vector DB instead\")\n",
    "    loaded_store = FaissVectorStore(dim=768)\n",
    "    loaded_store.save(vector_dbname, vector_dbmeta)  # <-- persist empty index here\n",
    "\n",
    "# ========= Load / init BM25 (and clear like FAISS) =========\n",
    "try:\n",
    "    bm25_index = BM25Index.load(bm25_dbmeta)\n",
    "    bm25_index.clear()\n",
    "    bm25_index.save(bm25_dbmeta)\n",
    "except Exception:\n",
    "    bm25_index = BM25Index()\n",
    "    bm25_index.save(bm25_dbmeta)\n",
    "# ==================================\n",
    "# Ingest loop (load -> add -> save)\n",
    "# ==================================\n",
    "for idx, uploaded_filename in enumerate(uploaded_filenames_list):\n",
    "    print(f\"\\n==== Start Processing {idx}st CV from {len(uploaded_filenames_list)} CVs: {uploaded_filename} ====\")\n",
    "    try:\n",
    "        print(f\">>>>>  Start Summarizing: {uploaded_filename}\")\n",
    "        # Step 1: Raw CV to summarized CV\n",
    "        resp_cv, logs_df = master_gemini_upload_cv_prompt_w_log(\n",
    "            uploaded_filename, prompt_text, logs_df,\n",
    "            system_text       = system_text,\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature       = temperature,\n",
    "            top_p             = top_p, \n",
    "            top_k             = top_k, \n",
    "        )\n",
    "\n",
    "        # Step 2: Embedding the summarized CV\n",
    "        print(f\">>>>>  Start Embedding: {uploaded_filename}\")\n",
    "        doc_text = (resp_cv.text or \"\").strip()\n",
    "        docs = [doc_text]\n",
    "        per_item_meta = [{\"id\": idx, \"uploaded_filename\": uploaded_filename, \"applied_position\": applied_position},]\n",
    "        defaults = {\"embedding_model\": embedding_modelname, \"project\": \"cv-summarization\"}\n",
    "        doc_embs = embed_texts_with_gemini(client, docs, embedding_modelname = embedding_modelname)\n",
    "\n",
    "        # Step 3: Saving the embedding\n",
    "        print(f\">>>>>  Start Saving to FAISS db + Meta data: {uploaded_filename}\")\n",
    "        loaded_store = FaissVectorStore.load(vector_dbname, vector_dbmeta)\n",
    "        loaded_store.add_texts_and_metadata(\n",
    "            texts=docs,\n",
    "            embeddings=doc_embs,\n",
    "            metadata_list=per_item_meta,\n",
    "            default_metadata=defaults,\n",
    "        )\n",
    "        loaded_store.save(vector_dbname, vector_dbmeta)\n",
    "\n",
    "        # Step 4: Save to BM25 (load -> add -> save, same style)\n",
    "        print(f\">>>>>  Start Adding BM25 index: {uploaded_filename}\")\n",
    "        # reload current BM25 from disk to mirror FAISS pattern (optional but \"same way\")\n",
    "        bm25_index = BM25Index.load(bm25_dbmeta)\n",
    "        bm25_meta = {**defaults, **per_item_meta[0], \"text\": doc_text}\n",
    "        bm25_index.add_or_replace(bm25_meta)\n",
    "        bm25_index.save(bm25_dbmeta)\n",
    "        print(f\"Added {uploaded_filename} to BM25 (total {len(bm25_index)} docs).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload and/or extract CV: {e}\")\n",
    "print(f\"\\n==== Completed Processing {len(uploaded_filenames_list)} CVs ====\")\n",
    "logs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251db35-5c27-42ca-ae82-aeca476f6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Example query after loop =========\n",
    "query = \"Study in Australia\"\n",
    "q_emb, _ = embed_query_with_gemini(client, query, embedding_modelname=embedding_modelname)\n",
    "\n",
    "vec_hits = loaded_store.search(q_emb, k=3)\n",
    "bm25_hits = bm25_index.search(query, k=3)\n",
    "\n",
    "def show(hits, title):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for meta, score in hits:\n",
    "        print(f\"[score={score:.4f}] {meta['uploaded_filename']}\")\n",
    "\n",
    "show(vec_hits, \"Vector Search\")\n",
    "show(bm25_hits, \"BM25 Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c924a-445e-4e37-8edc-169439c8412b",
   "metadata": {},
   "source": [
    "# Creating context prompt with RAG & BM25 from query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb3735-d690-4326-bbb5-23a742d95e68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _doc_id(m: dict) -> str:\n",
    "    return m.get(\"id\") or m.get(\"uploaded_filename\") or str(id(m))\n",
    "\n",
    "def _safe(txt: str) -> str:\n",
    "    return (txt or \"\").strip()\n",
    "\n",
    "def _truncate_chars(txt: str, max_chars: int) -> str:\n",
    "    t = _safe(txt)\n",
    "    return t if len(t) <= max_chars else (t[:max_chars].rstrip() + \"â€¦\")\n",
    "    \n",
    "def _rrf_fuse(\n",
    "    vec_hits: List[Tuple[Dict[str, Any], float]],\n",
    "    bm25_hits: List[Tuple[Dict[str, Any], float]],\n",
    "    k_final: int = 5,\n",
    "    rrf_k: int = 60,\n",
    "    w_vec: float = 1.0,\n",
    "    w_bm: float = 1.0,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fuse two ranked result lists (vector search and BM25) with Weighted Reciprocal Rank Fusion (RRF).\n",
    "\n",
    "    Args:\n",
    "        vec_hits: A list of (metadata, score) pairs from the vector/embedding search.\n",
    "                  `metadata` must contain a stable identifier consumable by `_doc_id(meta)`.\n",
    "        bm25_hits: A list of (metadata, score) pairs from the BM25 keyword search.\n",
    "        k_final: Number of fused items to return.\n",
    "        rrf_k: RRF damping constant. Larger values reduce the influence of rank differences.\n",
    "               Classic literature often uses 60. Must be > 0.\n",
    "        w_vec: Weight for the vector-search contribution to the fused score.\n",
    "        w_bm: Weight for the BM25 contribution to the fused score.\n",
    "\n",
    "    Returns:\n",
    "        A list of metadata dicts (length â‰¤ `k_final`) ranked by fused score (descending).\n",
    "        Each returned dict is a shallow copy of the original metadata and includes:\n",
    "            - \"_score_vec\": the original vector score for that item (0 if absent)\n",
    "            - \"_score_bm25\": the original BM25 score for that item (0 if absent)\n",
    "            - \"_score_fused\": the final fused score (higher = better)\n",
    "\n",
    "    Notes:\n",
    "        - The fusion is rank-based, robust to incomparable raw score scales.\n",
    "        - Items present in only one list are still included (the other rank treated as âˆž).\n",
    "        - This function depends on an external `_doc_id(meta: dict) -> str` helper that\n",
    "          returns a unique/stable doc ID (e.g., meta[\"id\"] or meta[\"uploaded_filename\"]).\n",
    "\n",
    "    Example:\n",
    "        fused = _rrf_fuse(vec_hits, bm25_hits, k_final=5, rrf_k=60, w_vec=1.0, w_bm=0.7)\n",
    "    \"\"\"\n",
    "    # Build rank maps: lower rank number = better (1 is best).\n",
    "    # We sort each hits list by score descending to assign ranks.\n",
    "    vec_ranks = {\n",
    "        _doc_id(m): r\n",
    "        for r, (m, _) in enumerate(sorted(vec_hits, key=lambda x: x[1], reverse=True), 1)\n",
    "    }\n",
    "    bm_ranks = {\n",
    "        _doc_id(m): r\n",
    "        for r, (m, _) in enumerate(sorted(bm25_hits, key=lambda x: x[1], reverse=True), 1)\n",
    "    }\n",
    "\n",
    "    # Remember original scores so we can attach them to outputs later.\n",
    "    vec_scores = { _doc_id(m): s for m, s in vec_hits }\n",
    "    bm_scores  = { _doc_id(m): s for m, s in bm25_hits }\n",
    "\n",
    "    # Union of all doc IDs across both runs.\n",
    "    all_ids = set(vec_ranks.keys()) | set(bm_ranks.keys())\n",
    "\n",
    "    fused: List[Tuple[str, float]] = []\n",
    "    for did in all_ids:\n",
    "        # If a doc is missing in one list, give it an effectively infinite rank.\n",
    "        r_vec = vec_ranks.get(did, 10**9)\n",
    "        r_bm  = bm_ranks.get(did, 10**9)\n",
    "\n",
    "        # Weighted Reciprocal Rank Fusion:\n",
    "        #   fused = w_vec * 1/(rrf_k + rank_vec) + w_bm * 1/(rrf_k + rank_bm)\n",
    "        f = (w_vec * (1.0 / (rrf_k + r_vec))) + (w_bm * (1.0 / (rrf_k + r_bm)))\n",
    "        fused.append((did, f))\n",
    "\n",
    "    # Sort by fused score (descending).\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Build a lookup from ID -> original metadata (first one encountered wins;\n",
    "    # both lists should contain identical metas for the same ID in sane pipelines).\n",
    "    by_id_meta: Dict[str, Dict[str, Any]] = {}\n",
    "    for m, _ in vec_hits + bm25_hits:\n",
    "        did = _doc_id(m)\n",
    "        if did not in by_id_meta:\n",
    "            by_id_meta[did] = m\n",
    "\n",
    "    # Materialize the top-k results with attached score diagnostics.\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for did, f in fused[:k_final]:\n",
    "        m = dict(by_id_meta[did])  # shallow copy to avoid mutating upstream metadata\n",
    "        m[\"_score_vec\"] = float(vec_scores.get(did, 0.0))\n",
    "        m[\"_score_bm25\"] = float(bm_scores.get(did, 0.0))\n",
    "        m[\"_score_fused\"] = float(f)\n",
    "        out.append(m)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _build_context_block(ranked_ctx: List[dict], max_chars_per_ctx=900) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact context with numeric citations [1], [2], ...\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for i, m in enumerate(ranked_ctx, 1):\n",
    "        uf = m.get(\"uploaded_filename\", \"doc\")\n",
    "        preview = _truncate_chars(m.get(\"text\", \"\"), max_chars_per_ctx)\n",
    "        lines.append(\n",
    "            f\"[{i}] file: {uf} | vec={m.get('_score_vec',0):.3f} | bm25={m.get('_score_bm25',0):.3f} | fused={m.get('_score_fused',0):.3f}\\n{preview}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "# --------- Main: RAG + BM25 + Gemini ---------\n",
    "def master_gemini_rag_bm25_answer_w_log(\n",
    "    client,\n",
    "    query: str,\n",
    "    vector_store,         # your FaissVectorStore handle\n",
    "    bm25_index,           # your BM25Index handle (with .search)\n",
    "    logs_df,\n",
    "    # retrieval knobs\n",
    "    k_vec=4, k_bm25=6, k_final=5,\n",
    "    rrf_k=60, w_vec=1.0, w_bm=1.0,\n",
    "    max_chars_per_ctx=4096,\n",
    "    # model knobs (same style as your JD helper)\n",
    "    system_text = \"\"\"\n",
    "    You are an expert HR assistant.\n",
    "    Use only the provided CONTEXT to answer. If not found, say so.\n",
    "    Cite sources with [n] where n matches the context block number.\n",
    "    Respond concisely for recruiters.\n",
    "    \"\"\",\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    max_output_tokens=4096*3,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    verbose = 0\n",
    "):\n",
    "    # 1) Vector + BM25\n",
    "    q_emb, _ = embed_query_with_gemini(client, query, embedding_modelname=\"text-embedding-004\")\n",
    "    vec_hits  = vector_store.search(q_emb, k=k_vec)       # [(meta, sim)]\n",
    "    bm_hits   = bm25_index.search(query, k=k_bm25)        # [(meta, score)]\n",
    "\n",
    "    # 2) Fuse\n",
    "    fused_ctx = _rrf_fuse(vec_hits, bm_hits, k_final=k_final, rrf_k=rrf_k, w_vec=w_vec, w_bm=w_bm)\n",
    "    context_block = _build_context_block(fused_ctx, max_chars_per_ctx=max_chars_per_ctx)\n",
    "\n",
    "    # 3) Build final prompt\n",
    "    user_prompt = f\"\"\"QUESTION:\n",
    "    {query}\n",
    "    \n",
    "    CONTEXT (numbered; cite as [n]):\n",
    "    {context_block}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    - Answer the QUESTION based only on the CONTEXT.\n",
    "    - If multiple candidates match, list top matches with 1-2 line justifications.\n",
    "    - Use [n] citations after each claim referencing a specific context block.\n",
    "    - If the answer is not present, say \"Not found in provided documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # 4) Call Gemini\n",
    "    resp = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 5) Optional: pretty print + logging\n",
    "    if verbose > 0:\n",
    "        try:\n",
    "            print(\"LLM answer:\\n\", resp.text)\n",
    "        except Exception:\n",
    "            print(\"LLM answer:\\n <no text>\")\n",
    "\n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=query,\n",
    "        uploaded_file=None,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df, fused_ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bbc37-8a9b-4fd3-a044-69d074e6f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which candidates have work experience in Singapore?\"\n",
    "resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "    client=client,\n",
    "    query=query,\n",
    "    vector_store=loaded_store,\n",
    "    bm25_index=bm25_index,\n",
    "    logs_df=logs_df,\n",
    "    k_vec=3, k_bm25=3, k_final=2, \n",
    "    w_vec=1.0, w_bm=1.0,\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc84c5-0eb0-4c19-a51c-6e65535a67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which candidates study in Australia?\"\n",
    "resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "    client=client,\n",
    "    query=query,\n",
    "    vector_store=loaded_store,\n",
    "    bm25_index=bm25_index,\n",
    "    logs_df=logs_df,\n",
    "    k_vec=3, k_bm25=3, k_final=2, \n",
    "    w_vec=1.0, w_bm=1.0,\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ccfa6-7b3d-478f-9a9a-55e9c5efd9f9",
   "metadata": {},
   "source": [
    "# Simple chat GUI with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25678e4a-dd99-43c3-a1a0-4b4b87edda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b9fc8-f7b5-4c9b-867f-1fa6ab8779a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_handler(user_msg, history, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens):\n",
    "    global logs_df\n",
    "    history = history or []  # history is a list of {\"role\":..., \"content\":...}\n",
    "\n",
    "    # Call your RAG+BM25 function\n",
    "    resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "        client=client,\n",
    "        query=user_msg,\n",
    "        vector_store=loaded_store,\n",
    "        bm25_index=bm25_index,\n",
    "        logs_df=logs_df,\n",
    "        k_vec=int(k_vec),\n",
    "        k_bm25=int(k_bm25),\n",
    "        k_final=int(k_final),\n",
    "        rrf_k=int(rrf_k),\n",
    "        w_vec=float(w_vec),\n",
    "        w_bm=float(w_bm),\n",
    "        max_output_tokens=int(max_tokens),\n",
    "        temperature=float(temperature),\n",
    "    )\n",
    "\n",
    "    answer = getattr(resp, \"text\", \"\") or \"(no text)\"\n",
    "\n",
    "    # Build sources block\n",
    "    src_lines = []\n",
    "    for i, m in enumerate(used_ctx or [], 1):\n",
    "        src_lines.append(\n",
    "            f\"[{i}] {m.get('uploaded_filename','doc')} | \"\n",
    "            f\"vec={m.get('_score_vec',0):.3f} bm25={m.get('_score_bm25',0):.3f} fused={m.get('_score_fused',0):.3f}\"\n",
    "        )\n",
    "    sources_block = \"**Sources**\\n\" + (\"\\n\".join(src_lines) if src_lines else \"No sources.\")\n",
    "    assistant_msg = answer.strip() + \"\\n\\n\" + sources_block\n",
    "\n",
    "    # ðŸš¨ IMPORTANT: append as dicts with role/content\n",
    "    history.append({\"role\": \"user\",      \"content\": user_msg})\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    return history, \"\\n\".join(src_lines)\n",
    "\n",
    "# ---- Build the simple UI ----\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ðŸ”Ž RAG + BM25 Chat (Notebook)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chat = gr.Chatbot(label=\"Conversation\", type='messages', height=480, show_copy_button=True)\n",
    "            user_box = gr.Textbox(label=\"Your question\", placeholder=\"e.g., Which candidates worked in Singapore?\")\n",
    "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "        with gr.Column(scale=3):\n",
    "            with gr.Accordion(\"Settings\", open=False):\n",
    "                k_vec   = gr.Slider(1, 10, value=4, step=1, label=\"Top-k (Vector)\")\n",
    "                k_bm25  = gr.Slider(1, 10, value=6, step=1, label=\"Top-k (BM25)\")\n",
    "                k_final = gr.Slider(1, 10, value=5, step=1, label=\"Top-k (Fused)\")\n",
    "                rrf_k   = gr.Slider(1, 200, value=60, step=1, label=\"RRF k (damping)\")\n",
    "                w_vec   = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label=\"Weight: Vector\")\n",
    "                w_bm    = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label=\"Weight: BM25\")\n",
    "                temperature = gr.Slider(0.0, 1.5, value=0.1, step=0.05, label=\"Temperature\")\n",
    "                max_tokens  = gr.Slider(1024, 4096*3, value=1024, step=64, label=\"Max output tokens\")\n",
    "            sources = gr.Textbox(label=\"Sources (debug)\", lines=8, show_copy_button=True)\n",
    "\n",
    "    # Wire up Enter key and button\n",
    "    user_box.submit(\n",
    "        fn=chat_handler,\n",
    "        inputs=[user_box, chat, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens],\n",
    "        outputs=[chat, sources],\n",
    "    ).then(fn=lambda: \"\", inputs=None, outputs=user_box)\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=chat_handler,\n",
    "        inputs=[user_box, chat, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens],\n",
    "        outputs=[chat, sources],\n",
    "    ).then(fn=lambda: \"\", inputs=None, outputs=user_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685786d-c09b-48cc-bec2-add586d0a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch(inline=False, share=False, show_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc8179-d88c-48bd-a975-52b4e7da1582",
   "metadata": {},
   "source": [
    "## Example question \n",
    "* Who are a good candidates for developing dashboard, rank and score (0-1) too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f71526-a7bf-4943-8fe0-eb4a3a15fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a99db7-86f9-4131-b234-b150cfd4550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3c69-6d14-4752-a98a-e394775a7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960d017-242b-4c6d-9103-c1b1f7642612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
