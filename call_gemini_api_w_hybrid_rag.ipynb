{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a122010-af18-42a8-91ef-24e404f5e45f",
   "metadata": {},
   "source": [
    "# Gemini API [DO COST $$$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8250050a-406d-4f6e-a6b3-9bcd24fdebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai as genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509ca7cb-e3fa-4304-ae01-bd371c1b0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d90d2-c968-4091-80cc-44197240254b",
   "metadata": {},
   "source": [
    "### APIs Usage Log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e8ea5b-ec72-4a9c-bc8b-7b83845a91c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "LOG_COLUMNS = [\n",
    "    \"timestamp\",\"query\",\"uploaded_file\",\"response_text\",\"finish_reason\",\n",
    "    \"cached_content_token_count\",\"candidates_token_count\",\n",
    "    \"prompt_token_count\",\"thoughts_token_count\",\"total_token_count\",\n",
    "]\n",
    "\n",
    "def ensure_logs_df(logs_df: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    if logs_df is None or logs_df.empty:\n",
    "        # Initialize with correct columns (no rows). Keep dtypes flexible.\n",
    "        logs_df = pd.DataFrame({c: pd.Series(dtype=\"object\") for c in LOG_COLUMNS})\n",
    "    # Guarantee column order/superset\n",
    "    for c in LOG_COLUMNS:\n",
    "        if c not in logs_df.columns:\n",
    "            logs_df[c] = pd.Series(dtype=\"object\")\n",
    "    return logs_df[LOG_COLUMNS]\n",
    "\n",
    "def append_usage_log(logs_df, query_text, uploaded_file, resp=None):\n",
    "    \"\"\"\n",
    "    Append a new usage log entry to an existing DataFrame (no FutureWarning).\n",
    "    \"\"\"\n",
    "    logs_df = ensure_logs_df(logs_df)\n",
    "\n",
    "    # Safe extraction\n",
    "    usage = getattr(resp, \"usage_metadata\", None) if resp else None\n",
    "    try:\n",
    "        finish_reason = resp.candidates[0].finish_reason.name if (resp and getattr(resp, \"candidates\", None)) else None\n",
    "    except Exception:\n",
    "        finish_reason = None\n",
    "    output_text = getattr(resp, \"text\", None) if resp else None\n",
    "\n",
    "    # Build row\n",
    "    new_row = {\n",
    "        \"timestamp\": datetime.now(timezone.utc),  # tz-aware\n",
    "        \"query\": query_text,\n",
    "        \"uploaded_file\": uploaded_file,\n",
    "        \"response_text\": output_text,\n",
    "        \"finish_reason\": finish_reason,\n",
    "        \"cached_content_token_count\": getattr(usage, \"cached_content_token_count\", None) if usage else None,\n",
    "        \"candidates_token_count\": getattr(usage, \"candidates_token_count\", None) if usage else None,\n",
    "        \"prompt_token_count\": getattr(usage, \"prompt_token_count\", None) if usage else None,\n",
    "        \"thoughts_token_count\": getattr(usage, \"thoughts_token_count\", None) if usage else None,\n",
    "        \"total_token_count\": getattr(usage, \"total_token_count\", None) if usage else None,\n",
    "    }\n",
    "\n",
    "    # Append without concat (avoids the deprecation)\n",
    "    logs_df.loc[len(logs_df)] = [new_row.get(c, None) for c in LOG_COLUMNS]\n",
    "    return logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4748cf-0a08-4850-b156-7d801489eb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, query, uploaded_file, response_text, finish_reason, cached_content_token_count, candidates_token_count, prompt_token_count, thoughts_token_count, total_token_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df = pd.DataFrame({\n",
    "    \"timestamp\": pd.Series(dtype=\"datetime64[ns]\"),\n",
    "    \"query\": pd.Series(dtype=\"string\"),\n",
    "    \"uploaded_file\": pd.Series(dtype=\"string\"),\n",
    "    \"response_text\": pd.Series(dtype=\"string\"),\n",
    "    \"finish_reason\": pd.Series(dtype=\"string\"),\n",
    "    \"cached_content_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"candidates_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"prompt_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"thoughts_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "    \"total_token_count\": pd.Series(dtype=\"Int64\"),\n",
    "})\n",
    "logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c447d1e-69b5-4917-bb8c-06f1598de977",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyDHOSjzr-AedFPftuIK7iiZ0yTqaTkSDYQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d15e6b-f236-42ef-bdde-3ff92c3df7b6",
   "metadata": {},
   "source": [
    "### Fail to get response example due to 'MAX_TOKENS'>\n",
    "GenerateContentResponse(\n",
    "  automatic_function_calling_history=[],\n",
    "  candidates=[\n",
    "    Candidate(\n",
    "      content=Content(\n",
    "        role='model'\n",
    "      ),\n",
    "      finish_reason=<FinishReason.MAX_TOKENS: 'MAX_TOKENS'>,\n",
    "      index=0\n",
    "    ),\n",
    "  ],\n",
    "  model_version='gemini-2.5-flash',\n",
    "  response_id='tPemaLqfEaOtmtkPiLTW4QI',\n",
    "  sdk_http_response=HttpResponse(\n",
    "    headers=<dict len=11>\n",
    "  ),\n",
    "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
    "    prompt_token_count=8,\n",
    "    prompt_tokens_details=[\n",
    "      ModalityTokenCount(\n",
    "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
    "        token_count=8\n",
    "      ),\n",
    "    ],\n",
    "    thoughts_token_count=199,\n",
    "    total_token_count=207\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4fbe35-bc18-4d55-b451-61e7ebfe3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 key bullets about Analog-\n"
     ]
    }
   ],
   "source": [
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": \"Give 3 bullets about ADC.\"}]}],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.6,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Robust extractor\n",
    "def extract_text(r):\n",
    "    out = []\n",
    "    for c in getattr(r, \"candidates\", []) or []:\n",
    "        # finish_reason can be useful: types.FinishReason.STOP, SAFETY, etc.\n",
    "        # print(\"finish_reason:\", c.finish_reason)\n",
    "        content = getattr(c, \"content\", None)\n",
    "        if content:\n",
    "            for p in getattr(content, \"parts\", []) or []:\n",
    "                t = getattr(p, \"text\", None)\n",
    "                if t:\n",
    "                    out.append(t)\n",
    "    # Fallback to r.text if present\n",
    "    return \"\\n\".join(out) or getattr(r, \"text\", \"\") or \"\"\n",
    "\n",
    "print(extract_text(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b7b04-5d66-4954-b65a-ffed89e9a7b9",
   "metadata": {},
   "source": [
    "### PDF upload with text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bee6c1-dad8-441d-88ac-313bea78292c",
   "metadata": {},
   "source": [
    "#### Step1: CV Extraction and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b178347-6027-4071-9110-d153be1f9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the datetime object into a string\n",
    "formatted_today = datetime.utcnow().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a98f352-b2e9-4323-bd1a-4b203db2a8cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def master_gemini_upload_cv_prompt_w_log(\n",
    "    uploaded_filename, prompt_text, logs_df,\n",
    "    system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\",\n",
    "    max_output_tokens = 4096,\n",
    "    temperature       = 0.1,\n",
    "    top_p             = 0.9, # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "    top_k             = 40,  # At each step, the model only considers the top_k most likely tokens.\n",
    "):\n",
    "    # Step 1: Upload the PDF\n",
    "    uploaded_file = client.files.upload(\n",
    "        file=uploaded_filename, \n",
    "        config={\"display_name\": \"CV\"}\n",
    "    )\n",
    "\n",
    "    # Step 2: Pass the file reference into the request\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt_text},\n",
    "                    {\"file_data\": {\"file_uri\": uploaded_file.uri}}\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            top_k = top_k,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"resp.text:\\n {resp.text}\")\n",
    "        size_info = calc_resp_text_size(resp)\n",
    "        print(f\"[resp.text size] chars={size_info['char_length']}, tokens‚âà{size_info['token_est']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"resp.text: Not Found ({e})\")\n",
    "        size_info = {\"char_length\": 0, \"token_est\": 0}\n",
    "        \n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=prompt_text,\n",
    "        uploaded_file=uploaded_filename,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df\n",
    "\n",
    "def calc_resp_text_size(resp) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the size of resp.text in characters and estimated tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    resp : object\n",
    "        Gemini response object (with .text)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : {\"char_length\": int, \"token_est\": int}\n",
    "    \"\"\"\n",
    "    if not hasattr(resp, \"text\") or resp.text is None:\n",
    "        return {\"char_length\": 0, \"token_est\": 0}\n",
    "    \n",
    "    txt = resp.text\n",
    "    char_length = len(txt)\n",
    "    token_est   = len(txt.split())  # rough estimate\n",
    "    \n",
    "    return {\"char_length\": char_length, \"token_est\": token_est}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2a462b-c41a-4aaf-a039-e529727a474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploaded_filename = \"ExampleCV/NLP-CV-NachaiLim.pdf\"\n",
    "# uploaded_filename = \"ExampleCV/CV-Oranid.pdf\"\n",
    "uploaded_filename = \"ExampleCV/Natthaporn_CV2022.pdf\"\n",
    "system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\"\n",
    "# prompt_text       = \"Who is the individual in the CV? Summarize the uploaded CV's education and work experience in 5-10 concise bullet points with experience years. Then list his/her skills in python list.\"\n",
    "prompt_text       = \"\"\"\n",
    "Analyze the uploaded CV and provide the following:\n",
    "\n",
    "1. Identify the individual (full name).\n",
    "2. Summarize education and work experience in **10‚Äì15 concise bullet points**, including years of experience for each role.\n",
    "3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "\"\"\"\n",
    "\n",
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "122cd0d6-b315-4a03-9561-d56575dadbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp.text:\n",
      " Here's a summarization of the provided CV:\n",
      "\n",
      "### 1. Individual Identification\n",
      "**Full Name:** Natthaporn Takpho\n",
      "\n",
      "### 2. Education and Work Experience Summary (11 bullet points)\n",
      "\n",
      "*   **Assistant Manager, Research Division, Mitsui Chemicals Singapore R&D Centre (May 2021 ‚Äì Present; 4 years 4 months):** Leads and directs R&D projects in life science and healthcare, focusing on business development, market analysis, and patent management.\n",
      "*   **Researcher, Mitsui Chemicals Singapore R&D Centre (Nov 2017 ‚Äì Apr 2021; 3 years 6 months):** Planned and supervised R&D projects, developed business ideas, validated technology, and prepared patent applications.\n",
      "*   **Molecular Microbiologist, Mahidol-Oxford Tropical Medicine Research Unit (Oct 2013 ‚Äì Sep 2014; 1 year):** Performed molecular genetic analysis for epidemiological studies and worked in a BSL-3 laboratory.\n",
      "*   **PhD in Biological Science, Nara Institute of Science and Technology (completed Sep 2017):** Focused on biological science, supported by a Japanese government (MEXT) scholarship.\n",
      "*   **Internship at Gekkeikan Co, Ltd (Japan) during PhD:** Gained experience in product development and basic science research.\n",
      "*   **Internship at University of California Davis (USA) during PhD:** Researched regulation of NAD+ homeostasis and cellular life span in yeast.\n",
      "*   **MSc in Biotechnology, Mahidol University (completed Jul 2013):** Awarded multiple research scholarships and recognized for academic research.\n",
      "*   **Training at Osaka University (Japan) during MSc:** Focused on molecular mechanisms of thermotolerant yeast.\n",
      "*   **Training at International Center for Biotechnology, Osaka University (Japan) during MSc:** Conducted DNA microarray study of thermotolerant yeast.\n",
      "*   **BSc in Biotechnology, King Mongkut's Institute of Technology Lad Krabang (completed Mar 2010):** Senior project focused on bioactivities of morning glory extracts.\n",
      "*   **Internship at Chulalongkorn University (Thailand) during BSc:** Gained experience in TB diagnosis and immunohistochemistry.\n",
      "\n",
      "### 3. Total Experience by Position Title\n",
      "\n",
      "*   **Assistant Manager:** 4 years 4 months\n",
      "*   **Researcher:** 3 years 6 months\n",
      "*   **Molecular Microbiologist:** 1 year\n",
      "\n",
      "### 4. Skills (Python List)\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Molecular biology\",\n",
      "    \"DNA sequencing\",\n",
      "    \"NGS\",\n",
      "    \"Real-time PCR\",\n",
      "    \"Vector construction\",\n",
      "    \"Gene & protein expression\",\n",
      "    \"Molecular genome typing\",\n",
      "    \"Biochemistry & Analysis\",\n",
      "    \"Western blot\",\n",
      "    \"Northern blot\",\n",
      "    \"Enzymatic assay\",\n",
      "    \"HPLC\",\n",
      "    \"Biotechnology\",\n",
      "    \"Microbial strain engineering\",\n",
      "    \"Lab-scale fermentation\",\n",
      "    \"Substrate evaluation\",\n",
      "    \"Microbiology\",\n",
      "    \"Bacteria\",\n",
      "    \"Yeast\"\n",
      "]\n",
      "```\n",
      "[resp.text size] chars=2693, tokens‚âà345\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 09:33:29.233297+00:00</td>\n",
       "      <td>\\nAnalyze the uploaded CV and provide the foll...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the provided CV:\\n\\n...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>670</td>\n",
       "      <td>1172</td>\n",
       "      <td>1888</td>\n",
       "      <td>3730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp  \\\n",
       "0 2025-09-01 09:33:29.233297+00:00   \n",
       "\n",
       "                                               query  \\\n",
       "0  \\nAnalyze the uploaded CV and provide the foll...   \n",
       "\n",
       "                     uploaded_file  \\\n",
       "0  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "\n",
       "                                       response_text finish_reason  \\\n",
       "0  Here's a summarization of the provided CV:\\n\\n...          STOP   \n",
       "\n",
       "  cached_content_token_count  candidates_token_count  prompt_token_count  \\\n",
       "0                       None                     670                1172   \n",
       "\n",
       "   thoughts_token_count  total_token_count  \n",
       "0                  1888               3730  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_cv, logs_df = master_gemini_upload_cv_prompt_w_log(\n",
    "    uploaded_filename, prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a380769-0806-49f6-ad39-c0f3ef82923a",
   "metadata": {},
   "source": [
    "#### Step2: JD Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82b5b0c-734c-4291-a294-ac39fbc5e704",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def master_gemini_jd_prompt_w_log(\n",
    "    prompt_text, logs_df,\n",
    "    system_text       = f\"\"\"\n",
    "    You are an expert HR assistant, \n",
    "    make a summarization of the job description in a list of Key responsibilities, Mandaotry experiences & skills, Prefer experiences & skills.\n",
    "    \"\"\",\n",
    "    max_output_tokens = 4096,\n",
    "    temperature       = 0.1,\n",
    "    top_p             = 0.9, # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "    top_k             = 40,  # At each step, the model only considers the top_k most likely tokens.\n",
    "):\n",
    "\n",
    "    # Step 1: Pass the file reference into the request\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt_text},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature = temperature,\n",
    "            top_p = top_p,\n",
    "            top_k = top_k,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"resp.text:\\n {format_job_description(resp.text)}\")\n",
    "    except:\n",
    "        print(f\"resp.text:\\n Not Found\")\n",
    "        \n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=prompt_text,\n",
    "        uploaded_file=uploaded_filename,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df\n",
    "\n",
    "def format_job_description(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and format a job description string into a more readable format.\n",
    "    Supports Markdown-style output for readability.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from textwrap import dedent\n",
    "\n",
    "    # Step 1: Replace \\n escape sequences with real line breaks\n",
    "    formatted = raw_text.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Step 2: Remove leading/trailing quotes or boilerplate\n",
    "    formatted = re.sub(r'^\"|\"$', '', formatted.strip())\n",
    "\n",
    "    # Step 3: Normalize multiple blank lines\n",
    "    formatted = re.sub(r'\\n{3,}', '\\n\\n', formatted)\n",
    "\n",
    "    # Step 4: Dedent to align properly\n",
    "    formatted = dedent(formatted)\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67911427-9d25-4f8b-8b05-47f2ea8f8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = f\"\"\"\n",
    "Senior Data Scientist\n",
    "\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ‡∏õ‡∏µ ‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô Key responsibilities: ‚Ä¢ Interpret data, and analyze results using statistical methods ‚Ä¢ Prepare and deliver business reports that effectively communicate trends, patterns, risks, and insights using data ‚Ä¢ Research, design, and develop machine-learning/artificial-intelligence/computer-vision systems to address key business challenges ‚Ä¢ Perform data quality testing, validation, and assurance as a part of designing, and implementing scalable data solutions ‚Ä¢ Support identification, triage, and remediation of data quality issues across the data and technology organizations ‚Ä¢ Effectively manage and develop small data analytics teams ‚Ä¢ Effectively engage and partner with team members across data analytics, technology development, and business strategy to drive outcomes and impact ‚Ä¢ Ensure end user requirements of data solutions are effectively met ‚Ä¢ Develop user friendly documentation to communicate the use and value of data solutions ‚Ä¢ Identify and push forward process improvement opportunities and solutions ‚Ä¢ Review and keep up to date with developments in the data analytics and ML fields\n",
    "\n",
    "Education: ‚Ä¢ B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science\n",
    "\n",
    "Relevant experience required: ‚Ä¢ 5+ years hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health / monitoring frameworks ‚Ä¢ Proficiency with one or more programming languages or data engineering frameworks, such as Python, SQL, Spark, Java, C++, TypeScript/JavaScript, or similar ‚Ä¢ Proficiency with any of the major machine learning and computer vision frameworks preferred ‚Ä¢ Hands-on experience on BI solutions (e.g. Tableau, Power BI, Qlik, Looker) ‚Ä¢ Ability to work well with cross-disciplinary teams ‚Ä¢ Strong numerical and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy ‚Ä¢ Communicate in English fluently\n",
    "\n",
    "üòÜ Our Benefits: ¬∑ Free lunch every Tuesday and Friday ¬∑ WFH/WFA (Work From Home/Work From Anywhere) ¬∑ Group insurance ¬∑ Retirement savings fund ¬∑ Well-stocked Snack Bar with snacks and beverages ¬∑ Dress in your own style at work ¬∑ Annual vacation 15 days\n",
    "\"\"\"\n",
    "\n",
    "system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "make a summarization of the job description in a list of Key responsibilities, Mandaotry experiences & skills, Prefer experiences & skills.\n",
    "\"\"\"\n",
    "max_output_tokens = 4096\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9cb5eae-e2b5-48e3-9e04-536f799927a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp.text:\n",
      " Here's a summarization of the Senior Data Scientist job description:\n",
      "\n",
      "---\n",
      "\n",
      "**Senior Data Scientist**\n",
      "\n",
      "**Key Responsibilities:**\n",
      "\n",
      "*   Interpret data, analyze results using statistical methods, and prepare business reports to communicate trends, patterns, risks, and insights.\n",
      "*   Research, design, and develop machine learning, artificial intelligence, and computer vision systems to address key business challenges.\n",
      "*   Perform data quality testing, validation, and assurance as part of designing and implementing scalable data solutions.\n",
      "*   Support the identification, triage, and remediation of data quality issues across data and technology organizations.\n",
      "*   Effectively manage and develop small data analytics teams.\n",
      "*   Engage and partner with cross-functional teams (data analytics, technology development, business strategy) to drive outcomes.\n",
      "*   Ensure end-user requirements for data solutions are met and develop user-friendly documentation.\n",
      "*   Identify and implement process improvement opportunities and solutions.\n",
      "*   Stay updated with developments in data analytics and ML fields.\n",
      "\n",
      "**Mandatory Experiences & Skills:**\n",
      "\n",
      "*   **Education:** B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science.\n",
      "*   **Experience:** 5+ years of hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health/monitoring frameworks.\n",
      "*   **Programming:** Proficiency with one or more programming languages or data engineering frameworks (e.g., Python, SQL, Spark, Java, C++, TypeScript/JavaScript).\n",
      "*   **BI Solutions:** Hands-on experience with Business Intelligence tools (e.g., Tableau, Power BI, Qlik, Looker).\n",
      "*   **Analytical Skills:** Strong numerical and analytical abilities with attention to detail and accuracy in collecting, organizing, analyzing, and disseminating information.\n",
      "*   **Communication:** Fluent in English.\n",
      "*   **Collaboration:** Ability to work effectively with cross-disciplinary teams.\n",
      "\n",
      "**Preferred Experiences & Skills:**\n",
      "\n",
      "*   Proficiency with any of the major machine learning and computer vision frameworks.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 09:33:29.233297+00:00</td>\n",
       "      <td>\\nAnalyze the uploaded CV and provide the foll...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the provided CV:\\n\\n...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>670</td>\n",
       "      <td>1172</td>\n",
       "      <td>1888</td>\n",
       "      <td>3730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 09:33:38.423997+00:00</td>\n",
       "      <td>\\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the Senior Data Scie...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>437</td>\n",
       "      <td>489</td>\n",
       "      <td>1410</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp  \\\n",
       "0 2025-09-01 09:33:29.233297+00:00   \n",
       "1 2025-09-01 09:33:38.423997+00:00   \n",
       "\n",
       "                                               query  \\\n",
       "0  \\nAnalyze the uploaded CV and provide the foll...   \n",
       "1  \\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...   \n",
       "\n",
       "                     uploaded_file  \\\n",
       "0  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "1  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "\n",
       "                                       response_text finish_reason  \\\n",
       "0  Here's a summarization of the provided CV:\\n\\n...          STOP   \n",
       "1  Here's a summarization of the Senior Data Scie...          STOP   \n",
       "\n",
       "  cached_content_token_count  candidates_token_count  prompt_token_count  \\\n",
       "0                       None                     670                1172   \n",
       "1                       None                     437                 489   \n",
       "\n",
       "   thoughts_token_count  total_token_count  \n",
       "0                  1888               3730  \n",
       "1                  1410               2336  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_jd, logs_df = master_gemini_jd_prompt_w_log(    \n",
    "    prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1e24e-e46c-4997-ba64-2f0b5e396a00",
   "metadata": {},
   "source": [
    "#### Step3: JD vs CV scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5523570-ea8d-4167-9c61-56c9579366d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a summarization of the Senior Data Scientist job description:\\n\\n---\\n\\n**Senior Data Scientist**\\n\\n**Key Responsibilities:**\\n\\n*   Interpret data, analyze results using statistical methods, and prepare business reports to communicate trends, patterns, risks, and insights.\\n*   Research, design, and develop machine learning, artificial intelligence, and computer vision systems to address key business challenges.\\n*   Perform data quality testing, validation, and assurance as part of designing and\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_jd.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c093a28-63e6-4cff-b0e7-d1509a5a688c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a summarization of the provided CV:\\n\\n### 1. Individual Identification\\n**Full Name:** Natthaporn Takpho\\n\\n### 2. Education and Work Experience Summary (11 bullet points)\\n\\n*   **Assistant Manager, Research Division, Mitsui Chemicals Singapore R&D Centre (May 2021 ‚Äì Present; 4 years 4 months):** Leads and directs R&D projects in life science and healthcare, focusing on business development, market analysis, and patent management.\\n*   **Researcher, Mitsui Chemicals Singapore R&D Centre (Nov 20\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_cv.text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00fa82b9-a129-42e1-aa02-0ac2068d194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_template = \"\"\"\n",
    "### JD Mandatory Requirements [‚úì: Pass | ‚úó: Missing]\n",
    "1. Requirement 1 ‚Äî [‚úì/‚úó]\n",
    "2. Requirement 2 ‚Äî [‚úì/‚úó]\n",
    "\n",
    "### JD Preferred Requirements [‚úì: Pass | ‚úó: Missing]\n",
    "1. Requirement 1 ‚Äî [‚úì/‚úó]\n",
    "2. Requirement 2 ‚Äî [‚úì/‚úó]\n",
    "\n",
    "### Candidate Strengths (1‚Äì5 bullets)\n",
    "1. Strength 1\n",
    "2. Strength 2\n",
    "\n",
    "### Candidate Weaknesses (1‚Äì5 bullets)\n",
    "1. Weakness 1\n",
    "2. Weakness 2\n",
    "\n",
    "### JD vs CV Matching Score  \n",
    "**Score:** X.X / 10.0  \n",
    "\n",
    "**Reasoning (3‚Äì5 bullets):**  \n",
    "1. Reason 1  \n",
    "2. Reason 2  \n",
    "3. Reason 3  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1dbbad-e072-4382-9db4-8c40db9b4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_text = f\"\"\"\n",
    "You are an expert HR assistant, \n",
    "Filling in the output_template below based on the given job description (JD) and summarized resume (CV).\n",
    "Only answer in the given output_template.\n",
    "\n",
    "# output_template:\n",
    "{output_template}\n",
    "\"\"\"\n",
    "\n",
    "prompt_text = f\"\"\"\n",
    "# job description (JD):\n",
    "{format_job_description(resp_jd.text)}\n",
    "\n",
    "# summarized resume (CV):\n",
    "{resp_cv.text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233e4fcf-100e-4dd4-a813-e0f2f5976192",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a04d81b-15bf-43db-bf0c-ce4590f08ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp.text:\n",
      " ### JD Mandatory Requirements [‚úì: Pass | ‚úó: Missing]\n",
      "1.  **Education:** B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science. ‚Äî ‚úó\n",
      "2.  **Experience:** 5+ years of hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health/monitoring frameworks. ‚Äî ‚úó\n",
      "3.  **Programming:** Proficiency with one or more programming languages or data engineering frameworks (e.g., Python, SQL, Spark, Java, C++, TypeScript/JavaScript). ‚Äî ‚úó\n",
      "4.  **BI Solutions:** Hands-on experience with Business Intelligence tools (e.g., Tableau, Power BI, Qlik, Looker). ‚Äî ‚úó\n",
      "5.  **Analytical Skills:** Strong numerical and analytical abilities with attention to detail and accuracy in collecting, organizing, analyzing, and disseminating information. ‚Äî ‚úì\n",
      "6.  **Communication:** Fluent in English. ‚Äî ‚úì\n",
      "7.  **Collaboration:** Ability to work effectively with cross-disciplinary teams. ‚Äî ‚úì\n",
      "\n",
      "### JD Preferred Requirements [‚úì: Pass | ‚úó: Missing]\n",
      "1.  Proficiency with any of the major machine learning and computer vision frameworks. ‚Äî ‚úó\n",
      "\n",
      "### Candidate Strengths (1‚Äì5 bullets)\n",
      "1.  Strong academic background with multiple degrees (BSc, MSc, PhD) from reputable institutions.\n",
      "2.  Extensive research and R&D experience (over 8 years), including leadership as an Assistant Manager.\n",
      "3.  Demonstrated analytical and problem-solving skills through scientific research and project management.\n",
      "4.  Experience in managing R&D projects, business development, and market analysis within a scientific context.\n",
      "5.  International experience through studies and internships in Japan and the USA.\n",
      "\n",
      "### Candidate Weaknesses (1‚Äì5 bullets)\n",
      "1.  Lack of direct experience in core data science areas such as data mining, distributed computing, data pipelining, or data health frameworks.\n",
      "2.  No explicit mention of proficiency in required programming languages (e.g., Python, SQL, Spark) or Business Intelligence tools (e.g., Tableau, Power BI).\n",
      "3.  Educational background is in biological sciences/biotechnology, which does not directly align with the specified fields for a Senior Data Scientist role.\n",
      "4.  No experience with machine learning or computer vision frameworks, which is a preferred skill for the position.\n",
      "5.  The candidate's current skill set is highly specialized in molecular biology and lab techniques, which are not relevant to the technical demands of a Senior Data Scientist.\n",
      "\n",
      "### JD vs CV Matching Score\n",
      "**Score:** 2.5 / 10.0\n",
      "\n",
      "**Reasoning (3‚Äì5 bullets):**\n",
      "1.  The candidate's educational background and extensive professional experience are primarily in biological sciences and R&D, which do not align with the core technical requirements for a Senior Data Scientist.\n",
      "2.  Critical mandatory technical skills, including proficiency in programming languages (Python, SQL), data engineering frameworks, and Business Intelligence tools, are entirely absent from the CV.\n",
      "3.  While the candidate meets the general experience duration, the specific hands-on experience in data mining, statistical analysis (in a data science context), distributed computing, and data pipelining is missing.\n",
      "4.  The CV does not demonstrate any experience with machine learning or computer vision frameworks, which is a preferred skill for the role.\n",
      "5.  The candidate's strengths lie in scientific research and project management within a different domain, making the technical skill gap significant for this data scientist position.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 09:33:29.233297+00:00</td>\n",
       "      <td>\\nAnalyze the uploaded CV and provide the foll...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the provided CV:\\n\\n...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>670</td>\n",
       "      <td>1172</td>\n",
       "      <td>1888</td>\n",
       "      <td>3730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 09:33:38.423997+00:00</td>\n",
       "      <td>\\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the Senior Data Scie...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>437</td>\n",
       "      <td>489</td>\n",
       "      <td>1410</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01 09:33:51.643159+00:00</td>\n",
       "      <td>\\n# job description (JD):\\nHere's a summarizat...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>### JD Mandatory Requirements [‚úì: Pass | ‚úó: Mi...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>766</td>\n",
       "      <td>1356</td>\n",
       "      <td>1881</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp  \\\n",
       "0 2025-09-01 09:33:29.233297+00:00   \n",
       "1 2025-09-01 09:33:38.423997+00:00   \n",
       "2 2025-09-01 09:33:51.643159+00:00   \n",
       "\n",
       "                                               query  \\\n",
       "0  \\nAnalyze the uploaded CV and provide the foll...   \n",
       "1  \\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...   \n",
       "2  \\n# job description (JD):\\nHere's a summarizat...   \n",
       "\n",
       "                     uploaded_file  \\\n",
       "0  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "1  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "2  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "\n",
       "                                       response_text finish_reason  \\\n",
       "0  Here's a summarization of the provided CV:\\n\\n...          STOP   \n",
       "1  Here's a summarization of the Senior Data Scie...          STOP   \n",
       "2  ### JD Mandatory Requirements [‚úì: Pass | ‚úó: Mi...          STOP   \n",
       "\n",
       "  cached_content_token_count  candidates_token_count  prompt_token_count  \\\n",
       "0                       None                     670                1172   \n",
       "1                       None                     437                 489   \n",
       "2                       None                     766                1356   \n",
       "\n",
       "   thoughts_token_count  total_token_count  \n",
       "0                  1888               3730  \n",
       "1                  1410               2336  \n",
       "2                  1881               4003  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_summary, logs_df = master_gemini_jd_prompt_w_log(    \n",
    "    prompt_text, logs_df,\n",
    "    system_text       = system_text,\n",
    "    max_output_tokens = max_output_tokens,\n",
    "    temperature       = temperature,\n",
    "    top_p             = top_p, \n",
    "    top_k             = top_k, \n",
    ")\n",
    "logs_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35925d7-d2e4-42cf-93b9-8455365d04e2",
   "metadata": {},
   "source": [
    "## Terra job description example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b441f-f176-429a-a826-ef7fc5adccfa",
   "metadata": {},
   "source": [
    "source: https://www.jobfinfin.com/job/659cf72b64805d54e65dc789\n",
    "\n",
    "Senior Data Scientist\n",
    "\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô\n",
    "5-10 ‡∏õ‡∏µ\n",
    "‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\n",
    "Key responsibilities:\n",
    "‚Ä¢ Interpret data, and analyze results using statistical methods\n",
    "‚Ä¢ Prepare and deliver business reports that effectively communicate trends, patterns, risks, and insights using data\n",
    "‚Ä¢ Research, design, and develop machine-learning/artificial-intelligence/computer-vision systems to address key business challenges \n",
    "‚Ä¢ Perform data quality testing, validation, and assurance as a part of designing, and implementing scalable data solutions\n",
    "‚Ä¢ Support identification, triage, and remediation of data quality issues across the data and technology organizations\n",
    "‚Ä¢ Effectively manage and develop small data analytics teams\n",
    "‚Ä¢ Effectively engage and partner with team members across data analytics, technology development, and business strategy to drive outcomes and impact\n",
    "‚Ä¢ Ensure end user requirements of data solutions are effectively met\n",
    "‚Ä¢ Develop user friendly documentation to communicate the use and value of data solutions\n",
    "‚Ä¢ Identify and push forward process improvement opportunities and solutions\n",
    "‚Ä¢ Review and keep up to date with developments in the data analytics and ML fields\n",
    " \n",
    "Education:\n",
    "‚Ä¢ B.S., M.S., or Ph.D. degree in Computer Science, Mathematics, Software Engineering, Physics, and/or Data Science\n",
    " \n",
    "Relevant experience required:\n",
    "‚Ä¢ 5+ years hands-on experience with data mining, statistical analysis, distributed computing, data pipelining tools, and data health / monitoring frameworks\n",
    "‚Ä¢ Proficiency with one or more programming languages or data engineering frameworks, such as Python, SQL, Spark, Java, C++, TypeScript/JavaScript, or similar\n",
    "‚Ä¢ Proficiency with any of the major machine learning and computer vision frameworks preferred\n",
    "‚Ä¢ Hands-on experience on BI solutions (e.g. Tableau, Power BI, Qlik, Looker)\n",
    "‚Ä¢ Ability to work well with cross-disciplinary teams\n",
    "‚Ä¢ Strong numerical and analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy\n",
    "‚Ä¢ Communicate in English fluently\n",
    "\n",
    "üòÜ Our Benefits:\n",
    "¬∑ Free lunch every Tuesday and Friday\n",
    "¬∑ WFH/WFA (Work From Home/Work From Anywhere)\n",
    "¬∑ Group insurance\n",
    "¬∑ Retirement savings fund\n",
    "¬∑ Well-stocked Snack Bar with snacks and beverages\n",
    "¬∑ Dress in your own style at work\n",
    "¬∑ Annual vacation 15 days "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c76734-3408-4d43-9d3e-83c4ea8bc6cd",
   "metadata": {},
   "source": [
    "# Text Embedding for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbe52ad-9f41-4256-8977-246ac1db178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from google import genai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ffef4c6-0271-406b-a7aa-feea7cd9b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Cloud adoption helps startups scale infrastructure with low upfront costs.\",\n",
    "    \"Hybrid cloud combines private and public resources for flexibility.\",\n",
    "    \"On-premise solutions give companies more control but higher costs.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cf82ca7-92d5-4951-8c22-516f6fbe07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embeddings = [\n",
    "    client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": doc}]}]\n",
    "    ).embeddings[0].values\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "# Convert to NumPy array for FAISS\n",
    "emb_matrix = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index (cosine similarity ‚Üí use inner product + normalized vectors)\n",
    "faiss.normalize_L2(emb_matrix)\n",
    "index = faiss.IndexFlatIP(emb_matrix.shape[1])\n",
    "index.add(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdefc1fa-fdbe-4558-a9ed-8a55dcb1803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved docs: ['Cloud adoption helps startups scale infrastructure with low upfront costs.', 'Hybrid cloud combines private and public resources for flexibility.']\n"
     ]
    }
   ],
   "source": [
    "# Search for nearest docs\n",
    "query = \"What are the benefits of cloud computing?\"\n",
    "query_emb = client.models.embed_content(\n",
    "    model=\"text-embedding-004\",\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": query}]}]\n",
    ").embeddings[0].values\n",
    "\n",
    "query_emb = np.array([query_emb]).astype(\"float32\")\n",
    "faiss.normalize_L2(query_emb)  # only if using cosine/IP index\n",
    "\n",
    "D, I = index.search(query_emb, k=2)  # distances, indices\n",
    "retrieved_docs = [docs[i] for i in I[0]]\n",
    "print(\"Retrieved docs:\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f28690e-d8a2-40cc-bd66-4bab7adf276a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_token_count(text: str) -> int:\n",
    "    \"\"\"Very rough token estimate (whitespace split).\n",
    "    Replace with your tokenizer if needed.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "class FaissVectorStore:\n",
    "    \"\"\"\n",
    "    Minimal FAISS + metadata store with cosine similarity search.\n",
    "    - Stores vectors in FAISS (IndexFlatIP) with L2-normalized vectors (so IP == cosine sim)\n",
    "    - Stores metadata separately (JSONL or pickle)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatIP(dim)  # inner product (use with normalized vectors)\n",
    "        self._vectors = []                  # keep in RAM until saved (optional)\n",
    "        self._metadata: List[Dict] = []     # [{\"text\":..., \"timestamp\":..., \"token_length\":...}, ...]\n",
    "\n",
    "    # ---------- Embedding helpers (plug your embedding client here) ----------\n",
    "    @staticmethod\n",
    "    def _to_float32(arr) -> np.ndarray:\n",
    "        return np.array(arr, dtype=\"float32\")\n",
    "\n",
    "    def add_texts_and_metadata(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        embeddings: List[List[float]],\n",
    "        metadata_list: Optional[List[Dict]] = None,\n",
    "        default_metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add texts with precomputed embeddings + flexible per-item metadata.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        texts : List[str]\n",
    "            Plaintext chunks to index.\n",
    "        embeddings : List[List[float]]\n",
    "            Precomputed embeddings matching each text (dim == self.dim).\n",
    "            These will be L2-normalized for cosine similarity with IndexFlatIP.\n",
    "        metadata_list : Optional[List[Dict]]\n",
    "            A list of metadata dicts (one per text). Each dict can contain any keys\n",
    "            (JSONL-friendly). Missing keys are allowed and will be auto-filled where applicable.\n",
    "            If None, minimal metadata will be generated.\n",
    "        default_metadata : Optional[Dict]\n",
    "            Default metadata merged into each item *before* item-specific overrides.\n",
    "            Example: {\"source\": \"my_corpus\", \"tag\": \"v1\"}.\n",
    "    \n",
    "        Behavior\n",
    "        --------\n",
    "        - For each item i, final metadata = {**default_metadata, **metadata_list[i]} (if provided)\n",
    "        - Auto-fill:\n",
    "            * \"timestamp\" (UTC ISO 8601) if not present\n",
    "            * \"token_length\" via simple_token_count(text) if not present\n",
    "        - All keys are preserved as-is to support JSONL dumps without schema constraints.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If lengths of texts, embeddings, and metadata_list (when provided) mismatch.\n",
    "        \"\"\"\n",
    "        n = len(texts)\n",
    "        if len(embeddings) != n:\n",
    "            raise ValueError(\"embeddings length must match texts length\")\n",
    "    \n",
    "        if metadata_list is not None and len(metadata_list) != n:\n",
    "            raise ValueError(\"metadata_list length must match texts length when provided\")\n",
    "    \n",
    "        # Normalize and add vectors\n",
    "        vecs = np.asarray(embeddings, dtype=\"float32\")\n",
    "        faiss.normalize_L2(vecs)\n",
    "        self.index.add(vecs)\n",
    "        self._vectors.extend(vecs)\n",
    "    \n",
    "        # Prepare defaults\n",
    "        default_metadata = default_metadata or {}\n",
    "    \n",
    "        # Build metadata rows JSONL-friendly (arbitrary keys preserved)\n",
    "        now_iso = datetime.utcnow().isoformat()\n",
    "        for i, t in enumerate(texts):\n",
    "            item_meta = metadata_list[i] if metadata_list is not None else {}\n",
    "            # Merge: defaults first, then item-specific override\n",
    "            meta = {**default_metadata, **item_meta}\n",
    "    \n",
    "            # Auto-fill timestamp if missing\n",
    "            meta.setdefault(\"timestamp\", now_iso)\n",
    "            # Auto-fill token_length if missing (won't overwrite if provided)\n",
    "            meta.setdefault(\"token_length\", simple_token_count(t))\n",
    "            # Always store the raw text (if you want to make it optional, remove this line)\n",
    "            meta.setdefault(\"text\", t)\n",
    "    \n",
    "            self._metadata.append(meta)\n",
    "            \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clear all vectors and metadata from the store.\n",
    "        This reinitializes the FAISS index and empties metadata.\n",
    "        \"\"\"\n",
    "        # Recreate a fresh FAISS index with the same dimension\n",
    "        self.index = faiss.IndexFlatIP(self.dim)  # cosine sim (normalized IP)\n",
    "        \n",
    "        # Reset vectors and metadata\n",
    "        self._vectors = []\n",
    "        self._metadata = []         \n",
    "\n",
    "    def search(self, query_embedding: List[float], k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Search the index by embedding. Returns top-k as [(metadata, similarity), ...].\n",
    "        Similarity is cosine similarity (since we normalized and use IP index).\n",
    "        \"\"\"\n",
    "        q = self._to_float32([query_embedding])\n",
    "        faiss.normalize_L2(q)  # normalize query for cosine/IP\n",
    "        D, I = self.index.search(q, k)\n",
    "        hits = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1:  # no result\n",
    "                continue\n",
    "            hits.append((self._metadata[idx], float(score)))  # score ‚àà [-1, 1]\n",
    "        return hits\n",
    "\n",
    "    # ---------- Persistence ----------\n",
    "    def save(self, index_path: str, metadata_path: str):\n",
    "        \"\"\"Save FAISS index and metadata (JSONL if .jsonl else pickle).\"\"\"\n",
    "        faiss.write_index(self.index, index_path)\n",
    "\n",
    "        # Save metadata (choose JSONL by default for readability)\n",
    "        if metadata_path.endswith(\".jsonl\"):\n",
    "            with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for row in self._metadata:\n",
    "                    f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "        else:\n",
    "            with open(metadata_path, \"wb\") as f:\n",
    "                pickle.dump(self._metadata, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, index_path: str, metadata_path: str):\n",
    "        \"\"\"Load FAISS index and metadata; returns an initialized store.\"\"\"\n",
    "        index = faiss.read_index(index_path)\n",
    "\n",
    "        # Load metadata\n",
    "        if metadata_path.endswith(\".jsonl\"):\n",
    "            metadata = []\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    metadata.append(json.loads(line))\n",
    "        else:\n",
    "            with open(metadata_path, \"rb\") as f:\n",
    "                metadata = pickle.load(f)\n",
    "\n",
    "        dim = index.d  # read dimension from index\n",
    "        store = cls(dim)\n",
    "        store.index = index\n",
    "        store._metadata = metadata\n",
    "        return store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a773b-ce50-44ae-bdb2-5b95bd32a4a2",
   "metadata": {},
   "source": [
    "# BM25 for keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37a75061-de4f-4d84-84e1-cffcb3f88ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bd12173-6b6e-4529-ab2a-29990ebe97c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    \"\"\"\n",
    "    Persistent BM25 index with UNIQUE docs keyed by doc_id.\n",
    "    - doc_id := doc['id'] if present, else doc['uploaded_filename']\n",
    "    - add_or_replace(): replaces existing doc with same doc_id\n",
    "    - load(): dedupes by doc_id (last one wins)\n",
    "    \"\"\"\n",
    "    def __init__(self, by_id=None):\n",
    "        self.by_id = by_id or {}  # doc_id -> meta (includes 'text')\n",
    "        self._bm25 = None\n",
    "        self._ids = []     # order aligned with _tokens\n",
    "        self._tokens = []  # tokenized texts in same order\n",
    "        self._docs = {} \n",
    "\n",
    "    # ---------- persistence ----------\n",
    "    @classmethod\n",
    "    def load(cls, meta_path: str):\n",
    "        by_id = {}\n",
    "        if os.path.exists(meta_path):\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    d = json.loads(line)\n",
    "                    doc_id = d.get(\"id\") or d.get(\"uploaded_filename\")\n",
    "                    if not doc_id:\n",
    "                        # skip malformed rows\n",
    "                        continue\n",
    "                    # normalize stored id & text\n",
    "                    d[\"id\"] = doc_id\n",
    "                    d[\"text\"] = (d.get(\"text\") or \"\").strip()\n",
    "                    # last one wins for same doc_id\n",
    "                    by_id[doc_id] = d\n",
    "        return cls(by_id)\n",
    "\n",
    "    def save(self, meta_path: str):\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for doc_id, d in self.by_id.items():\n",
    "                f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def clear(self):\n",
    "        self.by_id.clear()\n",
    "        self._bm25 = None\n",
    "        self._ids = []\n",
    "        self._tokens = []\n",
    "\n",
    "    # ---------- core ops ----------\n",
    "    def _rebuild(self):\n",
    "        # build arrays in a stable order\n",
    "        self._ids = list(self.by_id.keys())\n",
    "        self._tokens = [_tokenize(self.by_id[i].get(\"text\", \"\")) for i in self._ids]\n",
    "        self._bm25 = BM25Okapi(self._tokens)\n",
    "\n",
    "    def add_or_replace(self, doc_meta: dict):\n",
    "        doc_id = doc_meta.get(\"id\") or doc_meta.get(\"uploaded_filename\")\n",
    "        if not doc_id:\n",
    "            raise ValueError(\"doc_meta must include 'id' or 'uploaded_filename'\")\n",
    "        # normalize\n",
    "        doc_meta = {**doc_meta, \"id\": doc_id, \"text\": (doc_meta.get(\"text\") or \"\").strip()}\n",
    "        self.by_id[doc_id] = doc_meta\n",
    "        self._bm25 = None  # mark dirty (lazy rebuild)\n",
    "\n",
    "    def search(self, query: str, k: int = 5):\n",
    "        if not self.by_id:\n",
    "            return []\n",
    "        if self._bm25 is None:\n",
    "            self._rebuild()\n",
    "        q_tokens = _tokenize(query or \"\")\n",
    "        scores = self._bm25.get_scores(q_tokens)\n",
    "        order = np.argsort(scores)[::-1]\n",
    "        hits = []\n",
    "        for i in order:\n",
    "            doc_id = self._ids[i]\n",
    "            meta = self.by_id[doc_id]\n",
    "            hits.append((meta, float(scores[i])))\n",
    "            if len(hits) >= k:\n",
    "                break\n",
    "        return hits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._docs)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self._docs)\n",
    "\n",
    "def _tokenize(txt: str):\n",
    "    return re.findall(r\"[A-Za-z0-9\\-]+\", (txt or \"\").lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bdc3e-3f20-4d1c-8235-7745b0c44990",
   "metadata": {},
   "source": [
    "# Creating Hybrid index with RAG and BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "919a21cc-5b2d-4f0e-ac89-0c836b3da839",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def embed_texts_with_gemini(client, texts: List[str], embedding_modelname:str =\"text-embedding-004\") -> List[List[float]]:\n",
    "    emb_list = []\n",
    "    for t in texts:\n",
    "        emb = client.models.embed_content(\n",
    "            model=embedding_modelname,\n",
    "            contents=[{\"role\": \"user\", \"parts\": [{\"text\": t}]}]\n",
    "        ).embeddings[0].values\n",
    "        emb_list.append(emb)\n",
    "    return emb_list\n",
    "\n",
    "def embed_query_with_gemini(client, query: str, embedding_modelname:str =\"text-embedding-004\") -> List[float]:\n",
    "    resp = client.models.embed_content(\n",
    "        model=embedding_modelname,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": query}]}]\n",
    "    )\n",
    "    embed_query = resp.embeddings[0].values\n",
    "    \n",
    "    return embed_query, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dafb3a7-0fdb-44f6-998e-07b0445471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the datetime object into a string\n",
    "formatted_today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "system_text       =  f\"You are an expert HR assistant, make a summarization of the uploaded CV. Note that today is {formatted_today}.\"\n",
    "prompt_text       = \"\"\"\n",
    "Analyze the uploaded CV and provide the following:\n",
    "\n",
    "1. Identify the individual (full name).\n",
    "2. Summarize education and work experience in **10‚Äì15 concise bullet points**, including years of experience for each role.\n",
    "3. Provide a breakdown of total experience (in years) aggregated by **position title** across the job history.\n",
    "4. Extract the list of skills and output them as a valid Python list (e.g., [\"skill1\", \"skill2\", \"skill3\"]).\n",
    "\"\"\"\n",
    "\n",
    "max_output_tokens = 4096*2\n",
    "temperature       = 0.1\n",
    "top_p             = 0.9 # At each step, the model sorts possible next tokens by probability, then keeps only the smallest set whose cumulative probability ‚â• top_p.\n",
    "top_k             = 40  # At each step, the model only considers the top_k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c55d6ac-df91-4dd3-9ce6-c00bac717f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_filenames_list = [\n",
    "    \"ExampleCV/NLP-CV-NachaiLim.pdf\",\n",
    "    \"ExampleCV/CV-Oranid.pdf\",\n",
    "    \"ExampleCV/Natthaporn_CV2022.pdf\",\n",
    "]\n",
    "applied_position = \"senior data scientist\"\n",
    "embedding_modelname = \"text-embedding-004\"\n",
    "vector_dbname = \"vector_and_bm25_dbs/vector_index.faiss\"\n",
    "vector_dbmeta = \"vector_and_bm25_dbs/vector_metadata.jsonl\"\n",
    "bm25_dbmeta   = \"vector_and_bm25_dbs/bm25_metadata.jsonl\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "053f37ae-1a5b-4fa2-aaa6-efd6b82145cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Start Processing 0st CV from 3 CVs: ExampleCV/NLP-CV-NachaiLim.pdf ====\n",
      ">>>>>  Start Summarizing: ExampleCV/NLP-CV-NachaiLim.pdf\n",
      "resp.text:\n",
      " Here's a summarization of the uploaded CV:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Individual's Full Name:**\n",
      "Nachai Limsettho\n",
      "\n",
      "---\n",
      "\n",
      "**2. Education and Work Experience Summary:**\n",
      "\n",
      "*   **Senior Data Scientist at TipTip Network PTE. LTD. (Singapore) (2022-Present, 3 years):** Led the design and implementation of Digital Content and Creator recommendation systems on AWS.\n",
      "*   **Senior Data Scientist at TipTip Network PTE. LTD. (Singapore) (2022-Present, 3 years):** Engineered and implemented automated eKYC, video, and text moderation systems, significantly reducing API costs.\n",
      "*   **Data Scientist at OVO (PT Visionet Internasional) (Singapore) (2019-2022, 3 years):** Led the development of multiple customer score models and a sentiment analysis model using NLP and topic modeling.\n",
      "*   **Data Scientist at OVO (PT Visionet Internasional) (Singapore) (2019-2022, 3 years):** Developed and engineered a feature store containing over 300 advanced features.\n",
      "*   **Senior Executive, Data Analytics at Allianz SE (Singapore) (2018-2019, 1 year):** Collaborated with risk teams, developed Tableau dashboards, and led a Call Volume Prediction model using RNN.\n",
      "*   **Senior Executive, Data Analytics at Allianz SE (Singapore) (2018-2019, 1 year):** Managed multiple data science projects for Thailand OE and analyzed projects for international OEs.\n",
      "*   **Data Scientist at True Corporation Public Company Limited (Thailand) (2016-2018, 2 years):** Developed classification models and affinity score models using telecom data and neural networks.\n",
      "*   **Data Scientist at True Corporation Public Company Limited (Thailand) (2016-2018, 2 years):** Provided technical consultations to management and analyzed customer characteristics for retention.\n",
      "*   **Doctor of Philosophy in Computer Engineering:** Nara Institute of Science and Technology, Japan (2013-2016, 3 years).\n",
      "*   **Master in Computer Engineering:** Kasetsart University, Thailand (2010-2012, 2 years), GPA: 3.90.\n",
      "*   **Bachelor in Computer Engineering:** Kasetsart University, Thailand (2006-2010, 4 years), GPA: 3.10.\n",
      "*   **Awards & Achievements:** Invited speaker at Monetizing Big Data in Telecoms World Summit (2017), Japanese Government Scholarship (2013-2016), and KU Scholarship (2010-2012).\n",
      "\n",
      "---\n",
      "\n",
      "**3. Total Experience by Position Title:**\n",
      "\n",
      "*   **Senior Data Scientist:** 3 years\n",
      "*   **Data Scientist:** 5 years\n",
      "*   **Senior Executive, Data Analytics:** 1 year\n",
      "\n",
      "---\n",
      "\n",
      "**4. Skills List (Python List Format):**\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Python\", \"PySpark\", \"SQL\", \"C#\",\n",
      "    \"Amazon Web Services (AWS)\", \"S3\", \"Redshift\", \"SageMaker\", \"Rekognition\", \"Textract\",\n",
      "    \"LLM\", \"CNN\", \"LSTM\", \"DNN\", \"RNN\", \"Transformer\", \"Auto-Encoder\",\n",
      "    \"Classification\", \"Clustering\", \"Ensemble\", \"Imbalanced dataset\",\n",
      "    \"Sentiment Analysis\", \"Word2vec\", \"Tokenization\",\n",
      "    \"GitLab\", \"GitHub\", \"Notion\", \"JIRA\",\n",
      "    \"Tableau\", \"Pyplot\", \"Seaborn\",\n",
      "    \"English\", \"Thai\", \"Japanese (Basic)\",\n",
      "    \"Project Management\", \"Interdivisional Collaboration\", \"Presentation Skills\", \"Well organized\", \"Good at teamwork\"\n",
      "]\n",
      "```\n",
      "[resp.text size] chars=3023, tokens‚âà385\n",
      ">>>>>  Start Embedding: ExampleCV/NLP-CV-NachaiLim.pdf\n",
      ">>>>>  Start Saving to FAISS db + Meta data: ExampleCV/NLP-CV-NachaiLim.pdf\n",
      ">>>>>  Start Adding BM25 index: ExampleCV/NLP-CV-NachaiLim.pdf\n",
      "Added ExampleCV/NLP-CV-NachaiLim.pdf to BM25 (total 0 docs).\n",
      "\n",
      "==== Start Processing 1st CV from 3 CVs: ExampleCV/CV-Oranid.pdf ====\n",
      ">>>>>  Start Summarizing: ExampleCV/CV-Oranid.pdf\n",
      "resp.text:\n",
      " Here's a summarization of the provided CV:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Individual Identification\n",
      "**Full Name:** Oranid Yenradee\n",
      "\n",
      "### 2. Education and Work Experience Summary (12 concise bullet points)\n",
      "\n",
      "*   **Master of Commerce (Extension) in Data Analytics**, University of Sydney (Feb 2023 ‚Äì Jan 2025, 2 years), achieving First Class Honor.\n",
      "*   **Bachelor of Engineering in Nanoengineering**, Chulalongkorn University (Jul 2016 ‚Äì Aug 2020, 4 years 1 month), with First Class Honor and Gold Medal.\n",
      "*   **Exchange Student** in Material Science and Engineering, Seoul National University (Sep 2019 ‚Äì Dec 2019, 4 months).\n",
      "*   **Data Scientist** at University of Sydney in collaboration with Madpaw (Feb 2024 ‚Äì May 2024, 4 months), developing a classification model (F1 score 0.5344) to predict booking completion.\n",
      "*   As Data Scientist, conducted data cleaning, explanatory data analysis, and built various classification models including logistic regression, random forests, and gradient boosting.\n",
      "*   **Senior Product Owner (Part-time)** at Ion Energy Corporation Limited (Feb 2023 ‚Äì Jan 2024, 1 year), leading the development of a smart home energy monitoring system.\n",
      "*   **Senior Product Owner (Full-time)** at Ion Energy Corporation Limited (Feb 2022 ‚Äì Jan 2023, 1 year), collaborating with Samsung SmartThings to define requirements and implementation strategies.\n",
      "*   As Senior Product Owner, designed logic, data sources, and transformation processes to derive valuable energy insights for smart home users.\n",
      "*   **Product Owner** at Ion Energy Corporation Limited (Dec 2020 ‚Äì Jan 2022, 1 year 2 months), leading a cross-functional team for a solar power plant monitoring and payment application.\n",
      "*   As Product Owner, defined product features, plans, and user stories, integrated e-payment technology, and provided platform training to stakeholders.\n",
      "*   **Digital Marketer** at Adheseal Company Limited (Jun 2020 ‚Äì Nov 2020, 6 months), achieving a 300% increase in organic monthly website traffic.\n",
      "*   As Digital Marketer, introduced products to the B2C market for the first time through e-commerce platforms.\n",
      "\n",
      "### 3. Total Experience by Position Title\n",
      "\n",
      "*   **Senior Product Owner:** 2 years (24 months)\n",
      "*   **Product Owner:** 1 year 2 months (14 months)\n",
      "*   **Digital Marketer:** 6 months (0.5 years)\n",
      "*   **Data Scientist:** 4 months (0.33 years)\n",
      "\n",
      "### 4. Skills List\n",
      "\n",
      "```python\n",
      "[\n",
      "    \"Thai\",\n",
      "    \"English\",\n",
      "    \"Microsoft Office\",\n",
      "    \"Tableau\",\n",
      "    \"SPSS\",\n",
      "    \"Excel Open-Solver\",\n",
      "    \"ArcGIS Pro\",\n",
      "    \"PrecisionTree\",\n",
      "    \"@Risk\",\n",
      "    \"Trello\",\n",
      "    \"Python\",\n",
      "    \"Pandas\",\n",
      "    \"Matplotlib\",\n",
      "    \"Scikit-learn\",\n",
      "    \"Statsmodels\",\n",
      "    \"TensorFlow\",\n",
      "    \"Gurobi\",\n",
      "    \"R Studio\",\n",
      "    \"API Integration\",\n",
      "    \"SQL\",\n",
      "    \"Statistical Analysis\",\n",
      "    \"Linear Programming\",\n",
      "    \"Data Visualization\",\n",
      "    \"Time-series Forecasting\",\n",
      "    \"Linear Regression\",\n",
      "    \"Spatial Analytics\",\n",
      "    \"Logistic Regression and Classification\",\n",
      "    \"Neural Network\",\n",
      "    \"Clustering\",\n",
      "    \"Basic NLP\",\n",
      "    \"Analytical and Critical Thinking\",\n",
      "    \"Problem-Solving\",\n",
      "    \"Communication\",\n",
      "    \"Project/Product Management\"\n",
      "]\n",
      "```\n",
      "[resp.text size] chars=3092, tokens‚âà408\n",
      ">>>>>  Start Embedding: ExampleCV/CV-Oranid.pdf\n",
      ">>>>>  Start Saving to FAISS db + Meta data: ExampleCV/CV-Oranid.pdf\n",
      ">>>>>  Start Adding BM25 index: ExampleCV/CV-Oranid.pdf\n",
      "Added ExampleCV/CV-Oranid.pdf to BM25 (total 0 docs).\n",
      "\n",
      "==== Start Processing 2st CV from 3 CVs: ExampleCV/Natthaporn_CV2022.pdf ====\n",
      ">>>>>  Start Summarizing: ExampleCV/Natthaporn_CV2022.pdf\n",
      "resp.text:\n",
      " Here's a summary of the provided CV:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Individual Identification\n",
      "**Full Name:** Natthaporn Takpho\n",
      "\n",
      "### 2. Education and Work Experience Summary (10‚Äì15 concise bullet points)\n",
      "\n",
      "*   **Assistant Manager, Research Division, Mitsui Chemicals Singapore R&D Centre (May 2021 ‚Äì Present; 4 years, 4 months)**: Leads and directs R&D projects in life science and healthcare, focusing on new business opportunities and market needs in Southeast Asia.\n",
      "*   **Assistant Manager, Research Division, Mitsui Chemicals Singapore R&D Centre (May 2021 ‚Äì Present; 4 years, 4 months)**: Established R&D frameworks, managed patent applications, collaborated with stakeholders, and consistently received top performance reviews.\n",
      "*   **Researcher, Mitsui Chemicals Singapore R&D Centre (Nov 2017 ‚Äì Apr 2021; 3 years, 6 months)**: Planned and supervised R&D projects in life science and healthcare, developing new business ideas and models.\n",
      "*   **Researcher, Mitsui Chemicals Singapore R&D Centre (Nov 2017 ‚Äì Apr 2021; 3 years, 6 months)**: Validated technology feasibility, established Proof-of-Concept (PoC) platforms, prepared patent drafts, and provided technical support and coaching.\n",
      "*   **Molecular Microbiologist, Mahidol-Oxford Tropical Medicine Research Unit (Oct 2013 ‚Äì Sep 2014; 1 year)**: Operated molecular genetic analysis for epidemiological studies, discovered new bacterial isolates, and worked in a CDC-certified BSL-3 laboratory.\n",
      "*   **PhD in Biological Science, Nara Institute of Science and Technology (Completed Sep 2017; approx. 4 years, 2 months)**: Awarded a Japanese government (MEXT) scholarship and completed internships at Gekkeikan Co, Ltd and University of California Davis.\n",
      "*   **MSc in Biotechnology, Mahidol University (Completed Jul 2013; approx. 2 years)**: Received multiple research scholarships and awards for academic research, with training at Osaka University in yeast genetic molecular and DNA microarray studies.\n",
      "*   **BSc in Biotechnology, King Mongkut's Institute of Technology Lad Krabang (Completed Mar 2010; approx. 4 years)**: Focused on bioactivities of morning glory extracts and completed an internship in TB diagnosis and immunohistochemistry.\n",
      "\n",
      "### 3. Total Experience by Position Title\n",
      "*   **Assistant Manager:** 4 years, 4 months\n",
      "*   **Researcher:** 3 years, 6 months\n",
      "*   **Molecular Microbiologist:** 1 year\n",
      "\n",
      "### 4. Skills List (Python list format)\n",
      "```python\n",
      "[\n",
      "    \"Molecular biology\",\n",
      "    \"DNA sequencing\",\n",
      "    \"NGS\",\n",
      "    \"Real time PCR\",\n",
      "    \"Vector construction\",\n",
      "    \"Gene & protein expression\",\n",
      "    \"Molecular genome typing\",\n",
      "    \"Biochemistry & Analysis\",\n",
      "    \"Western blot\",\n",
      "    \"Northern blot\",\n",
      "    \"Enzymatic assay\",\n",
      "    \"HPLC\",\n",
      "    \"Biotechnology\",\n",
      "    \"Microbial strain engineering\",\n",
      "    \"Lab-scale fermentation\",\n",
      "    \"Substrate evaluation\",\n",
      "    \"Microbiology\",\n",
      "    \"Bacteria\",\n",
      "    \"Yeast\"\n",
      "]\n",
      "```\n",
      "[resp.text size] chars=2847, tokens‚âà374\n",
      ">>>>>  Start Embedding: ExampleCV/Natthaporn_CV2022.pdf\n",
      ">>>>>  Start Saving to FAISS db + Meta data: ExampleCV/Natthaporn_CV2022.pdf\n",
      ">>>>>  Start Adding BM25 index: ExampleCV/Natthaporn_CV2022.pdf\n",
      "Added ExampleCV/Natthaporn_CV2022.pdf to BM25 (total 0 docs).\n",
      "\n",
      "==== Completed Processing 3 CVs ====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>uploaded_file</th>\n",
       "      <th>response_text</th>\n",
       "      <th>finish_reason</th>\n",
       "      <th>cached_content_token_count</th>\n",
       "      <th>candidates_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>thoughts_token_count</th>\n",
       "      <th>total_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 09:33:29.233297+00:00</td>\n",
       "      <td>\\nAnalyze the uploaded CV and provide the foll...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the provided CV:\\n\\n...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>670</td>\n",
       "      <td>1172</td>\n",
       "      <td>1888</td>\n",
       "      <td>3730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 09:33:38.423997+00:00</td>\n",
       "      <td>\\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>Here's a summarization of the Senior Data Scie...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>437</td>\n",
       "      <td>489</td>\n",
       "      <td>1410</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01 09:33:51.643159+00:00</td>\n",
       "      <td>\\n# job description (JD):\\nHere's a summarizat...</td>\n",
       "      <td>ExampleCV/Natthaporn_CV2022.pdf</td>\n",
       "      <td>### JD Mandatory Requirements [‚úì: Pass | ‚úó: Mi...</td>\n",
       "      <td>STOP</td>\n",
       "      <td>None</td>\n",
       "      <td>766</td>\n",
       "      <td>1356</td>\n",
       "      <td>1881</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp  \\\n",
       "0 2025-09-01 09:33:29.233297+00:00   \n",
       "1 2025-09-01 09:33:38.423997+00:00   \n",
       "2 2025-09-01 09:33:51.643159+00:00   \n",
       "\n",
       "                                               query  \\\n",
       "0  \\nAnalyze the uploaded CV and provide the foll...   \n",
       "1  \\nSenior Data Scientist\\n\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5-10 ...   \n",
       "2  \\n# job description (JD):\\nHere's a summarizat...   \n",
       "\n",
       "                     uploaded_file  \\\n",
       "0  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "1  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "2  ExampleCV/Natthaporn_CV2022.pdf   \n",
       "\n",
       "                                       response_text finish_reason  \\\n",
       "0  Here's a summarization of the provided CV:\\n\\n...          STOP   \n",
       "1  Here's a summarization of the Senior Data Scie...          STOP   \n",
       "2  ### JD Mandatory Requirements [‚úì: Pass | ‚úó: Mi...          STOP   \n",
       "\n",
       "  cached_content_token_count  candidates_token_count  prompt_token_count  \\\n",
       "0                       None                     670                1172   \n",
       "1                       None                     437                 489   \n",
       "2                       None                     766                1356   \n",
       "\n",
       "   thoughts_token_count  total_token_count  \n",
       "0                  1888               3730  \n",
       "1                  1410               2336  \n",
       "2                  1881               4003  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Load / init FAISS (and clear) =========\n",
    "try:\n",
    "    loaded_store = FaissVectorStore.load(vector_dbname, vector_dbmeta)\n",
    "    loaded_store.clear()\n",
    "    loaded_store.save(vector_dbname, vector_dbmeta)  # persist cleared index\n",
    "except Exception:\n",
    "    print(f\"Fail to load '{vector_dbname}' & '{vector_dbmeta}', creating new vector DB instead\")\n",
    "    loaded_store = FaissVectorStore(dim=768)\n",
    "    loaded_store.save(vector_dbname, vector_dbmeta)  # <-- persist empty index here\n",
    "\n",
    "# ========= Load / init BM25 (and clear like FAISS) =========\n",
    "try:\n",
    "    bm25_index = BM25Index.load(bm25_dbmeta)\n",
    "    bm25_index.clear()\n",
    "    bm25_index.save(bm25_dbmeta)\n",
    "except Exception:\n",
    "    bm25_index = BM25Index()\n",
    "    bm25_index.save(bm25_dbmeta)\n",
    "# ==================================\n",
    "# Ingest loop (load -> add -> save)\n",
    "# ==================================\n",
    "for idx, uploaded_filename in enumerate(uploaded_filenames_list):\n",
    "    print(f\"\\n==== Start Processing {idx}st CV from {len(uploaded_filenames_list)} CVs: {uploaded_filename} ====\")\n",
    "    try:\n",
    "        print(f\">>>>>  Start Summarizing: {uploaded_filename}\")\n",
    "        # Step 1: Raw CV to summarized CV\n",
    "        resp_cv, logs_df = master_gemini_upload_cv_prompt_w_log(\n",
    "            uploaded_filename, prompt_text, logs_df,\n",
    "            system_text       = system_text,\n",
    "            max_output_tokens = max_output_tokens,\n",
    "            temperature       = temperature,\n",
    "            top_p             = top_p, \n",
    "            top_k             = top_k, \n",
    "        )\n",
    "\n",
    "        # Step 2: Embedding the summarized CV\n",
    "        print(f\">>>>>  Start Embedding: {uploaded_filename}\")\n",
    "        doc_text = (resp_cv.text or \"\").strip()\n",
    "        docs = [doc_text]\n",
    "        per_item_meta = [{\"id\": idx, \"uploaded_filename\": uploaded_filename, \"applied_position\": applied_position},]\n",
    "        defaults = {\"embedding_model\": embedding_modelname, \"project\": \"cv-summarization\"}\n",
    "        doc_embs = embed_texts_with_gemini(client, docs, embedding_modelname = embedding_modelname)\n",
    "\n",
    "        # Step 3: Saving the embedding\n",
    "        print(f\">>>>>  Start Saving to FAISS db + Meta data: {uploaded_filename}\")\n",
    "        loaded_store = FaissVectorStore.load(vector_dbname, vector_dbmeta)\n",
    "        loaded_store.add_texts_and_metadata(\n",
    "            texts=docs,\n",
    "            embeddings=doc_embs,\n",
    "            metadata_list=per_item_meta,\n",
    "            default_metadata=defaults,\n",
    "        )\n",
    "        loaded_store.save(vector_dbname, vector_dbmeta)\n",
    "\n",
    "        # Step 4: Save to BM25 (load -> add -> save, same style)\n",
    "        print(f\">>>>>  Start Adding BM25 index: {uploaded_filename}\")\n",
    "        # reload current BM25 from disk to mirror FAISS pattern (optional but \"same way\")\n",
    "        bm25_index = BM25Index.load(bm25_dbmeta)\n",
    "        bm25_meta = {**defaults, **per_item_meta[0], \"text\": doc_text}\n",
    "        bm25_index.add_or_replace(bm25_meta)\n",
    "        bm25_index.save(bm25_dbmeta)\n",
    "        print(f\"Added {uploaded_filename} to BM25 (total {len(bm25_index)} docs).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload and/or extract CV: {e}\")\n",
    "print(f\"\\n==== Completed Processing {len(uploaded_filenames_list)} CVs ====\")\n",
    "logs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8251db35-5c27-42ca-ae82-aeca476f6a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Vector Search ===\n",
      "[score=0.4102] ExampleCV/CV-Oranid.pdf\n",
      "[score=0.3402] ExampleCV/Natthaporn_CV2022.pdf\n",
      "[score=0.3237] ExampleCV/NLP-CV-NachaiLim.pdf\n",
      "\n",
      "=== BM25 Search ===\n",
      "[score=0.0925] ExampleCV/Natthaporn_CV2022.pdf\n",
      "[score=0.0823] ExampleCV/CV-Oranid.pdf\n",
      "[score=0.0783] ExampleCV/NLP-CV-NachaiLim.pdf\n"
     ]
    }
   ],
   "source": [
    "# ========= Example query after loop =========\n",
    "query = \"Study in Australia\"\n",
    "q_emb, _ = embed_query_with_gemini(client, query, embedding_modelname=embedding_modelname)\n",
    "\n",
    "vec_hits = loaded_store.search(q_emb, k=3)\n",
    "bm25_hits = bm25_index.search(query, k=3)\n",
    "\n",
    "def show(hits, title):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for meta, score in hits:\n",
    "        print(f\"[score={score:.4f}] {meta['uploaded_filename']}\")\n",
    "\n",
    "show(vec_hits, \"Vector Search\")\n",
    "show(bm25_hits, \"BM25 Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c924a-445e-4e37-8edc-169439c8412b",
   "metadata": {},
   "source": [
    "# Creating context prompt with RAG & BM25 from query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1fb3735-d690-4326-bbb5-23a742d95e68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _doc_id(m: dict) -> str:\n",
    "    return m.get(\"id\") or m.get(\"uploaded_filename\") or str(id(m))\n",
    "\n",
    "def _safe(txt: str) -> str:\n",
    "    return (txt or \"\").strip()\n",
    "\n",
    "def _truncate_chars(txt: str, max_chars: int) -> str:\n",
    "    t = _safe(txt)\n",
    "    return t if len(t) <= max_chars else (t[:max_chars].rstrip() + \"‚Ä¶\")\n",
    "    \n",
    "def _rrf_fuse(\n",
    "    vec_hits: List[Tuple[Dict[str, Any], float]],\n",
    "    bm25_hits: List[Tuple[Dict[str, Any], float]],\n",
    "    k_final: int = 5,\n",
    "    rrf_k: int = 60,\n",
    "    w_vec: float = 1.0,\n",
    "    w_bm: float = 1.0,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fuse two ranked result lists (vector search and BM25) with Weighted Reciprocal Rank Fusion (RRF).\n",
    "\n",
    "    Args:\n",
    "        vec_hits: A list of (metadata, score) pairs from the vector/embedding search.\n",
    "                  `metadata` must contain a stable identifier consumable by `_doc_id(meta)`.\n",
    "        bm25_hits: A list of (metadata, score) pairs from the BM25 keyword search.\n",
    "        k_final: Number of fused items to return.\n",
    "        rrf_k: RRF damping constant. Larger values reduce the influence of rank differences.\n",
    "               Classic literature often uses 60. Must be > 0.\n",
    "        w_vec: Weight for the vector-search contribution to the fused score.\n",
    "        w_bm: Weight for the BM25 contribution to the fused score.\n",
    "\n",
    "    Returns:\n",
    "        A list of metadata dicts (length ‚â§ `k_final`) ranked by fused score (descending).\n",
    "        Each returned dict is a shallow copy of the original metadata and includes:\n",
    "            - \"_score_vec\": the original vector score for that item (0 if absent)\n",
    "            - \"_score_bm25\": the original BM25 score for that item (0 if absent)\n",
    "            - \"_score_fused\": the final fused score (higher = better)\n",
    "\n",
    "    Notes:\n",
    "        - The fusion is rank-based, robust to incomparable raw score scales.\n",
    "        - Items present in only one list are still included (the other rank treated as ‚àû).\n",
    "        - This function depends on an external `_doc_id(meta: dict) -> str` helper that\n",
    "          returns a unique/stable doc ID (e.g., meta[\"id\"] or meta[\"uploaded_filename\"]).\n",
    "\n",
    "    Example:\n",
    "        fused = _rrf_fuse(vec_hits, bm25_hits, k_final=5, rrf_k=60, w_vec=1.0, w_bm=0.7)\n",
    "    \"\"\"\n",
    "    # Build rank maps: lower rank number = better (1 is best).\n",
    "    # We sort each hits list by score descending to assign ranks.\n",
    "    vec_ranks = {\n",
    "        _doc_id(m): r\n",
    "        for r, (m, _) in enumerate(sorted(vec_hits, key=lambda x: x[1], reverse=True), 1)\n",
    "    }\n",
    "    bm_ranks = {\n",
    "        _doc_id(m): r\n",
    "        for r, (m, _) in enumerate(sorted(bm25_hits, key=lambda x: x[1], reverse=True), 1)\n",
    "    }\n",
    "\n",
    "    # Remember original scores so we can attach them to outputs later.\n",
    "    vec_scores = { _doc_id(m): s for m, s in vec_hits }\n",
    "    bm_scores  = { _doc_id(m): s for m, s in bm25_hits }\n",
    "\n",
    "    # Union of all doc IDs across both runs.\n",
    "    all_ids = set(vec_ranks.keys()) | set(bm_ranks.keys())\n",
    "\n",
    "    fused: List[Tuple[str, float]] = []\n",
    "    for did in all_ids:\n",
    "        # If a doc is missing in one list, give it an effectively infinite rank.\n",
    "        r_vec = vec_ranks.get(did, 10**9)\n",
    "        r_bm  = bm_ranks.get(did, 10**9)\n",
    "\n",
    "        # Weighted Reciprocal Rank Fusion:\n",
    "        #   fused = w_vec * 1/(rrf_k + rank_vec) + w_bm * 1/(rrf_k + rank_bm)\n",
    "        f = (w_vec * (1.0 / (rrf_k + r_vec))) + (w_bm * (1.0 / (rrf_k + r_bm)))\n",
    "        fused.append((did, f))\n",
    "\n",
    "    # Sort by fused score (descending).\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Build a lookup from ID -> original metadata (first one encountered wins;\n",
    "    # both lists should contain identical metas for the same ID in sane pipelines).\n",
    "    by_id_meta: Dict[str, Dict[str, Any]] = {}\n",
    "    for m, _ in vec_hits + bm25_hits:\n",
    "        did = _doc_id(m)\n",
    "        if did not in by_id_meta:\n",
    "            by_id_meta[did] = m\n",
    "\n",
    "    # Materialize the top-k results with attached score diagnostics.\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for did, f in fused[:k_final]:\n",
    "        m = dict(by_id_meta[did])  # shallow copy to avoid mutating upstream metadata\n",
    "        m[\"_score_vec\"] = float(vec_scores.get(did, 0.0))\n",
    "        m[\"_score_bm25\"] = float(bm_scores.get(did, 0.0))\n",
    "        m[\"_score_fused\"] = float(f)\n",
    "        out.append(m)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _build_context_block(ranked_ctx: List[dict], max_chars_per_ctx=900) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact context with numeric citations [1], [2], ...\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for i, m in enumerate(ranked_ctx, 1):\n",
    "        uf = m.get(\"uploaded_filename\", \"doc\")\n",
    "        preview = _truncate_chars(m.get(\"text\", \"\"), max_chars_per_ctx)\n",
    "        lines.append(\n",
    "            f\"[{i}] file: {uf} | vec={m.get('_score_vec',0):.3f} | bm25={m.get('_score_bm25',0):.3f} | fused={m.get('_score_fused',0):.3f}\\n{preview}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "# --------- Main: RAG + BM25 + Gemini ---------\n",
    "def master_gemini_rag_bm25_answer_w_log(\n",
    "    client,\n",
    "    query: str,\n",
    "    vector_store,         # your FaissVectorStore handle\n",
    "    bm25_index,           # your BM25Index handle (with .search)\n",
    "    logs_df,\n",
    "    # retrieval knobs\n",
    "    k_vec=4, k_bm25=6, k_final=5,\n",
    "    rrf_k=60, w_vec=1.0, w_bm=1.0,\n",
    "    max_chars_per_ctx=4096,\n",
    "    # model knobs (same style as your JD helper)\n",
    "    system_text = \"\"\"\n",
    "    You are an expert HR assistant.\n",
    "    Use only the provided CONTEXT to answer. If not found, say so.\n",
    "    Cite sources with [n] where n matches the context block number.\n",
    "    Respond concisely for recruiters.\n",
    "    \"\"\",\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    max_output_tokens=4096*3,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    verbose = 0\n",
    "):\n",
    "    # 1) Vector + BM25\n",
    "    q_emb, _ = embed_query_with_gemini(client, query, embedding_modelname=\"text-embedding-004\")\n",
    "    vec_hits  = vector_store.search(q_emb, k=k_vec)       # [(meta, sim)]\n",
    "    bm_hits   = bm25_index.search(query, k=k_bm25)        # [(meta, score)]\n",
    "\n",
    "    # 2) Fuse\n",
    "    fused_ctx = _rrf_fuse(vec_hits, bm_hits, k_final=k_final, rrf_k=rrf_k, w_vec=w_vec, w_bm=w_bm)\n",
    "    context_block = _build_context_block(fused_ctx, max_chars_per_ctx=max_chars_per_ctx)\n",
    "\n",
    "    # 3) Build final prompt\n",
    "    user_prompt = f\"\"\"QUESTION:\n",
    "    {query}\n",
    "    \n",
    "    CONTEXT (numbered; cite as [n]):\n",
    "    {context_block}\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    - Answer the QUESTION based only on the CONTEXT.\n",
    "    - If multiple candidates match, list top matches with 1-2 line justifications.\n",
    "    - Use [n] citations after each claim referencing a specific context block.\n",
    "    - If the answer is not present, say \"Not found in provided documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # 4) Call Gemini\n",
    "    resp = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=[{\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}],\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=[system_text],\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 5) Optional: pretty print + logging\n",
    "    if verbose > 0:\n",
    "        try:\n",
    "            print(\"LLM answer:\\n\", resp.text)\n",
    "        except Exception:\n",
    "            print(\"LLM answer:\\n <no text>\")\n",
    "\n",
    "    # Append a new log\n",
    "    logs_df = append_usage_log(\n",
    "        logs_df,\n",
    "        query_text=query,\n",
    "        uploaded_file=None,\n",
    "        resp=resp\n",
    "    )\n",
    "    return resp, logs_df, fused_ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a70bbc37-8a9b-4fd3-a044-69d074e6f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer:\n",
      " The following candidates have work experience in Singapore:\n",
      "\n",
      "*   **Natthaporn Takpho** has worked as an Assistant Manager and Researcher at Mitsui Chemicals Singapore R&D Centre [1].\n",
      "*   **Nachai Limsettho** has worked as a Senior Data Scientist at TipTip Network PTE. LTD. (Singapore), a Data Scientist at OVO (PT Visionet Internasional) (Singapore), and a Senior Executive, Data Analytics at Allianz SE (Singapore) [2].\n",
      "ExampleCV/Natthaporn_CV2022.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"Which candidates have work experience in Singapore?\"\n",
    "resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "    client=client,\n",
    "    query=query,\n",
    "    vector_store=loaded_store,\n",
    "    bm25_index=bm25_index,\n",
    "    logs_df=logs_df,\n",
    "    k_vec=3, k_bm25=3, k_final=2, \n",
    "    w_vec=1.0, w_bm=1.0,\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdcc84c5-0eb0-4c19-a51c-6e65535a67ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM answer:\n",
      " Oranid Yenradee studies in Australia, pursuing a Master of Commerce (Extension) in Data Analytics at the University of Sydney [1].\n",
      "ExampleCV/CV-Oranid.pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"Which candidates study in Australia?\"\n",
    "resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "    client=client,\n",
    "    query=query,\n",
    "    vector_store=loaded_store,\n",
    "    bm25_index=bm25_index,\n",
    "    logs_df=logs_df,\n",
    "    k_vec=3, k_bm25=3, k_final=2, \n",
    "    w_vec=1.0, w_bm=1.0,\n",
    "    verbose = 1\n",
    ")\n",
    "print(used_ctx[0]['uploaded_filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ccfa6-7b3d-478f-9a9a-55e9c5efd9f9",
   "metadata": {},
   "source": [
    "# Simple chat GUI with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25678e4a-dd99-43c3-a1a0-4b4b87edda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "501b9fc8-f7b5-4c9b-867f-1fa6ab8779a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def chat_handler(user_msg, history, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens):\n",
    "    global logs_df\n",
    "    history = history or []  # history is a list of {\"role\":..., \"content\":...}\n",
    "\n",
    "    # Call your RAG+BM25 function\n",
    "    resp, logs_df, used_ctx = master_gemini_rag_bm25_answer_w_log(\n",
    "        client=client,\n",
    "        query=user_msg,\n",
    "        vector_store=loaded_store,\n",
    "        bm25_index=bm25_index,\n",
    "        logs_df=logs_df,\n",
    "        k_vec=int(k_vec),\n",
    "        k_bm25=int(k_bm25),\n",
    "        k_final=int(k_final),\n",
    "        rrf_k=int(rrf_k),\n",
    "        w_vec=float(w_vec),\n",
    "        w_bm=float(w_bm),\n",
    "        max_output_tokens=int(max_tokens),\n",
    "        temperature=float(temperature),\n",
    "    )\n",
    "\n",
    "    answer = getattr(resp, \"text\", \"\") or \"(no text)\"\n",
    "\n",
    "    # Build sources block\n",
    "    src_lines = []\n",
    "    for i, m in enumerate(used_ctx or [], 1):\n",
    "        src_lines.append(\n",
    "            f\"[{i}] {m.get('uploaded_filename','doc')} | \"\n",
    "            f\"vec={m.get('_score_vec',0):.3f} bm25={m.get('_score_bm25',0):.3f} fused={m.get('_score_fused',0):.3f}\"\n",
    "        )\n",
    "    sources_block = \"**Sources**\\n\" + (\"\\n\".join(src_lines) if src_lines else \"No sources.\")\n",
    "    assistant_msg = answer.strip() + \"\\n\\n\" + sources_block\n",
    "\n",
    "    # üö® IMPORTANT: append as dicts with role/content\n",
    "    history.append({\"role\": \"user\",      \"content\": user_msg})\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    return history, \"\\n\".join(src_lines)\n",
    "\n",
    "# ---- Build the simple UI ----\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### üîé RAG + BM25 Chat (Notebook)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chat = gr.Chatbot(label=\"Conversation\", type='messages', height=480, show_copy_button=True)\n",
    "            user_box = gr.Textbox(label=\"Your question\", placeholder=\"e.g., Which candidates worked in Singapore?\")\n",
    "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
    "        with gr.Column(scale=3):\n",
    "            with gr.Accordion(\"Settings\", open=False):\n",
    "                k_vec   = gr.Slider(1, 10, value=4, step=1, label=\"Top-k (Vector)\")\n",
    "                k_bm25  = gr.Slider(1, 10, value=6, step=1, label=\"Top-k (BM25)\")\n",
    "                k_final = gr.Slider(1, 10, value=5, step=1, label=\"Top-k (Fused)\")\n",
    "                rrf_k   = gr.Slider(1, 200, value=60, step=1, label=\"RRF k (damping)\")\n",
    "                w_vec   = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label=\"Weight: Vector\")\n",
    "                w_bm    = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label=\"Weight: BM25\")\n",
    "                temperature = gr.Slider(0.0, 1.5, value=0.1, step=0.05, label=\"Temperature\")\n",
    "                max_tokens  = gr.Slider(1024, 4096*3, value=1024, step=64, label=\"Max output tokens\")\n",
    "            sources = gr.Textbox(label=\"Sources (debug)\", lines=8, show_copy_button=True)\n",
    "\n",
    "    # Wire up Enter key and button\n",
    "    user_box.submit(\n",
    "        fn=chat_handler,\n",
    "        inputs=[user_box, chat, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens],\n",
    "        outputs=[chat, sources],\n",
    "    ).then(fn=lambda: \"\", inputs=None, outputs=user_box)\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=chat_handler,\n",
    "        inputs=[user_box, chat, k_vec, k_bm25, k_final, rrf_k, w_vec, w_bm, temperature, max_tokens],\n",
    "        outputs=[chat, sources],\n",
    "    ).then(fn=lambda: \"\", inputs=None, outputs=user_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6685786d-c09b-48cc-bec2-add586d0a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(inline=False, share=False, show_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc8179-d88c-48bd-a975-52b4e7da1582",
   "metadata": {},
   "source": [
    "## Example question \n",
    "* Who are a good candidates for developing dashboard, rank and score (0-1) too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23f71526-a7bf-4943-8fe0-eb4a3a15fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a99db7-86f9-4131-b234-b150cfd4550f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3c69-6d14-4752-a98a-e394775a7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960d017-242b-4c6d-9103-c1b1f7642612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
